{"pageProps":{"category":{"key":"artificial-intelligence","name":"Artificial intelligence","description":"Artificial intelligence tools and applications","icon":"ðŸ—ï¸","color":"#44475a"},"guides":[{"name":"ollama","displayName":"Ollama Installation Guide","slug":"ollama","description":"Ollama is a free and open-source tool for running large language models (LLMs) locally on your machine. It serves as a FOSS alternative to cloud-based AI services like OpenAI API, Anthropic Claude API, or Google's Gemini API, enabling privacy-focused AI deployment, offline inference, and cost-effective local AI processing.","category":"artificial-intelligence","subcategory":"llm-runners","difficultyLevel":"intermediate","estimatedSetupTime":"15-30 minutes","supportedOS":["rhel","centos","rocky","almalinux","debian","ubuntu","arch","alpine","opensuse","sles","macos","windows"],"defaultPorts":[11434],"installationMethods":["official-installer","package-manager","manual-binary"],"features":["multi-os-support","local-llm-inference","gpu-acceleration","model-management","rest-api","privacy-focused","offline-capable","comprehensive-documentation","security-hardening","performance-optimization","backup-restore-procedures","troubleshooting-guides"],"tags":["ai","llm","machine-learning","local-inference","privacy","openai-alternative","gpu-acceleration","model-serving","rest-api"],"maintenanceStatus":"active","specVersion":"2.0","version":"1.0.0","license":"MIT","websiteUrl":"https://howtomgr.github.io/artificial-intelligence/ollama","documentationUrl":"https://howtomgr.github.io/artificial-intelligence/ollama","language":null,"stars":0,"forks":0,"topics":[],"githubUrl":"https://github.com/howtomgr/ollama","updatedAt":"2025-09-16T17:37:13Z","createdAt":"2025-09-16T10:48:43Z","readmeRaw":"# Ollama Installation Guide\n\nOllama is a free and open-source tool for running large language models (LLMs) locally on your machine. It serves as a FOSS alternative to cloud-based AI services like OpenAI API, Anthropic Claude API, Google's Gemini API, or Azure OpenAI Service. Ollama enables privacy-focused AI deployment, offline inference, and cost-effective local AI processing with support for popular models like Llama 3, Code Llama, Mistral, and many others.\n\n## Table of Contents\n1. [Prerequisites](#prerequisites)\n2. [Supported Operating Systems](#supported-operating-systems)\n3. [Installation](#installation)\n4. [Configuration](#configuration)\n5. [Service Management](#service-management)\n6. [Troubleshooting](#troubleshooting)\n7. [Security Considerations](#security-considerations)\n8. [Performance Tuning](#performance-tuning)\n9. [Backup and Restore](#backup-and-restore)\n10. [System Requirements](#system-requirements)\n11. [Support](#support)\n12. [Contributing](#contributing)\n13. [License](#license)\n14. [Acknowledgments](#acknowledgments)\n15. [Version History](#version-history)\n16. [Appendices](#appendices)\n\n## 1. Prerequisites\n\n### Hardware Requirements\n- **CPU**: Modern 64-bit processor (x86_64 or ARM64)\n- **RAM**: 8GB minimum (16GB+ recommended for larger models)\n- **Storage**: 10GB+ free space for models\n- **GPU**: Optional but recommended (NVIDIA with CUDA, AMD ROCm, or Apple Metal)\n\n### Software Requirements\n- **Operating System**: Linux, macOS, or Windows\n- **Internet**: Required for initial model downloads\n- **Docker**: Optional for containerized deployment\n\n### Network Requirements\n- **Ports**: \n  - 11434: Default API server port\n- **Bandwidth**: High-speed internet for model downloads (models range from 1GB to 70GB+)\n\n## 2. Supported Operating Systems\n\nOllama officially supports:\n- RHEL 8/9 and derivatives (CentOS Stream, Rocky Linux, AlmaLinux)\n- Debian 11/12\n- Ubuntu 20.04 LTS / 22.04 LTS / 24.04 LTS\n- Arch Linux\n- Alpine Linux 3.18+\n- openSUSE Leap 15.5+ / Tumbleweed\n- macOS 11+ (Big Sur and later)\n- Windows 10/11\n\n## 3. Installation\n\n### RHEL/CentOS/Rocky Linux/AlmaLinux\n\n```bash\n# Method 1: Official installer script\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Method 2: Manual installation\n# Download latest release\ncurl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Create ollama user\nsudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama\nsudo usermod -a -G render,video ollama\n\n# Create systemd service\nsudo tee /etc/systemd/system/ollama.service > /dev/null << 'EOF'\n[Unit]\nDescription=Ollama Service\nAfter=network-online.target\n\n[Service]\nExecStart=/usr/local/bin/ollama serve\nUser=ollama\nGroup=ollama\nRestart=always\nRestartSec=3\nEnvironment=\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\nEnvironment=\"OLLAMA_HOST=0.0.0.0\"\n\n[Install]\nWantedBy=default.target\nEOF\n\n# Enable and start service\nsudo systemctl daemon-reload\nsudo systemctl enable --now ollama\n```\n\n### Debian/Ubuntu\n\n```bash\n# Method 1: Official installer script\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Method 2: Package installation (if available)\n# Add official repository\ncurl -fsSL https://ollama.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/ollama-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/ollama-keyring.gpg] https://ollama.com/debian stable main\" | sudo tee /etc/apt/sources.list.d/ollama.list\n\n# Install package\nsudo apt update\nsudo apt install -y ollama\n\n# Start service\nsudo systemctl enable --now ollama\n```\n\n### Arch Linux\n\n```bash\n# Install from AUR\nyay -S ollama-bin\n# or\nparu -S ollama-bin\n\n# Alternative: Build from source\nyay -S ollama\n\n# Enable and start service\nsudo systemctl enable --now ollama\n```\n\n### Alpine Linux\n\n```bash\n# Install dependencies\napk add --no-cache curl\n\n# Install Ollama binary\ncurl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Create ollama user\nadduser -D -s /bin/false -h /usr/share/ollama ollama\naddgroup ollama video\naddgroup ollama render\n\n# Create OpenRC service\ntee /etc/init.d/ollama > /dev/null << 'EOF'\n#!/sbin/openrc-run\n\ndescription=\"Ollama Service\"\ncommand=\"/usr/local/bin/ollama\"\ncommand_args=\"serve\"\ncommand_user=\"ollama\"\ncommand_group=\"ollama\"\npidfile=\"/run/ollama.pid\"\ncommand_background=\"yes\"\n\ndepend() {\n    need net\n    after firewall\n}\n\nstart_pre() {\n    export OLLAMA_HOST=\"0.0.0.0\"\n    checkpath --directory --owner ollama:ollama --mode 0755 /run/ollama\n}\nEOF\n\nchmod +x /etc/init.d/ollama\nrc-update add ollama default\nrc-service ollama start\n```\n\n### openSUSE\n\n```bash\n# Install via zypper (if available) or manual installation\nsudo zypper refresh\n\n# Manual installation\ncurl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Create ollama user\nsudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama\nsudo usermod -a -G video,render ollama\n\n# Create systemd service (same as RHEL)\nsudo systemctl enable --now ollama\n```\n\n### macOS\n\n```bash\n# Method 1: Official app installer\n# Download from https://ollama.com/download/mac\n\n# Method 2: Homebrew\nbrew install ollama\n\n# Method 3: Manual installation\ncurl -L https://ollama.com/download/darwin-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Start Ollama\nollama serve &\n```\n\n### Windows\n\n```powershell\n# Method 1: Official installer\n# Download and run installer from https://ollama.com/download/windows\n\n# Method 2: Winget\nwinget install Ollama.Ollama\n\n# Method 3: Chocolatey\nchoco install ollama\n\n# Method 4: Scoop\nscoop bucket add extras\nscoop install ollama\n\n# Start Ollama service (automatic with installer)\n```\n\n## 4. Configuration\n\n### Environment Variables\n\nCreate `/etc/systemd/system/ollama.service.d/override.conf`:\n```ini\n[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0:11434\"\nEnvironment=\"OLLAMA_MODELS=/var/lib/ollama/models\"\nEnvironment=\"OLLAMA_NUM_PARALLEL=2\"\nEnvironment=\"OLLAMA_MAX_LOADED_MODELS=3\"\nEnvironment=\"OLLAMA_FLASH_ATTENTION=1\"\n```\n\n### Configuration Options\n\n```bash\n# Set custom models directory\nexport OLLAMA_MODELS=/custom/path/to/models\n\n# Configure host and port\nexport OLLAMA_HOST=127.0.0.1:11434\n\n# GPU configuration\nexport CUDA_VISIBLE_DEVICES=0,1  # Use specific GPUs\nexport OLLAMA_GPU_OVERHEAD=0     # Reduce GPU memory overhead\n\n# Performance tuning\nexport OLLAMA_NUM_PARALLEL=4     # Parallel requests\nexport OLLAMA_MAX_LOADED_MODELS=2 # Max models in memory\nexport OLLAMA_FLASH_ATTENTION=1  # Enable flash attention\n```\n\n### Model Management\n\n```bash\n# Download and run models\nollama pull llama3.1:8b\nollama pull codellama:13b\nollama pull mistral:7b\n\n# List installed models\nollama list\n\n# Run a model interactively\nollama run llama3.1:8b\n\n# Remove a model\nollama rm llama3.1:8b\n\n# Show model information\nollama show llama3.1:8b\n```\n\n## 5. Service Management\n\n### systemd (Linux)\n\n```bash\n# Start/stop/restart service\nsudo systemctl start ollama\nsudo systemctl stop ollama\nsudo systemctl restart ollama\n\n# Check service status\nsudo systemctl status ollama\n\n# View logs\nsudo journalctl -u ollama -f\n\n# Enable/disable auto-start\nsudo systemctl enable ollama\nsudo systemctl disable ollama\n```\n\n### Manual Service Management\n\n```bash\n# Start Ollama server\nollama serve\n\n# Start with custom configuration\nOLLAMA_HOST=0.0.0.0:11434 ollama serve\n\n# Background process\nnohup ollama serve > /var/log/ollama.log 2>&1 &\n```\n\n### Windows Service Management\n\n```powershell\n# Check service status\nGet-Service Ollama\n\n# Start/stop service\nStart-Service Ollama\nStop-Service Ollama\n\n# Restart service\nRestart-Service Ollama\n```\n\n## 6. Troubleshooting\n\n### Common Issues\n\n1. **Service won't start**:\n```bash\n# Check logs\nsudo journalctl -u ollama -n 50\n\n# Check if port is in use\nsudo netstat -tlnp | grep 11434\n\n# Verify user permissions\nsudo -u ollama /usr/local/bin/ollama serve\n```\n\n2. **GPU not detected**:\n```bash\n# Check NVIDIA GPU\nnvidia-smi\n\n# Check CUDA installation\nnvcc --version\n\n# Check Ollama GPU support\nollama info\n```\n\n3. **Model download fails**:\n```bash\n# Check internet connectivity\ncurl -I https://ollama.com\n\n# Check disk space\ndf -h /var/lib/ollama\n\n# Manual model download\ncurl -L https://huggingface.co/model-url -o model-file\n```\n\n4. **High memory usage**:\n```bash\n# Check model memory usage\nollama ps\n\n# Reduce loaded models\nexport OLLAMA_MAX_LOADED_MODELS=1\n\n# Monitor system resources\nhtop\n```\n\n### Debug Mode\n\n```bash\n# Enable debug logging\nexport OLLAMA_DEBUG=1\nollama serve\n\n# Verbose API logging\nexport OLLAMA_VERBOSE=1\n```\n\n## 7. Security Considerations\n\n### Network Security\n\n```bash\n# Bind to localhost only (default)\nexport OLLAMA_HOST=127.0.0.1:11434\n\n# Configure firewall (if exposing externally)\nsudo firewall-cmd --permanent --add-port=11434/tcp\nsudo firewall-cmd --reload\n\n# Use reverse proxy for external access\n```\n\n### Reverse Proxy Configuration (nginx)\n\n```nginx\nserver {\n    listen 80;\n    server_name ollama.example.com;\n    \n    location / {\n        proxy_pass http://127.0.0.1:11434;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n```\n\n### Authentication Setup\n\n```bash\n# Ollama doesn't have built-in auth, use reverse proxy\n# Example with basic auth in nginx:\nsudo apt install apache2-utils\nsudo htpasswd -c /etc/nginx/.htpasswd ollama_user\n\n# Add to nginx config:\n# auth_basic \"Ollama Access\";\n# auth_basic_user_file /etc/nginx/.htpasswd;\n```\n\n### File Permissions\n\n```bash\n# Secure model directory\nsudo chown -R ollama:ollama /var/lib/ollama\nsudo chmod -R 750 /var/lib/ollama\n\n# Secure configuration files\nsudo chmod 640 /etc/systemd/system/ollama.service\nsudo chown root:root /etc/systemd/system/ollama.service\n```\n\n## 8. Performance Tuning\n\n### GPU Optimization\n\n```bash\n# NVIDIA GPU settings\nexport CUDA_VISIBLE_DEVICES=0,1\nexport OLLAMA_GPU_OVERHEAD=0\n\n# Check GPU utilization\nnvidia-smi -l 1\n\n# AMD GPU (ROCm)\nexport HSA_OVERRIDE_GFX_VERSION=10.3.0\nexport ROCM_PATH=/opt/rocm\n```\n\n### CPU Optimization\n\n```bash\n# Set CPU affinity\ntaskset -c 0-7 ollama serve\n\n# Adjust parallel processing\nexport OLLAMA_NUM_PARALLEL=4\nexport OLLAMA_MAX_LOADED_MODELS=2\n\n# Enable optimizations\nexport OLLAMA_FLASH_ATTENTION=1\nexport OLLAMA_NUMA_PREFER=0\n```\n\n### Memory Management\n\n```bash\n# Monitor memory usage\nwatch -n 1 'free -h && echo \"=== Ollama Process ===\" && ps aux | grep ollama'\n\n# Limit model cache\nexport OLLAMA_MAX_LOADED_MODELS=1\n\n# Use swap if needed (not recommended for production)\nsudo swapon --show\n```\n\n### Storage Optimization\n\n```bash\n# Use SSD for models\nsudo mkdir -p /mnt/ssd/ollama/models\nsudo chown ollama:ollama /mnt/ssd/ollama/models\nexport OLLAMA_MODELS=/mnt/ssd/ollama/models\n\n# Clean up unused models\nollama list | grep -v \"NAME\" | awk '{print $1}' | xargs ollama rm\n```\n\n## 9. Backup and Restore\n\n### Model Backup\n\n```bash\n#!/bin/bash\n# backup-ollama-models.sh\n\nBACKUP_DIR=\"/var/backups/ollama\"\nMODELS_DIR=\"/var/lib/ollama/models\"\nDATE=$(date +%Y%m%d_%H%M%S)\n\n# Create backup directory\nmkdir -p $BACKUP_DIR\n\n# Backup models directory\ntar -czf $BACKUP_DIR/ollama_models_$DATE.tar.gz -C /var/lib/ollama models\n\n# Backup model list\nollama list > $BACKUP_DIR/ollama_models_list_$DATE.txt\n\necho \"Backup completed: $BACKUP_DIR/ollama_models_$DATE.tar.gz\"\n```\n\n### Configuration Backup\n\n```bash\n#!/bin/bash\n# backup-ollama-config.sh\n\nBACKUP_DIR=\"/var/backups/ollama\"\nDATE=$(date +%Y%m%d_%H%M%S)\n\n# Backup configuration\ntar -czf $BACKUP_DIR/ollama_config_$DATE.tar.gz \\\n    /etc/systemd/system/ollama.service \\\n    /etc/systemd/system/ollama.service.d/ 2>/dev/null || true\n\necho \"Configuration backup: $BACKUP_DIR/ollama_config_$DATE.tar.gz\"\n```\n\n### Restore Procedures\n\n```bash\n# Restore models\nsudo systemctl stop ollama\nsudo tar -xzf ollama_models_backup.tar.gz -C /var/lib/ollama\nsudo chown -R ollama:ollama /var/lib/ollama/models\nsudo systemctl start ollama\n\n# Verify restored models\nollama list\n```\n\n### Automated Backup\n\n```bash\n# Add to crontab\nsudo crontab -e\n\n# Daily model backup at 2 AM\n0 2 * * * /opt/ollama/scripts/backup-ollama-models.sh\n\n# Weekly configuration backup\n0 3 * * 0 /opt/ollama/scripts/backup-ollama-config.sh\n```\n\n## 10. System Requirements\n\n### Minimum Requirements\n- **CPU**: 2 cores, 2.0 GHz\n- **RAM**: 8GB\n- **Storage**: 20GB (small models)\n- **Network**: Broadband for model downloads\n\n### Recommended Requirements\n- **CPU**: 8+ cores, 3.0+ GHz\n- **RAM**: 32GB+\n- **GPU**: NVIDIA RTX 3060+ or AMD RX 6600 XT+\n- **Storage**: 100GB+ SSD/NVMe\n- **Network**: Gigabit for large model downloads\n\n### Model-Specific Requirements\n\n| Model Size | RAM Required | VRAM Required | Storage |\n|------------|--------------|---------------|---------|\n| 7B         | 8GB          | 4GB           | 4GB     |\n| 13B        | 16GB         | 8GB           | 7GB     |\n| 30B        | 32GB         | 20GB          | 19GB    |\n| 70B        | 64GB         | 48GB          | 39GB    |\n\n## 11. Support\n\n### Official Resources\n- **Website**: https://ollama.com\n- **GitHub**: https://github.com/ollama/ollama\n- **Documentation**: https://github.com/ollama/ollama/tree/main/docs\n- **Model Library**: https://ollama.com/library\n\n### Community Support\n- **Discord**: https://discord.gg/ollama\n- **Reddit**: r/ollama\n- **GitHub Issues**: https://github.com/ollama/ollama/issues\n- **Discussions**: https://github.com/ollama/ollama/discussions\n\n## 12. Contributing\n\n### How to Contribute\n1. Fork the repository on GitHub\n2. Create a feature branch\n3. Submit pull request\n4. Follow Go coding standards\n5. Include tests and documentation\n\n### Development Setup\n```bash\n# Clone repository\ngit clone https://github.com/ollama/ollama.git\ncd ollama\n\n# Install Go dependencies\ngo mod tidy\n\n# Build from source\ngo build .\n\n# Run tests\ngo test ./...\n```\n\n## 13. License\n\nOllama is licensed under the MIT License.\n\nKey points:\n- Free to use, modify, and distribute\n- Commercial use allowed\n- No warranty provided\n- Attribution required\n\n## 14. Acknowledgments\n\n### Credits\n- **Ollama Team**: Core development team\n- **Meta AI**: Llama model family\n- **Mistral AI**: Mistral models\n- **Community**: Model creators and contributors\n- **Hardware Vendors**: NVIDIA, AMD, Apple for acceleration support\n\n## 15. Version History\n\n### Recent Releases\n- **v0.3.x**: Latest stable with improved performance\n- **v0.2.x**: Added model management improvements\n- **v0.1.x**: Initial public release\n\n### Major Features by Version\n- **v0.3.0**: Enhanced GPU support, model compression\n- **v0.2.0**: REST API improvements, concurrent requests\n- **v0.1.0**: Basic model serving and CLI interface\n\n## 16. Appendices\n\n### A. API Usage Examples\n\n#### Basic Chat Completion\n```bash\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.1:8b\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"stream\": false\n}'\n```\n\n#### Streaming Response\n```bash\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.1:8b\",\n  \"prompt\": \"Write a poem about coding\",\n  \"stream\": true\n}'\n```\n\n#### Chat API\n```bash\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.1:8b\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello, how are you?\"\n    }\n  ]\n}'\n```\n\n### B. Integration Examples\n\n#### Python Integration\n```python\nimport requests\nimport json\n\ndef chat_with_ollama(prompt, model=\"llama3.1:8b\"):\n    url = \"http://localhost:11434/api/generate\"\n    data = {\n        \"model\": model,\n        \"prompt\": prompt,\n        \"stream\": False\n    }\n    \n    response = requests.post(url, json=data)\n    if response.status_code == 200:\n        return response.json()[\"response\"]\n    else:\n        return \"Error: \" + str(response.status_code)\n\n# Usage\nresponse = chat_with_ollama(\"Explain quantum computing\")\nprint(response)\n```\n\n#### Node.js Integration\n```javascript\nconst axios = require('axios');\n\nasync function chatWithOllama(prompt, model = 'llama3.1:8b') {\n    try {\n        const response = await axios.post('http://localhost:11434/api/generate', {\n            model: model,\n            prompt: prompt,\n            stream: false\n        });\n        \n        return response.data.response;\n    } catch (error) {\n        console.error('Error:', error.message);\n        return null;\n    }\n}\n\n// Usage\nchatWithOllama('What is machine learning?').then(response => {\n    console.log(response);\n});\n```\n\n### C. Model Customization\n\n#### Creating Custom Models\n```bash\n# Create Modelfile\ncat > Modelfile << 'EOF'\nFROM llama3.1:8b\n\n# Set parameters\nPARAMETER temperature 0.7\nPARAMETER top_p 0.9\n\n# Set system message\nSYSTEM \"\"\"\nYou are a helpful AI assistant specialized in programming.\nAlways provide code examples when relevant.\n\"\"\"\nEOF\n\n# Build custom model\nollama create my-coding-assistant -f Modelfile\n\n# Test custom model\nollama run my-coding-assistant \"How do I sort a list in Python?\"\n```\n\n#### Fine-tuning (Advanced)\n```bash\n# Prepare training data (JSONL format)\ncat > training_data.jsonl << 'EOF'\n{\"prompt\": \"Question: What is Python?\", \"completion\": \"Python is a programming language...\"}\n{\"prompt\": \"Question: How to install packages?\", \"completion\": \"Use pip install package_name...\"}\nEOF\n\n# Note: Fine-tuning requires additional tools and setup\n# Refer to Ollama documentation for detailed fine-tuning guide\n```\n\n### D. Performance Monitoring\n\n```bash\n#!/bin/bash\n# monitor-ollama.sh\n\necho \"=== Ollama Service Status ===\"\nsystemctl status ollama --no-pager\n\necho -e \"\\n=== Memory Usage ===\"\nps aux | grep ollama | grep -v grep\n\necho -e \"\\n=== GPU Usage ===\"\nif command -v nvidia-smi &> /dev/null; then\n    nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits\nfi\n\necho -e \"\\n=== API Health Check ===\"\ncurl -s http://localhost:11434/api/version || echo \"API not responding\"\n\necho -e \"\\n=== Loaded Models ===\"\nollama ps\n\necho -e \"\\n=== Disk Usage ===\"\ndu -sh /var/lib/ollama/models/*\n```\n\n---\n\nFor more information and updates, visit https://github.com/howtomgr/ollama","readmeHtml":"<p class=\"mobile-paragraph\">Ollama is a free and open-source tool for running large language models (LLMs) locally on your machine. It serves as a FOSS alternative to cloud-based AI services like OpenAI API, Anthropic Claude API, Google's Gemini API, or Azure OpenAI Service. Ollama enables privacy-focused AI deployment, offline inference, and cost-effective local AI processing with support for popular models like Llama 3, Code Llama, Mistral, and many others.</p>\n<h2 id=\"1-prerequisites\" class=\"mobile-header\">1. Prerequisites</h2>\n<h3 id=\"hardware-requirements\" class=\"mobile-header\">Hardware Requirements</h3>\n<li class=\"mobile-list-item\"><strong>CPU</strong>: Modern 64-bit processor (x86_64 or ARM64)</li>\n<li class=\"mobile-list-item\"><strong>RAM</strong>: 8GB minimum (16GB+ recommended for larger models)</li>\n<li class=\"mobile-list-item\"><strong>Storage</strong>: 10GB+ free space for models</li>\n<li class=\"mobile-list-item\"><strong>GPU</strong>: Optional but recommended (NVIDIA with CUDA, AMD ROCm, or Apple Metal)</li>\n<h3 id=\"software-requirements\" class=\"mobile-header\">Software Requirements</h3>\n<li class=\"mobile-list-item\"><strong>Operating System</strong>: Linux, macOS, or Windows</li>\n<li class=\"mobile-list-item\"><strong>Internet</strong>: Required for initial model downloads</li>\n<li class=\"mobile-list-item\"><strong>Docker</strong>: Optional for containerized deployment</li>\n<h3 id=\"network-requirements\" class=\"mobile-header\">Network Requirements</h3>\n<li class=\"mobile-list-item\"><strong>Ports</strong>: </li>\n<li class=\"mobile-list-item\">11434: Default API server port</li>\n<li class=\"mobile-list-item\"><strong>Bandwidth</strong>: High-speed internet for model downloads (models range from 1GB to 70GB+)</li>\n<h2 id=\"2-supported-operating-systems\" class=\"mobile-header\">2. Supported Operating Systems</h2>\n<p class=\"mobile-paragraph\">Ollama officially supports:</p>\n<li class=\"mobile-list-item\">RHEL 8/9 and derivatives (CentOS Stream, Rocky Linux, AlmaLinux)</li>\n<li class=\"mobile-list-item\">Debian 11/12</li>\n<li class=\"mobile-list-item\">Ubuntu 20.04 LTS / 22.04 LTS / 24.04 LTS</li>\n<li class=\"mobile-list-item\">Arch Linux</li>\n<li class=\"mobile-list-item\">Alpine Linux 3.18+</li>\n<li class=\"mobile-list-item\">openSUSE Leap 15.5+ / Tumbleweed</li>\n<li class=\"mobile-list-item\">macOS 11+ (Big Sur and later)</li>\n<li class=\"mobile-list-item\">Windows 10/11</li>\n<h2 id=\"3-installation\" class=\"mobile-header\">3. Installation</h2>\n<h3 id=\"rhelcentosrocky-linuxalmalinux\" class=\"mobile-header\">RHEL/CentOS/Rocky Linux/AlmaLinux</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Method 1: Official installer script\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Method 2: Manual installation\n# Download latest release\ncurl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Create ollama user\nsudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama\nsudo usermod -a -G render,video ollama\n\n# Create systemd service\nsudo tee /etc/systemd/system/ollama.service &gt; /dev/null &lt;&lt; &#039;EOF&#039;\n[Unit]\nDescription=Ollama Service\nAfter=network-online.target\n\n[Service]\nExecStart=/usr/local/bin/ollama serve\nUser=ollama\nGroup=ollama\nRestart=always\nRestartSec=3\nEnvironment=&quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;\nEnvironment=&quot;OLLAMA_HOST=0.0.0.0&quot;\n\n[Install]\nWantedBy=default.target\nEOF\n\n# Enable and start service\nsudo systemctl daemon-reload\nsudo systemctl enable --now ollama</code></pre>\n      </div>\n    </div>\n<h3 id=\"debianubuntu\" class=\"mobile-header\">Debian/Ubuntu</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Method 1: Official installer script\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Method 2: Package installation (if available)\n# Add official repository\ncurl -fsSL https://ollama.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/ollama-keyring.gpg\necho &quot;deb [signed-by=/usr/share/keyrings/ollama-keyring.gpg] https://ollama.com/debian stable main&quot; | sudo tee /etc/apt/sources.list.d/ollama.list\n\n# Install package\nsudo apt update\nsudo apt install -y ollama\n\n# Start service\nsudo systemctl enable --now ollama</code></pre>\n      </div>\n    </div>\n<h3 id=\"arch-linux\" class=\"mobile-header\">Arch Linux</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Install from AUR\nyay -S ollama-bin\n# or\nparu -S ollama-bin\n\n# Alternative: Build from source\nyay -S ollama\n\n# Enable and start service\nsudo systemctl enable --now ollama</code></pre>\n      </div>\n    </div>\n<h3 id=\"alpine-linux\" class=\"mobile-header\">Alpine Linux</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Install dependencies\napk add --no-cache curl\n\n# Install Ollama binary\ncurl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Create ollama user\nadduser -D -s /bin/false -h /usr/share/ollama ollama\naddgroup ollama video\naddgroup ollama render\n\n# Create OpenRC service\ntee /etc/init.d/ollama &gt; /dev/null &lt;&lt; &#039;EOF&#039;\n#!/sbin/openrc-run\n\ndescription=&quot;Ollama Service&quot;\ncommand=&quot;/usr/local/bin/ollama&quot;\ncommand_args=&quot;serve&quot;\ncommand_user=&quot;ollama&quot;\ncommand_group=&quot;ollama&quot;\npidfile=&quot;/run/ollama.pid&quot;\ncommand_background=&quot;yes&quot;\n\ndepend() {\n    need net\n    after firewall\n}\n\nstart_pre() {\n    export OLLAMA_HOST=&quot;0.0.0.0&quot;\n    checkpath --directory --owner ollama:ollama --mode 0755 /run/ollama\n}\nEOF\n\nchmod +x /etc/init.d/ollama\nrc-update add ollama default\nrc-service ollama start</code></pre>\n      </div>\n    </div>\n<h3 id=\"opensuse\" class=\"mobile-header\">openSUSE</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Install via zypper (if available) or manual installation\nsudo zypper refresh\n\n# Manual installation\ncurl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Create ollama user\nsudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama\nsudo usermod -a -G video,render ollama\n\n# Create systemd service (same as RHEL)\nsudo systemctl enable --now ollama</code></pre>\n      </div>\n    </div>\n<h3 id=\"macos\" class=\"mobile-header\">macOS</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Method 1: Official app installer\n# Download from https://ollama.com/download/mac\n\n# Method 2: Homebrew\nbrew install ollama\n\n# Method 3: Manual installation\ncurl -L https://ollama.com/download/darwin-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Start Ollama\nollama serve &amp;</code></pre>\n      </div>\n    </div>\n<h3 id=\"windows\" class=\"mobile-header\">Windows</h3>\n<div class=\"mobile-code-block\" data-language=\"powershell\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">powershell</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-powershell\"># Method 1: Official installer\n# Download and run installer from https://ollama.com/download/windows\n\n# Method 2: Winget\nwinget install Ollama.Ollama\n\n# Method 3: Chocolatey\nchoco install ollama\n\n# Method 4: Scoop\nscoop bucket add extras\nscoop install ollama\n\n# Start Ollama service (automatic with installer)</code></pre>\n      </div>\n    </div>\n<h2 id=\"4-configuration\" class=\"mobile-header\">4. Configuration</h2>\n<h3 id=\"environment-variables\" class=\"mobile-header\">Environment Variables</h3>\n<p class=\"mobile-paragraph\">Create <code class=\"mobile-inline-code\">/etc/systemd/system/ollama.service.d/override.conf</code>:</p>\n<div class=\"mobile-code-block\" data-language=\"ini\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">ini</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-ini\">[Service]\nEnvironment=&quot;OLLAMA_HOST=0.0.0.0:11434&quot;\nEnvironment=&quot;OLLAMA_MODELS=/var/lib/ollama/models&quot;\nEnvironment=&quot;OLLAMA_NUM_PARALLEL=2&quot;\nEnvironment=&quot;OLLAMA_MAX_LOADED_MODELS=3&quot;\nEnvironment=&quot;OLLAMA_FLASH_ATTENTION=1&quot;</code></pre>\n      </div>\n    </div>\n<h3 id=\"configuration-options\" class=\"mobile-header\">Configuration Options</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Set custom models directory\nexport OLLAMA_MODELS=/custom/path/to/models\n\n# Configure host and port\nexport OLLAMA_HOST=127.0.0.1:11434\n\n# GPU configuration\nexport CUDA_VISIBLE_DEVICES=0,1  # Use specific GPUs\nexport OLLAMA_GPU_OVERHEAD=0     # Reduce GPU memory overhead\n\n# Performance tuning\nexport OLLAMA_NUM_PARALLEL=4     # Parallel requests\nexport OLLAMA_MAX_LOADED_MODELS=2 # Max models in memory\nexport OLLAMA_FLASH_ATTENTION=1  # Enable flash attention</code></pre>\n      </div>\n    </div>\n<h3 id=\"model-management\" class=\"mobile-header\">Model Management</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Download and run models\nollama pull llama3.1:8b\nollama pull codellama:13b\nollama pull mistral:7b\n\n# List installed models\nollama list\n\n# Run a model interactively\nollama run llama3.1:8b\n\n# Remove a model\nollama rm llama3.1:8b\n\n# Show model information\nollama show llama3.1:8b</code></pre>\n      </div>\n    </div>\n<h2 id=\"5-service-management\" class=\"mobile-header\">5. Service Management</h2>\n<h3 id=\"systemd-linux\" class=\"mobile-header\">systemd (Linux)</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Start/stop/restart service\nsudo systemctl start ollama\nsudo systemctl stop ollama\nsudo systemctl restart ollama\n\n# Check service status\nsudo systemctl status ollama\n\n# View logs\nsudo journalctl -u ollama -f\n\n# Enable/disable auto-start\nsudo systemctl enable ollama\nsudo systemctl disable ollama</code></pre>\n      </div>\n    </div>\n<h3 id=\"manual-service-management\" class=\"mobile-header\">Manual Service Management</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Start Ollama server\nollama serve\n\n# Start with custom configuration\nOLLAMA_HOST=0.0.0.0:11434 ollama serve\n\n# Background process\nnohup ollama serve &gt; /var/log/ollama.log 2&gt;&amp;1 &amp;</code></pre>\n      </div>\n    </div>\n<h3 id=\"windows-service-management\" class=\"mobile-header\">Windows Service Management</h3>\n<div class=\"mobile-code-block\" data-language=\"powershell\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">powershell</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-powershell\"># Check service status\nGet-Service Ollama\n\n# Start/stop service\nStart-Service Ollama\nStop-Service Ollama\n\n# Restart service\nRestart-Service Ollama</code></pre>\n      </div>\n    </div>\n<h2 id=\"6-troubleshooting\" class=\"mobile-header\">6. Troubleshooting</h2>\n<h3 id=\"common-issues\" class=\"mobile-header\">Common Issues</h3>\n<p class=\"mobile-paragraph\">1. <strong>Service won't start</strong>:</p>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Check logs\nsudo journalctl -u ollama -n 50\n\n# Check if port is in use\nsudo netstat -tlnp | grep 11434\n\n# Verify user permissions\nsudo -u ollama /usr/local/bin/ollama serve</code></pre>\n      </div>\n    </div>\n<p class=\"mobile-paragraph\">2. <strong>GPU not detected</strong>:</p>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Check NVIDIA GPU\nnvidia-smi\n\n# Check CUDA installation\nnvcc --version\n\n# Check Ollama GPU support\nollama info</code></pre>\n      </div>\n    </div>\n<p class=\"mobile-paragraph\">3. <strong>Model download fails</strong>:</p>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Check internet connectivity\ncurl -I https://ollama.com\n\n# Check disk space\ndf -h /var/lib/ollama\n\n# Manual model download\ncurl -L https://huggingface.co/model-url -o model-file</code></pre>\n      </div>\n    </div>\n<p class=\"mobile-paragraph\">4. <strong>High memory usage</strong>:</p>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Check model memory usage\nollama ps\n\n# Reduce loaded models\nexport OLLAMA_MAX_LOADED_MODELS=1\n\n# Monitor system resources\nhtop</code></pre>\n      </div>\n    </div>\n<h3 id=\"debug-mode\" class=\"mobile-header\">Debug Mode</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Enable debug logging\nexport OLLAMA_DEBUG=1\nollama serve\n\n# Verbose API logging\nexport OLLAMA_VERBOSE=1</code></pre>\n      </div>\n    </div>\n<h2 id=\"7-security-considerations\" class=\"mobile-header\">7. Security Considerations</h2>\n<h3 id=\"network-security\" class=\"mobile-header\">Network Security</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Bind to localhost only (default)\nexport OLLAMA_HOST=127.0.0.1:11434\n\n# Configure firewall (if exposing externally)\nsudo firewall-cmd --permanent --add-port=11434/tcp\nsudo firewall-cmd --reload\n\n# Use reverse proxy for external access</code></pre>\n      </div>\n    </div>\n<h3 id=\"reverse-proxy-configuration-nginx\" class=\"mobile-header\">Reverse Proxy Configuration (nginx)</h3>\n<div class=\"mobile-code-block\" data-language=\"nginx\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">nginx</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-nginx\">server {\n    listen 80;\n    server_name ollama.example.com;\n    \n    location / {\n        proxy_pass http://127.0.0.1:11434;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}</code></pre>\n      </div>\n    </div>\n<h3 id=\"authentication-setup\" class=\"mobile-header\">Authentication Setup</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Ollama doesn&#039;t have built-in auth, use reverse proxy\n# Example with basic auth in nginx:\nsudo apt install apache2-utils\nsudo htpasswd -c /etc/nginx/.htpasswd ollama_user\n\n# Add to nginx config:\n# auth_basic &quot;Ollama Access&quot;;\n# auth_basic_user_file /etc/nginx/.htpasswd;</code></pre>\n      </div>\n    </div>\n<h3 id=\"file-permissions\" class=\"mobile-header\">File Permissions</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Secure model directory\nsudo chown -R ollama:ollama /var/lib/ollama\nsudo chmod -R 750 /var/lib/ollama\n\n# Secure configuration files\nsudo chmod 640 /etc/systemd/system/ollama.service\nsudo chown root:root /etc/systemd/system/ollama.service</code></pre>\n      </div>\n    </div>\n<h2 id=\"8-performance-tuning\" class=\"mobile-header\">8. Performance Tuning</h2>\n<h3 id=\"gpu-optimization\" class=\"mobile-header\">GPU Optimization</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># NVIDIA GPU settings\nexport CUDA_VISIBLE_DEVICES=0,1\nexport OLLAMA_GPU_OVERHEAD=0\n\n# Check GPU utilization\nnvidia-smi -l 1\n\n# AMD GPU (ROCm)\nexport HSA_OVERRIDE_GFX_VERSION=10.3.0\nexport ROCM_PATH=/opt/rocm</code></pre>\n      </div>\n    </div>\n<h3 id=\"cpu-optimization\" class=\"mobile-header\">CPU Optimization</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Set CPU affinity\ntaskset -c 0-7 ollama serve\n\n# Adjust parallel processing\nexport OLLAMA_NUM_PARALLEL=4\nexport OLLAMA_MAX_LOADED_MODELS=2\n\n# Enable optimizations\nexport OLLAMA_FLASH_ATTENTION=1\nexport OLLAMA_NUMA_PREFER=0</code></pre>\n      </div>\n    </div>\n<h3 id=\"memory-management\" class=\"mobile-header\">Memory Management</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Monitor memory usage\nwatch -n 1 &#039;free -h &amp;&amp; echo &quot;=== Ollama Process ===&quot; &amp;&amp; ps aux | grep ollama&#039;\n\n# Limit model cache\nexport OLLAMA_MAX_LOADED_MODELS=1\n\n# Use swap if needed (not recommended for production)\nsudo swapon --show</code></pre>\n      </div>\n    </div>\n<h3 id=\"storage-optimization\" class=\"mobile-header\">Storage Optimization</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Use SSD for models\nsudo mkdir -p /mnt/ssd/ollama/models\nsudo chown ollama:ollama /mnt/ssd/ollama/models\nexport OLLAMA_MODELS=/mnt/ssd/ollama/models\n\n# Clean up unused models\nollama list | grep -v &quot;NAME&quot; | awk &#039;{print $1}&#039; | xargs ollama rm</code></pre>\n      </div>\n    </div>\n<h2 id=\"9-backup-and-restore\" class=\"mobile-header\">9. Backup and Restore</h2>\n<h3 id=\"model-backup\" class=\"mobile-header\">Model Backup</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\">#!/bin/bash\n# backup-ollama-models.sh\n\nBACKUP_DIR=&quot;/var/backups/ollama&quot;\nMODELS_DIR=&quot;/var/lib/ollama/models&quot;\nDATE=$(date +%Y%m%d_%H%M%S)\n\n# Create backup directory\nmkdir -p $BACKUP_DIR\n\n# Backup models directory\ntar -czf $BACKUP_DIR/ollama_models_$DATE.tar.gz -C /var/lib/ollama models\n\n# Backup model list\nollama list &gt; $BACKUP_DIR/ollama_models_list_$DATE.txt\n\necho &quot;Backup completed: $BACKUP_DIR/ollama_models_$DATE.tar.gz&quot;</code></pre>\n      </div>\n    </div>\n<h3 id=\"configuration-backup\" class=\"mobile-header\">Configuration Backup</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\">#!/bin/bash\n# backup-ollama-config.sh\n\nBACKUP_DIR=&quot;/var/backups/ollama&quot;\nDATE=$(date +%Y%m%d_%H%M%S)\n\n# Backup configuration\ntar -czf $BACKUP_DIR/ollama_config_$DATE.tar.gz \\\n    /etc/systemd/system/ollama.service \\\n    /etc/systemd/system/ollama.service.d/ 2&gt;/dev/null || true\n\necho &quot;Configuration backup: $BACKUP_DIR/ollama_config_$DATE.tar.gz&quot;</code></pre>\n      </div>\n    </div>\n<h3 id=\"restore-procedures\" class=\"mobile-header\">Restore Procedures</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Restore models\nsudo systemctl stop ollama\nsudo tar -xzf ollama_models_backup.tar.gz -C /var/lib/ollama\nsudo chown -R ollama:ollama /var/lib/ollama/models\nsudo systemctl start ollama\n\n# Verify restored models\nollama list</code></pre>\n      </div>\n    </div>\n<h3 id=\"automated-backup\" class=\"mobile-header\">Automated Backup</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Add to crontab\nsudo crontab -e\n\n# Daily model backup at 2 AM\n0 2 * * * /opt/ollama/scripts/backup-ollama-models.sh\n\n# Weekly configuration backup\n0 3 * * 0 /opt/ollama/scripts/backup-ollama-config.sh</code></pre>\n      </div>\n    </div>\n<h2 id=\"10-system-requirements\" class=\"mobile-header\">10. System Requirements</h2>\n<h3 id=\"minimum-requirements\" class=\"mobile-header\">Minimum Requirements</h3>\n<li class=\"mobile-list-item\"><strong>CPU</strong>: 2 cores, 2.0 GHz</li>\n<li class=\"mobile-list-item\"><strong>RAM</strong>: 8GB</li>\n<li class=\"mobile-list-item\"><strong>Storage</strong>: 20GB (small models)</li>\n<li class=\"mobile-list-item\"><strong>Network</strong>: Broadband for model downloads</li>\n<h3 id=\"recommended-requirements\" class=\"mobile-header\">Recommended Requirements</h3>\n<li class=\"mobile-list-item\"><strong>CPU</strong>: 8+ cores, 3.0+ GHz</li>\n<li class=\"mobile-list-item\"><strong>RAM</strong>: 32GB+</li>\n<li class=\"mobile-list-item\"><strong>GPU</strong>: NVIDIA RTX 3060+ or AMD RX 6600 XT+</li>\n<li class=\"mobile-list-item\"><strong>Storage</strong>: 100GB+ SSD/NVMe</li>\n<li class=\"mobile-list-item\"><strong>Network</strong>: Gigabit for large model downloads</li>\n<h3 id=\"model-specific-requirements\" class=\"mobile-header\">Model-Specific Requirements</h3>\n<p class=\"mobile-paragraph\">| Model Size | RAM Required | VRAM Required | Storage |</p>\n<p class=\"mobile-paragraph\">|------------|--------------|---------------|---------|</p>\n<p class=\"mobile-paragraph\">| 7B         | 8GB          | 4GB           | 4GB     |</p>\n<p class=\"mobile-paragraph\">| 13B        | 16GB         | 8GB           | 7GB     |</p>\n<p class=\"mobile-paragraph\">| 30B        | 32GB         | 20GB          | 19GB    |</p>\n<p class=\"mobile-paragraph\">| 70B        | 64GB         | 48GB          | 39GB    |</p>\n<h2 id=\"11-support\" class=\"mobile-header\">11. Support</h2>\n<h3 id=\"official-resources\" class=\"mobile-header\">Official Resources</h3>\n<li class=\"mobile-list-item\"><strong>Website</strong>: https://ollama.com</li>\n<li class=\"mobile-list-item\"><strong>GitHub</strong>: https://github.com/ollama/ollama</li>\n<li class=\"mobile-list-item\"><strong>Documentation</strong>: https://github.com/ollama/ollama/tree/main/docs</li>\n<li class=\"mobile-list-item\"><strong>Model Library</strong>: https://ollama.com/library</li>\n<h3 id=\"community-support\" class=\"mobile-header\">Community Support</h3>\n<li class=\"mobile-list-item\"><strong>Discord</strong>: https://discord.gg/ollama</li>\n<li class=\"mobile-list-item\"><strong>Reddit</strong>: r/ollama</li>\n<li class=\"mobile-list-item\"><strong>GitHub Issues</strong>: https://github.com/ollama/ollama/issues</li>\n<li class=\"mobile-list-item\"><strong>Discussions</strong>: https://github.com/ollama/ollama/discussions</li>\n<h2 id=\"12-contributing\" class=\"mobile-header\">12. Contributing</h2>\n<h3 id=\"how-to-contribute\" class=\"mobile-header\">How to Contribute</h3>\n<p class=\"mobile-paragraph\">1. Fork the repository on GitHub</p>\n<p class=\"mobile-paragraph\">2. Create a feature branch</p>\n<p class=\"mobile-paragraph\">3. Submit pull request</p>\n<p class=\"mobile-paragraph\">4. Follow Go coding standards</p>\n<p class=\"mobile-paragraph\">5. Include tests and documentation</p>\n<h3 id=\"development-setup\" class=\"mobile-header\">Development Setup</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Clone repository\ngit clone https://github.com/ollama/ollama.git\ncd ollama\n\n# Install Go dependencies\ngo mod tidy\n\n# Build from source\ngo build .\n\n# Run tests\ngo test ./...</code></pre>\n      </div>\n    </div>\n<h2 id=\"13-license\" class=\"mobile-header\">13. License</h2>\n<p class=\"mobile-paragraph\">Ollama is licensed under the MIT License.</p>\n<p class=\"mobile-paragraph\">Key points:</p>\n<li class=\"mobile-list-item\">Free to use, modify, and distribute</li>\n<li class=\"mobile-list-item\">Commercial use allowed</li>\n<li class=\"mobile-list-item\">No warranty provided</li>\n<li class=\"mobile-list-item\">Attribution required</li>\n<h2 id=\"14-acknowledgments\" class=\"mobile-header\">14. Acknowledgments</h2>\n<h3 id=\"credits\" class=\"mobile-header\">Credits</h3>\n<li class=\"mobile-list-item\"><strong>Ollama Team</strong>: Core development team</li>\n<li class=\"mobile-list-item\"><strong>Meta AI</strong>: Llama model family</li>\n<li class=\"mobile-list-item\"><strong>Mistral AI</strong>: Mistral models</li>\n<li class=\"mobile-list-item\"><strong>Community</strong>: Model creators and contributors</li>\n<li class=\"mobile-list-item\"><strong>Hardware Vendors</strong>: NVIDIA, AMD, Apple for acceleration support</li>\n<h2 id=\"15-version-history\" class=\"mobile-header\">15. Version History</h2>\n<h3 id=\"recent-releases\" class=\"mobile-header\">Recent Releases</h3>\n<li class=\"mobile-list-item\"><strong>v0.3.x</strong>: Latest stable with improved performance</li>\n<li class=\"mobile-list-item\"><strong>v0.2.x</strong>: Added model management improvements</li>\n<li class=\"mobile-list-item\"><strong>v0.1.x</strong>: Initial public release</li>\n<h3 id=\"major-features-by-version\" class=\"mobile-header\">Major Features by Version</h3>\n<li class=\"mobile-list-item\"><strong>v0.3.0</strong>: Enhanced GPU support, model compression</li>\n<li class=\"mobile-list-item\"><strong>v0.2.0</strong>: REST API improvements, concurrent requests</li>\n<li class=\"mobile-list-item\"><strong>v0.1.0</strong>: Basic model serving and CLI interface</li>\n<h2 id=\"16-appendices\" class=\"mobile-header\">16. Appendices</h2>\n<h3 id=\"a-api-usage-examples\" class=\"mobile-header\">A. API Usage Examples</h3>\n<p class=\"mobile-paragraph\">#### Basic Chat Completion</p>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\">curl http://localhost:11434/api/generate -d &#039;{\n  &quot;model&quot;: &quot;llama3.1:8b&quot;,\n  &quot;prompt&quot;: &quot;Why is the sky blue?&quot;,\n  &quot;stream&quot;: false\n}&#039;</code></pre>\n      </div>\n    </div>\n<p class=\"mobile-paragraph\">#### Streaming Response</p>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\">curl http://localhost:11434/api/generate -d &#039;{\n  &quot;model&quot;: &quot;llama3.1:8b&quot;,\n  &quot;prompt&quot;: &quot;Write a poem about coding&quot;,\n  &quot;stream&quot;: true\n}&#039;</code></pre>\n      </div>\n    </div>\n<p class=\"mobile-paragraph\">#### Chat API</p>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\">curl http://localhost:11434/api/chat -d &#039;{\n  &quot;model&quot;: &quot;llama3.1:8b&quot;,\n  &quot;messages&quot;: [\n    {\n      &quot;role&quot;: &quot;user&quot;,\n      &quot;content&quot;: &quot;Hello, how are you?&quot;\n    }\n  ]\n}&#039;</code></pre>\n      </div>\n    </div>\n<h3 id=\"b-integration-examples\" class=\"mobile-header\">B. Integration Examples</h3>\n<p class=\"mobile-paragraph\">#### Python Integration</p>\n<div class=\"mobile-code-block\" data-language=\"python\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">python</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-python\">import requests\nimport json\n\ndef chat_with_ollama(prompt, model=&quot;llama3.1:8b&quot;):\n    url = &quot;http://localhost:11434/api/generate&quot;\n    data = {\n        &quot;model&quot;: model,\n        &quot;prompt&quot;: prompt,\n        &quot;stream&quot;: False\n    }\n    \n    response = requests.post(url, json=data)\n    if response.status_code == 200:\n        return response.json()[&quot;response&quot;]\n    else:\n        return &quot;Error: &quot; + str(response.status_code)\n\n# Usage\nresponse = chat_with_ollama(&quot;Explain quantum computing&quot;)\nprint(response)</code></pre>\n      </div>\n    </div>\n<p class=\"mobile-paragraph\">#### Node.js Integration</p>\n<div class=\"mobile-code-block\" data-language=\"javascript\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">javascript</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-javascript\">const axios = require(&#039;axios&#039;);\n\nasync function chatWithOllama(prompt, model = &#039;llama3.1:8b&#039;) {\n    try {\n        const response = await axios.post(&#039;http://localhost:11434/api/generate&#039;, {\n            model: model,\n            prompt: prompt,\n            stream: false\n        });\n        \n        return response.data.response;\n    } catch (error) {\n        console.error(&#039;Error:&#039;, error.message);\n        return null;\n    }\n}\n\n// Usage\nchatWithOllama(&#039;What is machine learning?&#039;).then(response =&gt; {\n    console.log(response);\n});</code></pre>\n      </div>\n    </div>\n<h3 id=\"c-model-customization\" class=\"mobile-header\">C. Model Customization</h3>\n<p class=\"mobile-paragraph\">#### Creating Custom Models</p>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Create Modelfile\ncat &gt; Modelfile &lt;&lt; &#039;EOF&#039;\nFROM llama3.1:8b\n\n# Set parameters\nPARAMETER temperature 0.7\nPARAMETER top_p 0.9\n\n# Set system message\nSYSTEM &quot;&quot;&quot;\nYou are a helpful AI assistant specialized in programming.\nAlways provide code examples when relevant.\n&quot;&quot;&quot;\nEOF\n\n# Build custom model\nollama create my-coding-assistant -f Modelfile\n\n# Test custom model\nollama run my-coding-assistant &quot;How do I sort a list in Python?&quot;</code></pre>\n      </div>\n    </div>\n<p class=\"mobile-paragraph\">#### Fine-tuning (Advanced)</p>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\"># Prepare training data (JSONL format)\ncat &gt; training_data.jsonl &lt;&lt; &#039;EOF&#039;\n{&quot;prompt&quot;: &quot;Question: What is Python?&quot;, &quot;completion&quot;: &quot;Python is a programming language...&quot;}\n{&quot;prompt&quot;: &quot;Question: How to install packages?&quot;, &quot;completion&quot;: &quot;Use pip install package_name...&quot;}\nEOF\n\n# Note: Fine-tuning requires additional tools and setup\n# Refer to Ollama documentation for detailed fine-tuning guide</code></pre>\n      </div>\n    </div>\n<h3 id=\"d-performance-monitoring\" class=\"mobile-header\">D. Performance Monitoring</h3>\n<div class=\"mobile-code-block\" data-language=\"bash\">\n      <div class=\"mobile-code-header\">\n        <span class=\"mobile-code-language\">bash</span>\n        <button class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\">\n          <span class=\"copy-icon\">ðŸ“‹</span>\n          <span class=\"copy-text\">Copy</span>\n        </button>\n      </div>\n      <div class=\"mobile-code-content\">\n        <pre><code class=\"language-bash\">#!/bin/bash\n# monitor-ollama.sh\n\necho &quot;=== Ollama Service Status ===&quot;\nsystemctl status ollama --no-pager\n\necho -e &quot;\\n=== Memory Usage ===&quot;\nps aux | grep ollama | grep -v grep\n\necho -e &quot;\\n=== GPU Usage ===&quot;\nif command -v nvidia-smi &amp;&gt; /dev/null; then\n    nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits\nfi\n\necho -e &quot;\\n=== API Health Check ===&quot;\ncurl -s http://localhost:11434/api/version || echo &quot;API not responding&quot;\n\necho -e &quot;\\n=== Loaded Models ===&quot;\nollama ps\n\necho -e &quot;\\n=== Disk Usage ===&quot;\ndu -sh /var/lib/ollama/models/*</code></pre>\n      </div>\n    </div>\n<p class=\"mobile-paragraph\">---</p>\n<p class=\"mobile-paragraph\">For more information and updates, visit https://github.com/howtomgr/ollama</p>","readTime":"12 min","wordCount":2213,"tableOfContents":[{"level":2,"text":"1. Prerequisites","id":"1-prerequisites"},{"level":3,"text":"Hardware Requirements","id":"hardware-requirements"},{"level":3,"text":"Software Requirements","id":"software-requirements"},{"level":3,"text":"Network Requirements","id":"network-requirements"},{"level":2,"text":"2. Supported Operating Systems","id":"2-supported-operating-systems"},{"level":2,"text":"3. Installation","id":"3-installation"},{"level":3,"text":"RHEL/CentOS/Rocky Linux/AlmaLinux","id":"rhelcentosrocky-linuxalmalinux"},{"level":3,"text":"Debian/Ubuntu","id":"debianubuntu"},{"level":3,"text":"Arch Linux","id":"arch-linux"},{"level":3,"text":"Alpine Linux","id":"alpine-linux"},{"level":3,"text":"openSUSE","id":"opensuse"},{"level":3,"text":"macOS","id":"macos"},{"level":3,"text":"Windows","id":"windows"},{"level":2,"text":"4. Configuration","id":"4-configuration"},{"level":3,"text":"Environment Variables","id":"environment-variables"},{"level":3,"text":"Configuration Options","id":"configuration-options"},{"level":3,"text":"Model Management","id":"model-management"},{"level":2,"text":"5. Service Management","id":"5-service-management"},{"level":3,"text":"systemd (Linux)","id":"systemd-linux"},{"level":3,"text":"Manual Service Management","id":"manual-service-management"},{"level":3,"text":"Windows Service Management","id":"windows-service-management"},{"level":2,"text":"6. Troubleshooting","id":"6-troubleshooting"},{"level":3,"text":"Common Issues","id":"common-issues"},{"level":3,"text":"Debug Mode","id":"debug-mode"},{"level":2,"text":"7. Security Considerations","id":"7-security-considerations"},{"level":3,"text":"Network Security","id":"network-security"},{"level":3,"text":"Reverse Proxy Configuration (nginx)","id":"reverse-proxy-configuration-nginx"},{"level":3,"text":"Authentication Setup","id":"authentication-setup"},{"level":3,"text":"File Permissions","id":"file-permissions"},{"level":2,"text":"8. Performance Tuning","id":"8-performance-tuning"},{"level":3,"text":"GPU Optimization","id":"gpu-optimization"},{"level":3,"text":"CPU Optimization","id":"cpu-optimization"},{"level":3,"text":"Memory Management","id":"memory-management"},{"level":3,"text":"Storage Optimization","id":"storage-optimization"},{"level":2,"text":"9. Backup and Restore","id":"9-backup-and-restore"},{"level":3,"text":"Model Backup","id":"model-backup"},{"level":3,"text":"Configuration Backup","id":"configuration-backup"},{"level":3,"text":"Restore Procedures","id":"restore-procedures"},{"level":3,"text":"Automated Backup","id":"automated-backup"},{"level":2,"text":"10. System Requirements","id":"10-system-requirements"},{"level":3,"text":"Minimum Requirements","id":"minimum-requirements"},{"level":3,"text":"Recommended Requirements","id":"recommended-requirements"},{"level":3,"text":"Model-Specific Requirements","id":"model-specific-requirements"},{"level":2,"text":"11. Support","id":"11-support"},{"level":3,"text":"Official Resources","id":"official-resources"},{"level":3,"text":"Community Support","id":"community-support"},{"level":2,"text":"12. Contributing","id":"12-contributing"},{"level":3,"text":"How to Contribute","id":"how-to-contribute"},{"level":3,"text":"Development Setup","id":"development-setup"},{"level":2,"text":"13. License","id":"13-license"},{"level":2,"text":"14. Acknowledgments","id":"14-acknowledgments"},{"level":3,"text":"Credits","id":"credits"},{"level":2,"text":"15. Version History","id":"15-version-history"},{"level":3,"text":"Recent Releases","id":"recent-releases"},{"level":3,"text":"Major Features by Version","id":"major-features-by-version"},{"level":2,"text":"16. Appendices","id":"16-appendices"},{"level":3,"text":"A. API Usage Examples","id":"a-api-usage-examples"},{"level":3,"text":"B. Integration Examples","id":"b-integration-examples"},{"level":3,"text":"C. Model Customization","id":"c-model-customization"},{"level":3,"text":"D. Performance Monitoring","id":"d-performance-monitoring"}],"lastBuilt":"2025-10-19T06:16:14.514Z","metadataVersion":"2.0"}]},"__N_SSG":true}