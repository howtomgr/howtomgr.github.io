<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, user-scalable=yes" data-next-head=""/><meta name="theme-color" content="#bd93f9" data-next-head=""/><link rel="icon" href="/favicon.ico" data-next-head=""/><link rel="preconnect" href="https://fonts.googleapis.com" data-next-head=""/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="true" data-next-head=""/><title class="jsx-6ca0ba86c5b4bb40" data-next-head="">Ollama Installation Guide Installation Guide - HowToMgr</title><meta name="description" content="Ollama is a free and open-source tool for running large language models (LLMs) locally on your machine. It serves as a FOSS alternative to cloud-based AI services like OpenAI API, Anthropic Claude API, or Google&#x27;s Gemini API, enabling privacy-focused AI deployment, offline inference, and cost-effective local AI processing." class="jsx-6ca0ba86c5b4bb40" data-next-head=""/><meta name="keywords" content="installation guides, tutorials, linux, docker, kubernetes, mobile-first, responsive" class="jsx-6ca0ba86c5b4bb40" data-next-head=""/><meta property="og:title" content="Ollama Installation Guide Installation Guide - HowToMgr" class="jsx-6ca0ba86c5b4bb40" data-next-head=""/><meta property="og:description" content="Ollama is a free and open-source tool for running large language models (LLMs) locally on your machine. It serves as a FOSS alternative to cloud-based AI services like OpenAI API, Anthropic Claude API, or Google&#x27;s Gemini API, enabling privacy-focused AI deployment, offline inference, and cost-effective local AI processing." class="jsx-6ca0ba86c5b4bb40" data-next-head=""/><meta property="og:type" content="website" class="jsx-6ca0ba86c5b4bb40" data-next-head=""/><meta property="og:url" content="https://howtomgr.github.io" class="jsx-6ca0ba86c5b4bb40" data-next-head=""/><meta property="og:image" content="https://howtomgr.github.io/social-preview.png" class="jsx-6ca0ba86c5b4bb40" data-next-head=""/><meta property="twitter:card" content="summary_large_image" class="jsx-6ca0ba86c5b4bb40" data-next-head=""/><meta property="twitter:title" content="Ollama Installation Guide Installation Guide - HowToMgr" class="jsx-6ca0ba86c5b4bb40" data-next-head=""/><meta property="twitter:description" content="Ollama is a free and open-source tool for running large language models (LLMs) locally on your machine. It serves as a FOSS alternative to cloud-based AI services like OpenAI API, Anthropic Claude API, or Google&#x27;s Gemini API, enabling privacy-focused AI deployment, offline inference, and cost-effective local AI processing." class="jsx-6ca0ba86c5b4bb40" data-next-head=""/><link rel="dns-prefetch" href="//api.github.com"/><link rel="dns-prefetch" href="//github.com"/><link rel="preload" href="/favicon.ico" as="image"/><meta name="application-name" content="HowToMgr"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="default"/><meta name="apple-mobile-web-app-title" content="HowToMgr"/><meta name="format-detection" content="telephone=no"/><meta name="mobile-web-app-capable" content="yes"/><noscript><style>
            .requires-js { display: none !important; }
            .no-js-message { display: block !important; }
          </style></noscript><link rel="preload" href="/_next/static/css/8f63910774442189.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8f63910774442189.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-8cac0b4b405cede1.js" defer=""></script><script src="/_next/static/chunks/framework-a6e0b7e30f98059a.js" defer=""></script><script src="/_next/static/chunks/main-453f16463ca269d8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-2c7289aecf732a54.js" defer=""></script><script src="/_next/static/chunks/473-0e819be9d34ec59b.js" defer=""></script><script src="/_next/static/chunks/pages/%5Bcategory%5D/%5Bguide%5D-be5cb82ed6bf6435.js" defer=""></script><script src="/_next/static/rRdSpB4o7Zzz13PZ1FjN4/_buildManifest.js" defer=""></script><script src="/_next/static/rRdSpB4o7Zzz13PZ1FjN4/_ssgManifest.js" defer=""></script><style id="__jsx-64efd206b36bc2dd">.breadcrumb.jsx-64efd206b36bc2dd{display:flex;align-items:center;gap:var(--space-2);margin-bottom:var(--space-6);font-size:var(--font-sm);color:var(--text-secondary);overflow-x:auto;white-space:nowrap;-webkit-overflow-scrolling:touch}.breadcrumb.jsx-64efd206b36bc2dd a.jsx-64efd206b36bc2dd{color:var(--accent-primary);text-decoration:none;flex-shrink:0}.breadcrumb.jsx-64efd206b36bc2dd a.jsx-64efd206b36bc2dd:hover{text-decoration:underline}.separator.jsx-64efd206b36bc2dd{color:var(--text-muted);flex-shrink:0}.current.jsx-64efd206b36bc2dd{color:var(--text-primary);font-weight:500;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}.guide-header.jsx-64efd206b36bc2dd{background:var(--bg-secondary);border-radius:var(--border-radius-lg);padding:var(--space-6);margin-bottom:var(--space-8);border:1px solid var(--bg-surface)}.guide-title.jsx-64efd206b36bc2dd{color:var(--accent-primary);font-size:var(--font-3xl);margin-bottom:var(--space-4);line-height:1.2;word-break:break-word}.guide-description.jsx-64efd206b36bc2dd{color:var(--text-secondary);font-size:var(--font-lg);margin-bottom:var(--space-5);line-height:1.5}.guide-badges.jsx-64efd206b36bc2dd{display:flex;gap:var(--space-3);margin-bottom:var(--space-5);flex-wrap:wrap}.guide-actions.jsx-64efd206b36bc2dd{display:flex;gap:var(--space-3);flex-wrap:wrap}.difficulty-beginner.jsx-64efd206b36bc2dd{background:var(--accent-secondary);color:var(--bg-primary)}.difficulty-intermediate.jsx-64efd206b36bc2dd{background:var(--accent-warning);color:var(--bg-primary)}.difficulty-advanced.jsx-64efd206b36bc2dd{background:var(--accent-error);color:var(--bg-primary)}.guide-content.jsx-64efd206b36bc2dd{background:var(--bg-secondary);border-radius:var(--border-radius-lg);padding:var(--space-6);margin-bottom:var(--space-8);border:1px solid var(--bg-surface);overflow-x:hidden}.related-section.jsx-64efd206b36bc2dd{margin-bottom:var(--space-8)}.section-title.jsx-64efd206b36bc2dd{color:var(--accent-primary);font-size:var(--font-2xl);text-align:center;margin-bottom:var(--space-6)}.guide-navigation.jsx-64efd206b36bc2dd{display:flex;gap:var(--space-4);justify-content:center;flex-wrap:wrap;margin-bottom:var(--space-8)}.loading-container.jsx-64efd206b36bc2dd,.error-container.jsx-64efd206b36bc2dd{text-align:center;padding:var(--space-8)}.error-actions.jsx-64efd206b36bc2dd{display:flex;gap:var(--space-4);justify-content:center;margin-top:var(--space-6);flex-wrap:wrap}.toc-level-1.jsx-64efd206b36bc2dd{font-weight:600}.toc-level-2.jsx-64efd206b36bc2dd{margin-left:var(--space-4)}.toc-level-3.jsx-64efd206b36bc2dd{margin-left:var(--space-8);font-size:var(--font-xs)}@media(max-width:767px){.guide-header.jsx-64efd206b36bc2dd{padding:var(--space-4);margin-bottom:var(--space-6)}.guide-title.jsx-64efd206b36bc2dd{font-size:var(--font-2xl)}.guide-description.jsx-64efd206b36bc2dd{font-size:var(--font-base)}.guide-content.jsx-64efd206b36bc2dd{padding:var(--space-4);margin-bottom:var(--space-6)}.guide-badges.jsx-64efd206b36bc2dd{gap:var(--space-2)}.guide-actions.jsx-64efd206b36bc2dd{flex-direction:column}.guide-navigation.jsx-64efd206b36bc2dd{flex-direction:column}.breadcrumb.jsx-64efd206b36bc2dd{margin-bottom:var(--space-4)}}</style><style id="__jsx-6ca0ba86c5b4bb40">.app-container.jsx-6ca0ba86c5b4bb40{min-height:100vh;display:flex;flex-direction:column;padding-top:56px}.main-content.jsx-6ca0ba86c5b4bb40{flex:1;padding:var(--space-6)0}.mobile-footer.jsx-6ca0ba86c5b4bb40{background:var(--bg-secondary);border-top:1px solid var(--bg-surface);padding:var(--space-6)0;margin-top:auto}.footer-content.jsx-6ca0ba86c5b4bb40{text-align:center}.footer-links.jsx-6ca0ba86c5b4bb40{display:flex;justify-content:center;gap:var(--space-6);margin-bottom:var(--space-4);flex-wrap:wrap}.footer-links.jsx-6ca0ba86c5b4bb40 a.jsx-6ca0ba86c5b4bb40{color:var(--accent-primary);text-decoration:none;font-size:var(--font-sm)}.footer-links.jsx-6ca0ba86c5b4bb40 a.jsx-6ca0ba86c5b4bb40:hover{color:var(--accent-secondary)}.footer-separator.jsx-6ca0ba86c5b4bb40{height:1px;background:var(--bg-surface);margin:var(--space-4)auto;width:200px}.footer-text.jsx-6ca0ba86c5b4bb40,.footer-tech.jsx-6ca0ba86c5b4bb40{color:var(--text-secondary);font-size:var(--font-sm);margin-bottom:var(--space-2)}.footer-disclaimer.jsx-6ca0ba86c5b4bb40{color:var(--text-muted);font-size:var(--font-xs)}.footer-updated.jsx-6ca0ba86c5b4bb40{color:var(--text-muted);font-size:var(--font-xs);margin-top:var(--space-2)}.nav-links.jsx-6ca0ba86c5b4bb40{display:flex;gap:var(--space-4)}.nav-link.jsx-6ca0ba86c5b4bb40{color:var(--text-primary);text-decoration:none;padding:var(--space-2)var(--space-3);border-radius:var(--border-radius);font-size:var(--font-sm);font-weight:500;transition:var(--transition);min-height:36px;display:flex;align-items:center}.nav-link.jsx-6ca0ba86c5b4bb40:hover{background:var(--bg-surface);color:var(--accent-primary);text-decoration:none}[data-theme="light"].jsx-6ca0ba86c5b4bb40 .theme-icon-dark.jsx-6ca0ba86c5b4bb40{display:none}[data-theme="light"].jsx-6ca0ba86c5b4bb40 .theme-icon-light.jsx-6ca0ba86c5b4bb40{display:inline!important}@media(max-width:767px){.main-content.jsx-6ca0ba86c5b4bb40{padding:var(--space-4)0}.footer-links.jsx-6ca0ba86c5b4bb40{gap:var(--space-4)}}</style></head><body><noscript><div class="no-js-message" style="padding:1rem;background:#ff5555;color:#f8f8f2;text-align:center;font-size:0.9rem">This site requires JavaScript to function properly. Please enable JavaScript in your browser.</div></noscript><div id="__next"><div class="jsx-6ca0ba86c5b4bb40 app-container"><a href="#main-content" class="jsx-6ca0ba86c5b4bb40 skip-link">Skip to main content</a><nav class="jsx-6ca0ba86c5b4bb40 mobile-nav"><div class="jsx-6ca0ba86c5b4bb40 mobile-nav-content"><a class="mobile-brand" href="/"><span role="img" aria-label="Books" class="jsx-6ca0ba86c5b4bb40">üìö</span>HowToMgr</a><div class="jsx-6ca0ba86c5b4bb40 nav-links"><a class="nav-link" href="/all/">All</a><a class="nav-link" href="/search/">Search</a></div><button aria-label="Toggle theme" title="Switch theme" class="jsx-6ca0ba86c5b4bb40 mobile-theme-toggle touch-target"><span class="jsx-6ca0ba86c5b4bb40 theme-icon-dark">üåô</span><span style="display:none" class="jsx-6ca0ba86c5b4bb40 theme-icon-light">‚òÄÔ∏è</span></button></div></nav><main id="main-content" tabindex="-1" class="jsx-6ca0ba86c5b4bb40 main-content"><div class="jsx-6ca0ba86c5b4bb40 container"><nav aria-label="Breadcrumb" class="jsx-64efd206b36bc2dd breadcrumb"><a href="/">Home</a><span class="jsx-64efd206b36bc2dd separator">‚Üí</span><a href="/artificial-intelligence/">Artificial intelligence</a><span class="jsx-64efd206b36bc2dd separator">‚Üí</span><span class="jsx-64efd206b36bc2dd current">Ollama Installation Guide</span></nav><header class="jsx-64efd206b36bc2dd guide-header"><div class="jsx-64efd206b36bc2dd guide-meta-top"><h1 class="jsx-64efd206b36bc2dd guide-title">Ollama Installation Guide</h1><p class="jsx-64efd206b36bc2dd guide-description">Ollama is a free and open-source tool for running large language models (LLMs) locally on your machine. It serves as a FOSS alternative to cloud-based AI services like OpenAI API, Anthropic Claude API, or Google&#x27;s Gemini API, enabling privacy-focused AI deployment, offline inference, and cost-effective local AI processing.</p><div class="jsx-64efd206b36bc2dd guide-badges"><span class="jsx-64efd206b36bc2dd mobile-badge mobile-badge-primary">Artificial intelligence</span><span class="jsx-64efd206b36bc2dd mobile-badge difficulty-intermediate">üü°<!-- --> <!-- -->intermediate</span><span class="jsx-64efd206b36bc2dd mobile-badge mobile-badge-info">12 min</span><span class="jsx-64efd206b36bc2dd mobile-badge mobile-badge-secondary">‚è±Ô∏è <!-- -->15-30 minutes</span></div><div class="jsx-64efd206b36bc2dd guide-actions"><a href="https://github.com/howtomgr/ollama" target="_blank" rel="noopener" class="jsx-64efd206b36bc2dd btn btn-secondary">View on GitHub</a></div></div></header><article class="jsx-64efd206b36bc2dd guide-content"><nav class="jsx-64efd206b36bc2dd toc"><h4 class="jsx-64efd206b36bc2dd">Table of Contents</h4><ul class="jsx-64efd206b36bc2dd"><li class="jsx-64efd206b36bc2dd toc-level-2"><a href="#1-prerequisites" class="jsx-64efd206b36bc2dd">1. Prerequisites</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#hardware-requirements" class="jsx-64efd206b36bc2dd">Hardware Requirements</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#software-requirements" class="jsx-64efd206b36bc2dd">Software Requirements</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#network-requirements" class="jsx-64efd206b36bc2dd">Network Requirements</a></li><li class="jsx-64efd206b36bc2dd toc-level-2"><a href="#2-supported-operating-systems" class="jsx-64efd206b36bc2dd">2. Supported Operating Systems</a></li><li class="jsx-64efd206b36bc2dd toc-level-2"><a href="#3-installation" class="jsx-64efd206b36bc2dd">3. Installation</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#rhelcentosrocky-linuxalmalinux" class="jsx-64efd206b36bc2dd">RHEL/CentOS/Rocky Linux/AlmaLinux</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#debianubuntu" class="jsx-64efd206b36bc2dd">Debian/Ubuntu</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#arch-linux" class="jsx-64efd206b36bc2dd">Arch Linux</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#alpine-linux" class="jsx-64efd206b36bc2dd">Alpine Linux</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#opensuse" class="jsx-64efd206b36bc2dd">openSUSE</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#macos" class="jsx-64efd206b36bc2dd">macOS</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#windows" class="jsx-64efd206b36bc2dd">Windows</a></li><li class="jsx-64efd206b36bc2dd toc-level-2"><a href="#4-configuration" class="jsx-64efd206b36bc2dd">4. Configuration</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#environment-variables" class="jsx-64efd206b36bc2dd">Environment Variables</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#configuration-options" class="jsx-64efd206b36bc2dd">Configuration Options</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#model-management" class="jsx-64efd206b36bc2dd">Model Management</a></li><li class="jsx-64efd206b36bc2dd toc-level-2"><a href="#5-service-management" class="jsx-64efd206b36bc2dd">5. Service Management</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#systemd-linux" class="jsx-64efd206b36bc2dd">systemd (Linux)</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#manual-service-management" class="jsx-64efd206b36bc2dd">Manual Service Management</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#windows-service-management" class="jsx-64efd206b36bc2dd">Windows Service Management</a></li><li class="jsx-64efd206b36bc2dd toc-level-2"><a href="#6-troubleshooting" class="jsx-64efd206b36bc2dd">6. Troubleshooting</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#common-issues" class="jsx-64efd206b36bc2dd">Common Issues</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#debug-mode" class="jsx-64efd206b36bc2dd">Debug Mode</a></li><li class="jsx-64efd206b36bc2dd toc-level-2"><a href="#7-security-considerations" class="jsx-64efd206b36bc2dd">7. Security Considerations</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#network-security" class="jsx-64efd206b36bc2dd">Network Security</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#reverse-proxy-configuration-nginx" class="jsx-64efd206b36bc2dd">Reverse Proxy Configuration (nginx)</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#authentication-setup" class="jsx-64efd206b36bc2dd">Authentication Setup</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#file-permissions" class="jsx-64efd206b36bc2dd">File Permissions</a></li><li class="jsx-64efd206b36bc2dd toc-level-2"><a href="#8-performance-tuning" class="jsx-64efd206b36bc2dd">8. Performance Tuning</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#gpu-optimization" class="jsx-64efd206b36bc2dd">GPU Optimization</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#cpu-optimization" class="jsx-64efd206b36bc2dd">CPU Optimization</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#memory-management" class="jsx-64efd206b36bc2dd">Memory Management</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#storage-optimization" class="jsx-64efd206b36bc2dd">Storage Optimization</a></li><li class="jsx-64efd206b36bc2dd toc-level-2"><a href="#9-backup-and-restore" class="jsx-64efd206b36bc2dd">9. Backup and Restore</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#model-backup" class="jsx-64efd206b36bc2dd">Model Backup</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#configuration-backup" class="jsx-64efd206b36bc2dd">Configuration Backup</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#restore-procedures" class="jsx-64efd206b36bc2dd">Restore Procedures</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#automated-backup" class="jsx-64efd206b36bc2dd">Automated Backup</a></li><li class="jsx-64efd206b36bc2dd toc-level-2"><a href="#10-system-requirements" class="jsx-64efd206b36bc2dd">10. System Requirements</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#minimum-requirements" class="jsx-64efd206b36bc2dd">Minimum Requirements</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#recommended-requirements" class="jsx-64efd206b36bc2dd">Recommended Requirements</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#model-specific-requirements" class="jsx-64efd206b36bc2dd">Model-Specific Requirements</a></li><li class="jsx-64efd206b36bc2dd toc-level-2"><a href="#11-support" class="jsx-64efd206b36bc2dd">11. Support</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#official-resources" class="jsx-64efd206b36bc2dd">Official Resources</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#community-support" class="jsx-64efd206b36bc2dd">Community Support</a></li><li class="jsx-64efd206b36bc2dd toc-level-2"><a href="#12-contributing" class="jsx-64efd206b36bc2dd">12. Contributing</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#how-to-contribute" class="jsx-64efd206b36bc2dd">How to Contribute</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#development-setup" class="jsx-64efd206b36bc2dd">Development Setup</a></li><li class="jsx-64efd206b36bc2dd toc-level-2"><a href="#13-license" class="jsx-64efd206b36bc2dd">13. License</a></li><li class="jsx-64efd206b36bc2dd toc-level-2"><a href="#14-acknowledgments" class="jsx-64efd206b36bc2dd">14. Acknowledgments</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#credits" class="jsx-64efd206b36bc2dd">Credits</a></li><li class="jsx-64efd206b36bc2dd toc-level-2"><a href="#15-version-history" class="jsx-64efd206b36bc2dd">15. Version History</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#recent-releases" class="jsx-64efd206b36bc2dd">Recent Releases</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#major-features-by-version" class="jsx-64efd206b36bc2dd">Major Features by Version</a></li><li class="jsx-64efd206b36bc2dd toc-level-2"><a href="#16-appendices" class="jsx-64efd206b36bc2dd">16. Appendices</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#a-api-usage-examples" class="jsx-64efd206b36bc2dd">A. API Usage Examples</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#b-integration-examples" class="jsx-64efd206b36bc2dd">B. Integration Examples</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#c-model-customization" class="jsx-64efd206b36bc2dd">C. Model Customization</a></li><li class="jsx-64efd206b36bc2dd toc-level-3"><a href="#d-performance-monitoring" class="jsx-64efd206b36bc2dd">D. Performance Monitoring</a></li></ul></nav><div class="jsx-64efd206b36bc2dd readme-content"><p class="mobile-paragraph">Ollama is a free and open-source tool for running large language models (LLMs) locally on your machine. It serves as a FOSS alternative to cloud-based AI services like OpenAI API, Anthropic Claude API, Google's Gemini API, or Azure OpenAI Service. Ollama enables privacy-focused AI deployment, offline inference, and cost-effective local AI processing with support for popular models like Llama 3, Code Llama, Mistral, and many others.</p>
<h2 id="1-prerequisites" class="mobile-header">1. Prerequisites</h2>
<h3 id="hardware-requirements" class="mobile-header">Hardware Requirements</h3>
<li class="mobile-list-item"><strong>CPU</strong>: Modern 64-bit processor (x86_64 or ARM64)</li>
<li class="mobile-list-item"><strong>RAM</strong>: 8GB minimum (16GB+ recommended for larger models)</li>
<li class="mobile-list-item"><strong>Storage</strong>: 10GB+ free space for models</li>
<li class="mobile-list-item"><strong>GPU</strong>: Optional but recommended (NVIDIA with CUDA, AMD ROCm, or Apple Metal)</li>
<h3 id="software-requirements" class="mobile-header">Software Requirements</h3>
<li class="mobile-list-item"><strong>Operating System</strong>: Linux, macOS, or Windows</li>
<li class="mobile-list-item"><strong>Internet</strong>: Required for initial model downloads</li>
<li class="mobile-list-item"><strong>Docker</strong>: Optional for containerized deployment</li>
<h3 id="network-requirements" class="mobile-header">Network Requirements</h3>
<li class="mobile-list-item"><strong>Ports</strong>: </li>
<li class="mobile-list-item">11434: Default API server port</li>
<li class="mobile-list-item"><strong>Bandwidth</strong>: High-speed internet for model downloads (models range from 1GB to 70GB+)</li>
<h2 id="2-supported-operating-systems" class="mobile-header">2. Supported Operating Systems</h2>
<p class="mobile-paragraph">Ollama officially supports:</p>
<li class="mobile-list-item">RHEL 8/9 and derivatives (CentOS Stream, Rocky Linux, AlmaLinux)</li>
<li class="mobile-list-item">Debian 11/12</li>
<li class="mobile-list-item">Ubuntu 20.04 LTS / 22.04 LTS / 24.04 LTS</li>
<li class="mobile-list-item">Arch Linux</li>
<li class="mobile-list-item">Alpine Linux 3.18+</li>
<li class="mobile-list-item">openSUSE Leap 15.5+ / Tumbleweed</li>
<li class="mobile-list-item">macOS 11+ (Big Sur and later)</li>
<li class="mobile-list-item">Windows 10/11</li>
<h2 id="3-installation" class="mobile-header">3. Installation</h2>
<h3 id="rhelcentosrocky-linuxalmalinux" class="mobile-header">RHEL/CentOS/Rocky Linux/AlmaLinux</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Method 1: Official installer script
curl -fsSL https://ollama.com/install.sh | sh

# Method 2: Manual installation
# Download latest release
curl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama
chmod +x /usr/local/bin/ollama

# Create ollama user
sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama
sudo usermod -a -G render,video ollama

# Create systemd service
sudo tee /etc/systemd/system/ollama.service &gt; /dev/null &lt;&lt; &#039;EOF&#039;
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment=&quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;
Environment=&quot;OLLAMA_HOST=0.0.0.0&quot;

[Install]
WantedBy=default.target
EOF

# Enable and start service
sudo systemctl daemon-reload
sudo systemctl enable --now ollama</code></pre>
      </div>
    </div>
<h3 id="debianubuntu" class="mobile-header">Debian/Ubuntu</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Method 1: Official installer script
curl -fsSL https://ollama.com/install.sh | sh

# Method 2: Package installation (if available)
# Add official repository
curl -fsSL https://ollama.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/ollama-keyring.gpg
echo &quot;deb [signed-by=/usr/share/keyrings/ollama-keyring.gpg] https://ollama.com/debian stable main&quot; | sudo tee /etc/apt/sources.list.d/ollama.list

# Install package
sudo apt update
sudo apt install -y ollama

# Start service
sudo systemctl enable --now ollama</code></pre>
      </div>
    </div>
<h3 id="arch-linux" class="mobile-header">Arch Linux</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Install from AUR
yay -S ollama-bin
# or
paru -S ollama-bin

# Alternative: Build from source
yay -S ollama

# Enable and start service
sudo systemctl enable --now ollama</code></pre>
      </div>
    </div>
<h3 id="alpine-linux" class="mobile-header">Alpine Linux</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Install dependencies
apk add --no-cache curl

# Install Ollama binary
curl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama
chmod +x /usr/local/bin/ollama

# Create ollama user
adduser -D -s /bin/false -h /usr/share/ollama ollama
addgroup ollama video
addgroup ollama render

# Create OpenRC service
tee /etc/init.d/ollama &gt; /dev/null &lt;&lt; &#039;EOF&#039;
#!/sbin/openrc-run

description=&quot;Ollama Service&quot;
command=&quot;/usr/local/bin/ollama&quot;
command_args=&quot;serve&quot;
command_user=&quot;ollama&quot;
command_group=&quot;ollama&quot;
pidfile=&quot;/run/ollama.pid&quot;
command_background=&quot;yes&quot;

depend() {
    need net
    after firewall
}

start_pre() {
    export OLLAMA_HOST=&quot;0.0.0.0&quot;
    checkpath --directory --owner ollama:ollama --mode 0755 /run/ollama
}
EOF

chmod +x /etc/init.d/ollama
rc-update add ollama default
rc-service ollama start</code></pre>
      </div>
    </div>
<h3 id="opensuse" class="mobile-header">openSUSE</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Install via zypper (if available) or manual installation
sudo zypper refresh

# Manual installation
curl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama
chmod +x /usr/local/bin/ollama

# Create ollama user
sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama
sudo usermod -a -G video,render ollama

# Create systemd service (same as RHEL)
sudo systemctl enable --now ollama</code></pre>
      </div>
    </div>
<h3 id="macos" class="mobile-header">macOS</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Method 1: Official app installer
# Download from https://ollama.com/download/mac

# Method 2: Homebrew
brew install ollama

# Method 3: Manual installation
curl -L https://ollama.com/download/darwin-amd64 -o /usr/local/bin/ollama
chmod +x /usr/local/bin/ollama

# Start Ollama
ollama serve &amp;</code></pre>
      </div>
    </div>
<h3 id="windows" class="mobile-header">Windows</h3>
<div class="mobile-code-block" data-language="powershell">
      <div class="mobile-code-header">
        <span class="mobile-code-language">powershell</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-powershell"># Method 1: Official installer
# Download and run installer from https://ollama.com/download/windows

# Method 2: Winget
winget install Ollama.Ollama

# Method 3: Chocolatey
choco install ollama

# Method 4: Scoop
scoop bucket add extras
scoop install ollama

# Start Ollama service (automatic with installer)</code></pre>
      </div>
    </div>
<h2 id="4-configuration" class="mobile-header">4. Configuration</h2>
<h3 id="environment-variables" class="mobile-header">Environment Variables</h3>
<p class="mobile-paragraph">Create <code class="mobile-inline-code">/etc/systemd/system/ollama.service.d/override.conf</code>:</p>
<div class="mobile-code-block" data-language="ini">
      <div class="mobile-code-header">
        <span class="mobile-code-language">ini</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-ini">[Service]
Environment=&quot;OLLAMA_HOST=0.0.0.0:11434&quot;
Environment=&quot;OLLAMA_MODELS=/var/lib/ollama/models&quot;
Environment=&quot;OLLAMA_NUM_PARALLEL=2&quot;
Environment=&quot;OLLAMA_MAX_LOADED_MODELS=3&quot;
Environment=&quot;OLLAMA_FLASH_ATTENTION=1&quot;</code></pre>
      </div>
    </div>
<h3 id="configuration-options" class="mobile-header">Configuration Options</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Set custom models directory
export OLLAMA_MODELS=/custom/path/to/models

# Configure host and port
export OLLAMA_HOST=127.0.0.1:11434

# GPU configuration
export CUDA_VISIBLE_DEVICES=0,1  # Use specific GPUs
export OLLAMA_GPU_OVERHEAD=0     # Reduce GPU memory overhead

# Performance tuning
export OLLAMA_NUM_PARALLEL=4     # Parallel requests
export OLLAMA_MAX_LOADED_MODELS=2 # Max models in memory
export OLLAMA_FLASH_ATTENTION=1  # Enable flash attention</code></pre>
      </div>
    </div>
<h3 id="model-management" class="mobile-header">Model Management</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Download and run models
ollama pull llama3.1:8b
ollama pull codellama:13b
ollama pull mistral:7b

# List installed models
ollama list

# Run a model interactively
ollama run llama3.1:8b

# Remove a model
ollama rm llama3.1:8b

# Show model information
ollama show llama3.1:8b</code></pre>
      </div>
    </div>
<h2 id="5-service-management" class="mobile-header">5. Service Management</h2>
<h3 id="systemd-linux" class="mobile-header">systemd (Linux)</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Start/stop/restart service
sudo systemctl start ollama
sudo systemctl stop ollama
sudo systemctl restart ollama

# Check service status
sudo systemctl status ollama

# View logs
sudo journalctl -u ollama -f

# Enable/disable auto-start
sudo systemctl enable ollama
sudo systemctl disable ollama</code></pre>
      </div>
    </div>
<h3 id="manual-service-management" class="mobile-header">Manual Service Management</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Start Ollama server
ollama serve

# Start with custom configuration
OLLAMA_HOST=0.0.0.0:11434 ollama serve

# Background process
nohup ollama serve &gt; /var/log/ollama.log 2&gt;&amp;1 &amp;</code></pre>
      </div>
    </div>
<h3 id="windows-service-management" class="mobile-header">Windows Service Management</h3>
<div class="mobile-code-block" data-language="powershell">
      <div class="mobile-code-header">
        <span class="mobile-code-language">powershell</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-powershell"># Check service status
Get-Service Ollama

# Start/stop service
Start-Service Ollama
Stop-Service Ollama

# Restart service
Restart-Service Ollama</code></pre>
      </div>
    </div>
<h2 id="6-troubleshooting" class="mobile-header">6. Troubleshooting</h2>
<h3 id="common-issues" class="mobile-header">Common Issues</h3>
<p class="mobile-paragraph">1. <strong>Service won't start</strong>:</p>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Check logs
sudo journalctl -u ollama -n 50

# Check if port is in use
sudo netstat -tlnp | grep 11434

# Verify user permissions
sudo -u ollama /usr/local/bin/ollama serve</code></pre>
      </div>
    </div>
<p class="mobile-paragraph">2. <strong>GPU not detected</strong>:</p>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Check NVIDIA GPU
nvidia-smi

# Check CUDA installation
nvcc --version

# Check Ollama GPU support
ollama info</code></pre>
      </div>
    </div>
<p class="mobile-paragraph">3. <strong>Model download fails</strong>:</p>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Check internet connectivity
curl -I https://ollama.com

# Check disk space
df -h /var/lib/ollama

# Manual model download
curl -L https://huggingface.co/model-url -o model-file</code></pre>
      </div>
    </div>
<p class="mobile-paragraph">4. <strong>High memory usage</strong>:</p>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Check model memory usage
ollama ps

# Reduce loaded models
export OLLAMA_MAX_LOADED_MODELS=1

# Monitor system resources
htop</code></pre>
      </div>
    </div>
<h3 id="debug-mode" class="mobile-header">Debug Mode</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Enable debug logging
export OLLAMA_DEBUG=1
ollama serve

# Verbose API logging
export OLLAMA_VERBOSE=1</code></pre>
      </div>
    </div>
<h2 id="7-security-considerations" class="mobile-header">7. Security Considerations</h2>
<h3 id="network-security" class="mobile-header">Network Security</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Bind to localhost only (default)
export OLLAMA_HOST=127.0.0.1:11434

# Configure firewall (if exposing externally)
sudo firewall-cmd --permanent --add-port=11434/tcp
sudo firewall-cmd --reload

# Use reverse proxy for external access</code></pre>
      </div>
    </div>
<h3 id="reverse-proxy-configuration-nginx" class="mobile-header">Reverse Proxy Configuration (nginx)</h3>
<div class="mobile-code-block" data-language="nginx">
      <div class="mobile-code-header">
        <span class="mobile-code-language">nginx</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-nginx">server {
    listen 80;
    server_name ollama.example.com;
    
    location / {
        proxy_pass http://127.0.0.1:11434;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}</code></pre>
      </div>
    </div>
<h3 id="authentication-setup" class="mobile-header">Authentication Setup</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Ollama doesn&#039;t have built-in auth, use reverse proxy
# Example with basic auth in nginx:
sudo apt install apache2-utils
sudo htpasswd -c /etc/nginx/.htpasswd ollama_user

# Add to nginx config:
# auth_basic &quot;Ollama Access&quot;;
# auth_basic_user_file /etc/nginx/.htpasswd;</code></pre>
      </div>
    </div>
<h3 id="file-permissions" class="mobile-header">File Permissions</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Secure model directory
sudo chown -R ollama:ollama /var/lib/ollama
sudo chmod -R 750 /var/lib/ollama

# Secure configuration files
sudo chmod 640 /etc/systemd/system/ollama.service
sudo chown root:root /etc/systemd/system/ollama.service</code></pre>
      </div>
    </div>
<h2 id="8-performance-tuning" class="mobile-header">8. Performance Tuning</h2>
<h3 id="gpu-optimization" class="mobile-header">GPU Optimization</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># NVIDIA GPU settings
export CUDA_VISIBLE_DEVICES=0,1
export OLLAMA_GPU_OVERHEAD=0

# Check GPU utilization
nvidia-smi -l 1

# AMD GPU (ROCm)
export HSA_OVERRIDE_GFX_VERSION=10.3.0
export ROCM_PATH=/opt/rocm</code></pre>
      </div>
    </div>
<h3 id="cpu-optimization" class="mobile-header">CPU Optimization</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Set CPU affinity
taskset -c 0-7 ollama serve

# Adjust parallel processing
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2

# Enable optimizations
export OLLAMA_FLASH_ATTENTION=1
export OLLAMA_NUMA_PREFER=0</code></pre>
      </div>
    </div>
<h3 id="memory-management" class="mobile-header">Memory Management</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Monitor memory usage
watch -n 1 &#039;free -h &amp;&amp; echo &quot;=== Ollama Process ===&quot; &amp;&amp; ps aux | grep ollama&#039;

# Limit model cache
export OLLAMA_MAX_LOADED_MODELS=1

# Use swap if needed (not recommended for production)
sudo swapon --show</code></pre>
      </div>
    </div>
<h3 id="storage-optimization" class="mobile-header">Storage Optimization</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Use SSD for models
sudo mkdir -p /mnt/ssd/ollama/models
sudo chown ollama:ollama /mnt/ssd/ollama/models
export OLLAMA_MODELS=/mnt/ssd/ollama/models

# Clean up unused models
ollama list | grep -v &quot;NAME&quot; | awk &#039;{print $1}&#039; | xargs ollama rm</code></pre>
      </div>
    </div>
<h2 id="9-backup-and-restore" class="mobile-header">9. Backup and Restore</h2>
<h3 id="model-backup" class="mobile-header">Model Backup</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash">#!/bin/bash
# backup-ollama-models.sh

BACKUP_DIR=&quot;/var/backups/ollama&quot;
MODELS_DIR=&quot;/var/lib/ollama/models&quot;
DATE=$(date +%Y%m%d_%H%M%S)

# Create backup directory
mkdir -p $BACKUP_DIR

# Backup models directory
tar -czf $BACKUP_DIR/ollama_models_$DATE.tar.gz -C /var/lib/ollama models

# Backup model list
ollama list &gt; $BACKUP_DIR/ollama_models_list_$DATE.txt

echo &quot;Backup completed: $BACKUP_DIR/ollama_models_$DATE.tar.gz&quot;</code></pre>
      </div>
    </div>
<h3 id="configuration-backup" class="mobile-header">Configuration Backup</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash">#!/bin/bash
# backup-ollama-config.sh

BACKUP_DIR=&quot;/var/backups/ollama&quot;
DATE=$(date +%Y%m%d_%H%M%S)

# Backup configuration
tar -czf $BACKUP_DIR/ollama_config_$DATE.tar.gz \
    /etc/systemd/system/ollama.service \
    /etc/systemd/system/ollama.service.d/ 2&gt;/dev/null || true

echo &quot;Configuration backup: $BACKUP_DIR/ollama_config_$DATE.tar.gz&quot;</code></pre>
      </div>
    </div>
<h3 id="restore-procedures" class="mobile-header">Restore Procedures</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Restore models
sudo systemctl stop ollama
sudo tar -xzf ollama_models_backup.tar.gz -C /var/lib/ollama
sudo chown -R ollama:ollama /var/lib/ollama/models
sudo systemctl start ollama

# Verify restored models
ollama list</code></pre>
      </div>
    </div>
<h3 id="automated-backup" class="mobile-header">Automated Backup</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Add to crontab
sudo crontab -e

# Daily model backup at 2 AM
0 2 * * * /opt/ollama/scripts/backup-ollama-models.sh

# Weekly configuration backup
0 3 * * 0 /opt/ollama/scripts/backup-ollama-config.sh</code></pre>
      </div>
    </div>
<h2 id="10-system-requirements" class="mobile-header">10. System Requirements</h2>
<h3 id="minimum-requirements" class="mobile-header">Minimum Requirements</h3>
<li class="mobile-list-item"><strong>CPU</strong>: 2 cores, 2.0 GHz</li>
<li class="mobile-list-item"><strong>RAM</strong>: 8GB</li>
<li class="mobile-list-item"><strong>Storage</strong>: 20GB (small models)</li>
<li class="mobile-list-item"><strong>Network</strong>: Broadband for model downloads</li>
<h3 id="recommended-requirements" class="mobile-header">Recommended Requirements</h3>
<li class="mobile-list-item"><strong>CPU</strong>: 8+ cores, 3.0+ GHz</li>
<li class="mobile-list-item"><strong>RAM</strong>: 32GB+</li>
<li class="mobile-list-item"><strong>GPU</strong>: NVIDIA RTX 3060+ or AMD RX 6600 XT+</li>
<li class="mobile-list-item"><strong>Storage</strong>: 100GB+ SSD/NVMe</li>
<li class="mobile-list-item"><strong>Network</strong>: Gigabit for large model downloads</li>
<h3 id="model-specific-requirements" class="mobile-header">Model-Specific Requirements</h3>
<p class="mobile-paragraph">| Model Size | RAM Required | VRAM Required | Storage |</p>
<p class="mobile-paragraph">|------------|--------------|---------------|---------|</p>
<p class="mobile-paragraph">| 7B         | 8GB          | 4GB           | 4GB     |</p>
<p class="mobile-paragraph">| 13B        | 16GB         | 8GB           | 7GB     |</p>
<p class="mobile-paragraph">| 30B        | 32GB         | 20GB          | 19GB    |</p>
<p class="mobile-paragraph">| 70B        | 64GB         | 48GB          | 39GB    |</p>
<h2 id="11-support" class="mobile-header">11. Support</h2>
<h3 id="official-resources" class="mobile-header">Official Resources</h3>
<li class="mobile-list-item"><strong>Website</strong>: https://ollama.com</li>
<li class="mobile-list-item"><strong>GitHub</strong>: https://github.com/ollama/ollama</li>
<li class="mobile-list-item"><strong>Documentation</strong>: https://github.com/ollama/ollama/tree/main/docs</li>
<li class="mobile-list-item"><strong>Model Library</strong>: https://ollama.com/library</li>
<h3 id="community-support" class="mobile-header">Community Support</h3>
<li class="mobile-list-item"><strong>Discord</strong>: https://discord.gg/ollama</li>
<li class="mobile-list-item"><strong>Reddit</strong>: r/ollama</li>
<li class="mobile-list-item"><strong>GitHub Issues</strong>: https://github.com/ollama/ollama/issues</li>
<li class="mobile-list-item"><strong>Discussions</strong>: https://github.com/ollama/ollama/discussions</li>
<h2 id="12-contributing" class="mobile-header">12. Contributing</h2>
<h3 id="how-to-contribute" class="mobile-header">How to Contribute</h3>
<p class="mobile-paragraph">1. Fork the repository on GitHub</p>
<p class="mobile-paragraph">2. Create a feature branch</p>
<p class="mobile-paragraph">3. Submit pull request</p>
<p class="mobile-paragraph">4. Follow Go coding standards</p>
<p class="mobile-paragraph">5. Include tests and documentation</p>
<h3 id="development-setup" class="mobile-header">Development Setup</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Clone repository
git clone https://github.com/ollama/ollama.git
cd ollama

# Install Go dependencies
go mod tidy

# Build from source
go build .

# Run tests
go test ./...</code></pre>
      </div>
    </div>
<h2 id="13-license" class="mobile-header">13. License</h2>
<p class="mobile-paragraph">Ollama is licensed under the MIT License.</p>
<p class="mobile-paragraph">Key points:</p>
<li class="mobile-list-item">Free to use, modify, and distribute</li>
<li class="mobile-list-item">Commercial use allowed</li>
<li class="mobile-list-item">No warranty provided</li>
<li class="mobile-list-item">Attribution required</li>
<h2 id="14-acknowledgments" class="mobile-header">14. Acknowledgments</h2>
<h3 id="credits" class="mobile-header">Credits</h3>
<li class="mobile-list-item"><strong>Ollama Team</strong>: Core development team</li>
<li class="mobile-list-item"><strong>Meta AI</strong>: Llama model family</li>
<li class="mobile-list-item"><strong>Mistral AI</strong>: Mistral models</li>
<li class="mobile-list-item"><strong>Community</strong>: Model creators and contributors</li>
<li class="mobile-list-item"><strong>Hardware Vendors</strong>: NVIDIA, AMD, Apple for acceleration support</li>
<h2 id="15-version-history" class="mobile-header">15. Version History</h2>
<h3 id="recent-releases" class="mobile-header">Recent Releases</h3>
<li class="mobile-list-item"><strong>v0.3.x</strong>: Latest stable with improved performance</li>
<li class="mobile-list-item"><strong>v0.2.x</strong>: Added model management improvements</li>
<li class="mobile-list-item"><strong>v0.1.x</strong>: Initial public release</li>
<h3 id="major-features-by-version" class="mobile-header">Major Features by Version</h3>
<li class="mobile-list-item"><strong>v0.3.0</strong>: Enhanced GPU support, model compression</li>
<li class="mobile-list-item"><strong>v0.2.0</strong>: REST API improvements, concurrent requests</li>
<li class="mobile-list-item"><strong>v0.1.0</strong>: Basic model serving and CLI interface</li>
<h2 id="16-appendices" class="mobile-header">16. Appendices</h2>
<h3 id="a-api-usage-examples" class="mobile-header">A. API Usage Examples</h3>
<p class="mobile-paragraph">#### Basic Chat Completion</p>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash">curl http://localhost:11434/api/generate -d &#039;{
  &quot;model&quot;: &quot;llama3.1:8b&quot;,
  &quot;prompt&quot;: &quot;Why is the sky blue?&quot;,
  &quot;stream&quot;: false
}&#039;</code></pre>
      </div>
    </div>
<p class="mobile-paragraph">#### Streaming Response</p>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash">curl http://localhost:11434/api/generate -d &#039;{
  &quot;model&quot;: &quot;llama3.1:8b&quot;,
  &quot;prompt&quot;: &quot;Write a poem about coding&quot;,
  &quot;stream&quot;: true
}&#039;</code></pre>
      </div>
    </div>
<p class="mobile-paragraph">#### Chat API</p>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash">curl http://localhost:11434/api/chat -d &#039;{
  &quot;model&quot;: &quot;llama3.1:8b&quot;,
  &quot;messages&quot;: [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;Hello, how are you?&quot;
    }
  ]
}&#039;</code></pre>
      </div>
    </div>
<h3 id="b-integration-examples" class="mobile-header">B. Integration Examples</h3>
<p class="mobile-paragraph">#### Python Integration</p>
<div class="mobile-code-block" data-language="python">
      <div class="mobile-code-header">
        <span class="mobile-code-language">python</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-python">import requests
import json

def chat_with_ollama(prompt, model=&quot;llama3.1:8b&quot;):
    url = &quot;http://localhost:11434/api/generate&quot;
    data = {
        &quot;model&quot;: model,
        &quot;prompt&quot;: prompt,
        &quot;stream&quot;: False
    }
    
    response = requests.post(url, json=data)
    if response.status_code == 200:
        return response.json()[&quot;response&quot;]
    else:
        return &quot;Error: &quot; + str(response.status_code)

# Usage
response = chat_with_ollama(&quot;Explain quantum computing&quot;)
print(response)</code></pre>
      </div>
    </div>
<p class="mobile-paragraph">#### Node.js Integration</p>
<div class="mobile-code-block" data-language="javascript">
      <div class="mobile-code-header">
        <span class="mobile-code-language">javascript</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-javascript">const axios = require(&#039;axios&#039;);

async function chatWithOllama(prompt, model = &#039;llama3.1:8b&#039;) {
    try {
        const response = await axios.post(&#039;http://localhost:11434/api/generate&#039;, {
            model: model,
            prompt: prompt,
            stream: false
        });
        
        return response.data.response;
    } catch (error) {
        console.error(&#039;Error:&#039;, error.message);
        return null;
    }
}

// Usage
chatWithOllama(&#039;What is machine learning?&#039;).then(response =&gt; {
    console.log(response);
});</code></pre>
      </div>
    </div>
<h3 id="c-model-customization" class="mobile-header">C. Model Customization</h3>
<p class="mobile-paragraph">#### Creating Custom Models</p>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Create Modelfile
cat &gt; Modelfile &lt;&lt; &#039;EOF&#039;
FROM llama3.1:8b

# Set parameters
PARAMETER temperature 0.7
PARAMETER top_p 0.9

# Set system message
SYSTEM &quot;&quot;&quot;
You are a helpful AI assistant specialized in programming.
Always provide code examples when relevant.
&quot;&quot;&quot;
EOF

# Build custom model
ollama create my-coding-assistant -f Modelfile

# Test custom model
ollama run my-coding-assistant &quot;How do I sort a list in Python?&quot;</code></pre>
      </div>
    </div>
<p class="mobile-paragraph">#### Fine-tuning (Advanced)</p>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash"># Prepare training data (JSONL format)
cat &gt; training_data.jsonl &lt;&lt; &#039;EOF&#039;
{&quot;prompt&quot;: &quot;Question: What is Python?&quot;, &quot;completion&quot;: &quot;Python is a programming language...&quot;}
{&quot;prompt&quot;: &quot;Question: How to install packages?&quot;, &quot;completion&quot;: &quot;Use pip install package_name...&quot;}
EOF

# Note: Fine-tuning requires additional tools and setup
# Refer to Ollama documentation for detailed fine-tuning guide</code></pre>
      </div>
    </div>
<h3 id="d-performance-monitoring" class="mobile-header">D. Performance Monitoring</h3>
<div class="mobile-code-block" data-language="bash">
      <div class="mobile-code-header">
        <span class="mobile-code-language">bash</span>
        <button class="mobile-copy-button" onclick="copyCode(this)" title="Copy code">
          <span class="copy-icon">üìã</span>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <div class="mobile-code-content">
        <pre><code class="language-bash">#!/bin/bash
# monitor-ollama.sh

echo &quot;=== Ollama Service Status ===&quot;
systemctl status ollama --no-pager

echo -e &quot;\n=== Memory Usage ===&quot;
ps aux | grep ollama | grep -v grep

echo -e &quot;\n=== GPU Usage ===&quot;
if command -v nvidia-smi &amp;&gt; /dev/null; then
    nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits
fi

echo -e &quot;\n=== API Health Check ===&quot;
curl -s http://localhost:11434/api/version || echo &quot;API not responding&quot;

echo -e &quot;\n=== Loaded Models ===&quot;
ollama ps

echo -e &quot;\n=== Disk Usage ===&quot;
du -sh /var/lib/ollama/models/*</code></pre>
      </div>
    </div>
<p class="mobile-paragraph">---</p>
<p class="mobile-paragraph">For more information and updates, visit https://github.com/howtomgr/ollama</p></div></article><nav class="jsx-64efd206b36bc2dd guide-navigation"><a class="btn btn-secondary" href="/artificial-intelligence/">‚Üê All <!-- -->Artificial intelligence</a><a class="btn btn-secondary" href="/">All Categories</a></nav></div></main><footer class="jsx-6ca0ba86c5b4bb40 mobile-footer"><div class="jsx-6ca0ba86c5b4bb40 container"><div class="jsx-6ca0ba86c5b4bb40 footer-content"><div class="jsx-6ca0ba86c5b4bb40 footer-links"><a href="/">Home</a><a href="https://github.com/howtomgr" target="_blank" rel="noopener" class="jsx-6ca0ba86c5b4bb40">GitHub</a><a href="https://github.com/howtomgr/howtomgr.github.io" target="_blank" rel="noopener" class="jsx-6ca0ba86c5b4bb40">Source</a></div><div class="jsx-6ca0ba86c5b4bb40 footer-separator"></div><p class="jsx-6ca0ba86c5b4bb40 footer-text">¬© <!-- -->2025<!-- --> HowToMgr Organization. Open source installation guides.</p><p class="jsx-6ca0ba86c5b4bb40 footer-tech">Built with ‚ù§Ô∏è using Next.js + GitHub Pages</p><p class="jsx-6ca0ba86c5b4bb40 footer-disclaimer"><small class="jsx-6ca0ba86c5b4bb40">All guides are provided as-is. Please review and test before production use.</small></p></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"guide":{"name":"ollama","displayName":"Ollama Installation Guide","slug":"ollama","description":"Ollama is a free and open-source tool for running large language models (LLMs) locally on your machine. It serves as a FOSS alternative to cloud-based AI services like OpenAI API, Anthropic Claude API, or Google's Gemini API, enabling privacy-focused AI deployment, offline inference, and cost-effective local AI processing.","category":"artificial-intelligence","subcategory":"llm-runners","difficultyLevel":"intermediate","estimatedSetupTime":"15-30 minutes","supportedOS":["rhel","centos","rocky","almalinux","debian","ubuntu","arch","alpine","opensuse","sles","macos","windows"],"defaultPorts":[11434],"installationMethods":["official-installer","package-manager","manual-binary"],"features":["multi-os-support","local-llm-inference","gpu-acceleration","model-management","rest-api","privacy-focused","offline-capable","comprehensive-documentation","security-hardening","performance-optimization","backup-restore-procedures","troubleshooting-guides"],"tags":["ai","llm","machine-learning","local-inference","privacy","openai-alternative","gpu-acceleration","model-serving","rest-api"],"maintenanceStatus":"active","specVersion":"2.0","version":"1.0.0","license":"MIT","websiteUrl":"https://howtomgr.github.io/artificial-intelligence/ollama","documentationUrl":"https://howtomgr.github.io/artificial-intelligence/ollama","language":null,"stars":0,"forks":0,"topics":[],"githubUrl":"https://github.com/howtomgr/ollama","updatedAt":"2025-09-16T17:37:13Z","createdAt":"2025-09-16T10:48:43Z","readmeRaw":"# Ollama Installation Guide\n\nOllama is a free and open-source tool for running large language models (LLMs) locally on your machine. It serves as a FOSS alternative to cloud-based AI services like OpenAI API, Anthropic Claude API, Google's Gemini API, or Azure OpenAI Service. Ollama enables privacy-focused AI deployment, offline inference, and cost-effective local AI processing with support for popular models like Llama 3, Code Llama, Mistral, and many others.\n\n## Table of Contents\n1. [Prerequisites](#prerequisites)\n2. [Supported Operating Systems](#supported-operating-systems)\n3. [Installation](#installation)\n4. [Configuration](#configuration)\n5. [Service Management](#service-management)\n6. [Troubleshooting](#troubleshooting)\n7. [Security Considerations](#security-considerations)\n8. [Performance Tuning](#performance-tuning)\n9. [Backup and Restore](#backup-and-restore)\n10. [System Requirements](#system-requirements)\n11. [Support](#support)\n12. [Contributing](#contributing)\n13. [License](#license)\n14. [Acknowledgments](#acknowledgments)\n15. [Version History](#version-history)\n16. [Appendices](#appendices)\n\n## 1. Prerequisites\n\n### Hardware Requirements\n- **CPU**: Modern 64-bit processor (x86_64 or ARM64)\n- **RAM**: 8GB minimum (16GB+ recommended for larger models)\n- **Storage**: 10GB+ free space for models\n- **GPU**: Optional but recommended (NVIDIA with CUDA, AMD ROCm, or Apple Metal)\n\n### Software Requirements\n- **Operating System**: Linux, macOS, or Windows\n- **Internet**: Required for initial model downloads\n- **Docker**: Optional for containerized deployment\n\n### Network Requirements\n- **Ports**: \n  - 11434: Default API server port\n- **Bandwidth**: High-speed internet for model downloads (models range from 1GB to 70GB+)\n\n## 2. Supported Operating Systems\n\nOllama officially supports:\n- RHEL 8/9 and derivatives (CentOS Stream, Rocky Linux, AlmaLinux)\n- Debian 11/12\n- Ubuntu 20.04 LTS / 22.04 LTS / 24.04 LTS\n- Arch Linux\n- Alpine Linux 3.18+\n- openSUSE Leap 15.5+ / Tumbleweed\n- macOS 11+ (Big Sur and later)\n- Windows 10/11\n\n## 3. Installation\n\n### RHEL/CentOS/Rocky Linux/AlmaLinux\n\n```bash\n# Method 1: Official installer script\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Method 2: Manual installation\n# Download latest release\ncurl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Create ollama user\nsudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama\nsudo usermod -a -G render,video ollama\n\n# Create systemd service\nsudo tee /etc/systemd/system/ollama.service \u003e /dev/null \u003c\u003c 'EOF'\n[Unit]\nDescription=Ollama Service\nAfter=network-online.target\n\n[Service]\nExecStart=/usr/local/bin/ollama serve\nUser=ollama\nGroup=ollama\nRestart=always\nRestartSec=3\nEnvironment=\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\nEnvironment=\"OLLAMA_HOST=0.0.0.0\"\n\n[Install]\nWantedBy=default.target\nEOF\n\n# Enable and start service\nsudo systemctl daemon-reload\nsudo systemctl enable --now ollama\n```\n\n### Debian/Ubuntu\n\n```bash\n# Method 1: Official installer script\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Method 2: Package installation (if available)\n# Add official repository\ncurl -fsSL https://ollama.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/ollama-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/ollama-keyring.gpg] https://ollama.com/debian stable main\" | sudo tee /etc/apt/sources.list.d/ollama.list\n\n# Install package\nsudo apt update\nsudo apt install -y ollama\n\n# Start service\nsudo systemctl enable --now ollama\n```\n\n### Arch Linux\n\n```bash\n# Install from AUR\nyay -S ollama-bin\n# or\nparu -S ollama-bin\n\n# Alternative: Build from source\nyay -S ollama\n\n# Enable and start service\nsudo systemctl enable --now ollama\n```\n\n### Alpine Linux\n\n```bash\n# Install dependencies\napk add --no-cache curl\n\n# Install Ollama binary\ncurl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Create ollama user\nadduser -D -s /bin/false -h /usr/share/ollama ollama\naddgroup ollama video\naddgroup ollama render\n\n# Create OpenRC service\ntee /etc/init.d/ollama \u003e /dev/null \u003c\u003c 'EOF'\n#!/sbin/openrc-run\n\ndescription=\"Ollama Service\"\ncommand=\"/usr/local/bin/ollama\"\ncommand_args=\"serve\"\ncommand_user=\"ollama\"\ncommand_group=\"ollama\"\npidfile=\"/run/ollama.pid\"\ncommand_background=\"yes\"\n\ndepend() {\n    need net\n    after firewall\n}\n\nstart_pre() {\n    export OLLAMA_HOST=\"0.0.0.0\"\n    checkpath --directory --owner ollama:ollama --mode 0755 /run/ollama\n}\nEOF\n\nchmod +x /etc/init.d/ollama\nrc-update add ollama default\nrc-service ollama start\n```\n\n### openSUSE\n\n```bash\n# Install via zypper (if available) or manual installation\nsudo zypper refresh\n\n# Manual installation\ncurl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Create ollama user\nsudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama\nsudo usermod -a -G video,render ollama\n\n# Create systemd service (same as RHEL)\nsudo systemctl enable --now ollama\n```\n\n### macOS\n\n```bash\n# Method 1: Official app installer\n# Download from https://ollama.com/download/mac\n\n# Method 2: Homebrew\nbrew install ollama\n\n# Method 3: Manual installation\ncurl -L https://ollama.com/download/darwin-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Start Ollama\nollama serve \u0026\n```\n\n### Windows\n\n```powershell\n# Method 1: Official installer\n# Download and run installer from https://ollama.com/download/windows\n\n# Method 2: Winget\nwinget install Ollama.Ollama\n\n# Method 3: Chocolatey\nchoco install ollama\n\n# Method 4: Scoop\nscoop bucket add extras\nscoop install ollama\n\n# Start Ollama service (automatic with installer)\n```\n\n## 4. Configuration\n\n### Environment Variables\n\nCreate `/etc/systemd/system/ollama.service.d/override.conf`:\n```ini\n[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0:11434\"\nEnvironment=\"OLLAMA_MODELS=/var/lib/ollama/models\"\nEnvironment=\"OLLAMA_NUM_PARALLEL=2\"\nEnvironment=\"OLLAMA_MAX_LOADED_MODELS=3\"\nEnvironment=\"OLLAMA_FLASH_ATTENTION=1\"\n```\n\n### Configuration Options\n\n```bash\n# Set custom models directory\nexport OLLAMA_MODELS=/custom/path/to/models\n\n# Configure host and port\nexport OLLAMA_HOST=127.0.0.1:11434\n\n# GPU configuration\nexport CUDA_VISIBLE_DEVICES=0,1  # Use specific GPUs\nexport OLLAMA_GPU_OVERHEAD=0     # Reduce GPU memory overhead\n\n# Performance tuning\nexport OLLAMA_NUM_PARALLEL=4     # Parallel requests\nexport OLLAMA_MAX_LOADED_MODELS=2 # Max models in memory\nexport OLLAMA_FLASH_ATTENTION=1  # Enable flash attention\n```\n\n### Model Management\n\n```bash\n# Download and run models\nollama pull llama3.1:8b\nollama pull codellama:13b\nollama pull mistral:7b\n\n# List installed models\nollama list\n\n# Run a model interactively\nollama run llama3.1:8b\n\n# Remove a model\nollama rm llama3.1:8b\n\n# Show model information\nollama show llama3.1:8b\n```\n\n## 5. Service Management\n\n### systemd (Linux)\n\n```bash\n# Start/stop/restart service\nsudo systemctl start ollama\nsudo systemctl stop ollama\nsudo systemctl restart ollama\n\n# Check service status\nsudo systemctl status ollama\n\n# View logs\nsudo journalctl -u ollama -f\n\n# Enable/disable auto-start\nsudo systemctl enable ollama\nsudo systemctl disable ollama\n```\n\n### Manual Service Management\n\n```bash\n# Start Ollama server\nollama serve\n\n# Start with custom configuration\nOLLAMA_HOST=0.0.0.0:11434 ollama serve\n\n# Background process\nnohup ollama serve \u003e /var/log/ollama.log 2\u003e\u00261 \u0026\n```\n\n### Windows Service Management\n\n```powershell\n# Check service status\nGet-Service Ollama\n\n# Start/stop service\nStart-Service Ollama\nStop-Service Ollama\n\n# Restart service\nRestart-Service Ollama\n```\n\n## 6. Troubleshooting\n\n### Common Issues\n\n1. **Service won't start**:\n```bash\n# Check logs\nsudo journalctl -u ollama -n 50\n\n# Check if port is in use\nsudo netstat -tlnp | grep 11434\n\n# Verify user permissions\nsudo -u ollama /usr/local/bin/ollama serve\n```\n\n2. **GPU not detected**:\n```bash\n# Check NVIDIA GPU\nnvidia-smi\n\n# Check CUDA installation\nnvcc --version\n\n# Check Ollama GPU support\nollama info\n```\n\n3. **Model download fails**:\n```bash\n# Check internet connectivity\ncurl -I https://ollama.com\n\n# Check disk space\ndf -h /var/lib/ollama\n\n# Manual model download\ncurl -L https://huggingface.co/model-url -o model-file\n```\n\n4. **High memory usage**:\n```bash\n# Check model memory usage\nollama ps\n\n# Reduce loaded models\nexport OLLAMA_MAX_LOADED_MODELS=1\n\n# Monitor system resources\nhtop\n```\n\n### Debug Mode\n\n```bash\n# Enable debug logging\nexport OLLAMA_DEBUG=1\nollama serve\n\n# Verbose API logging\nexport OLLAMA_VERBOSE=1\n```\n\n## 7. Security Considerations\n\n### Network Security\n\n```bash\n# Bind to localhost only (default)\nexport OLLAMA_HOST=127.0.0.1:11434\n\n# Configure firewall (if exposing externally)\nsudo firewall-cmd --permanent --add-port=11434/tcp\nsudo firewall-cmd --reload\n\n# Use reverse proxy for external access\n```\n\n### Reverse Proxy Configuration (nginx)\n\n```nginx\nserver {\n    listen 80;\n    server_name ollama.example.com;\n    \n    location / {\n        proxy_pass http://127.0.0.1:11434;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n```\n\n### Authentication Setup\n\n```bash\n# Ollama doesn't have built-in auth, use reverse proxy\n# Example with basic auth in nginx:\nsudo apt install apache2-utils\nsudo htpasswd -c /etc/nginx/.htpasswd ollama_user\n\n# Add to nginx config:\n# auth_basic \"Ollama Access\";\n# auth_basic_user_file /etc/nginx/.htpasswd;\n```\n\n### File Permissions\n\n```bash\n# Secure model directory\nsudo chown -R ollama:ollama /var/lib/ollama\nsudo chmod -R 750 /var/lib/ollama\n\n# Secure configuration files\nsudo chmod 640 /etc/systemd/system/ollama.service\nsudo chown root:root /etc/systemd/system/ollama.service\n```\n\n## 8. Performance Tuning\n\n### GPU Optimization\n\n```bash\n# NVIDIA GPU settings\nexport CUDA_VISIBLE_DEVICES=0,1\nexport OLLAMA_GPU_OVERHEAD=0\n\n# Check GPU utilization\nnvidia-smi -l 1\n\n# AMD GPU (ROCm)\nexport HSA_OVERRIDE_GFX_VERSION=10.3.0\nexport ROCM_PATH=/opt/rocm\n```\n\n### CPU Optimization\n\n```bash\n# Set CPU affinity\ntaskset -c 0-7 ollama serve\n\n# Adjust parallel processing\nexport OLLAMA_NUM_PARALLEL=4\nexport OLLAMA_MAX_LOADED_MODELS=2\n\n# Enable optimizations\nexport OLLAMA_FLASH_ATTENTION=1\nexport OLLAMA_NUMA_PREFER=0\n```\n\n### Memory Management\n\n```bash\n# Monitor memory usage\nwatch -n 1 'free -h \u0026\u0026 echo \"=== Ollama Process ===\" \u0026\u0026 ps aux | grep ollama'\n\n# Limit model cache\nexport OLLAMA_MAX_LOADED_MODELS=1\n\n# Use swap if needed (not recommended for production)\nsudo swapon --show\n```\n\n### Storage Optimization\n\n```bash\n# Use SSD for models\nsudo mkdir -p /mnt/ssd/ollama/models\nsudo chown ollama:ollama /mnt/ssd/ollama/models\nexport OLLAMA_MODELS=/mnt/ssd/ollama/models\n\n# Clean up unused models\nollama list | grep -v \"NAME\" | awk '{print $1}' | xargs ollama rm\n```\n\n## 9. Backup and Restore\n\n### Model Backup\n\n```bash\n#!/bin/bash\n# backup-ollama-models.sh\n\nBACKUP_DIR=\"/var/backups/ollama\"\nMODELS_DIR=\"/var/lib/ollama/models\"\nDATE=$(date +%Y%m%d_%H%M%S)\n\n# Create backup directory\nmkdir -p $BACKUP_DIR\n\n# Backup models directory\ntar -czf $BACKUP_DIR/ollama_models_$DATE.tar.gz -C /var/lib/ollama models\n\n# Backup model list\nollama list \u003e $BACKUP_DIR/ollama_models_list_$DATE.txt\n\necho \"Backup completed: $BACKUP_DIR/ollama_models_$DATE.tar.gz\"\n```\n\n### Configuration Backup\n\n```bash\n#!/bin/bash\n# backup-ollama-config.sh\n\nBACKUP_DIR=\"/var/backups/ollama\"\nDATE=$(date +%Y%m%d_%H%M%S)\n\n# Backup configuration\ntar -czf $BACKUP_DIR/ollama_config_$DATE.tar.gz \\\n    /etc/systemd/system/ollama.service \\\n    /etc/systemd/system/ollama.service.d/ 2\u003e/dev/null || true\n\necho \"Configuration backup: $BACKUP_DIR/ollama_config_$DATE.tar.gz\"\n```\n\n### Restore Procedures\n\n```bash\n# Restore models\nsudo systemctl stop ollama\nsudo tar -xzf ollama_models_backup.tar.gz -C /var/lib/ollama\nsudo chown -R ollama:ollama /var/lib/ollama/models\nsudo systemctl start ollama\n\n# Verify restored models\nollama list\n```\n\n### Automated Backup\n\n```bash\n# Add to crontab\nsudo crontab -e\n\n# Daily model backup at 2 AM\n0 2 * * * /opt/ollama/scripts/backup-ollama-models.sh\n\n# Weekly configuration backup\n0 3 * * 0 /opt/ollama/scripts/backup-ollama-config.sh\n```\n\n## 10. System Requirements\n\n### Minimum Requirements\n- **CPU**: 2 cores, 2.0 GHz\n- **RAM**: 8GB\n- **Storage**: 20GB (small models)\n- **Network**: Broadband for model downloads\n\n### Recommended Requirements\n- **CPU**: 8+ cores, 3.0+ GHz\n- **RAM**: 32GB+\n- **GPU**: NVIDIA RTX 3060+ or AMD RX 6600 XT+\n- **Storage**: 100GB+ SSD/NVMe\n- **Network**: Gigabit for large model downloads\n\n### Model-Specific Requirements\n\n| Model Size | RAM Required | VRAM Required | Storage |\n|------------|--------------|---------------|---------|\n| 7B         | 8GB          | 4GB           | 4GB     |\n| 13B        | 16GB         | 8GB           | 7GB     |\n| 30B        | 32GB         | 20GB          | 19GB    |\n| 70B        | 64GB         | 48GB          | 39GB    |\n\n## 11. Support\n\n### Official Resources\n- **Website**: https://ollama.com\n- **GitHub**: https://github.com/ollama/ollama\n- **Documentation**: https://github.com/ollama/ollama/tree/main/docs\n- **Model Library**: https://ollama.com/library\n\n### Community Support\n- **Discord**: https://discord.gg/ollama\n- **Reddit**: r/ollama\n- **GitHub Issues**: https://github.com/ollama/ollama/issues\n- **Discussions**: https://github.com/ollama/ollama/discussions\n\n## 12. Contributing\n\n### How to Contribute\n1. Fork the repository on GitHub\n2. Create a feature branch\n3. Submit pull request\n4. Follow Go coding standards\n5. Include tests and documentation\n\n### Development Setup\n```bash\n# Clone repository\ngit clone https://github.com/ollama/ollama.git\ncd ollama\n\n# Install Go dependencies\ngo mod tidy\n\n# Build from source\ngo build .\n\n# Run tests\ngo test ./...\n```\n\n## 13. License\n\nOllama is licensed under the MIT License.\n\nKey points:\n- Free to use, modify, and distribute\n- Commercial use allowed\n- No warranty provided\n- Attribution required\n\n## 14. Acknowledgments\n\n### Credits\n- **Ollama Team**: Core development team\n- **Meta AI**: Llama model family\n- **Mistral AI**: Mistral models\n- **Community**: Model creators and contributors\n- **Hardware Vendors**: NVIDIA, AMD, Apple for acceleration support\n\n## 15. Version History\n\n### Recent Releases\n- **v0.3.x**: Latest stable with improved performance\n- **v0.2.x**: Added model management improvements\n- **v0.1.x**: Initial public release\n\n### Major Features by Version\n- **v0.3.0**: Enhanced GPU support, model compression\n- **v0.2.0**: REST API improvements, concurrent requests\n- **v0.1.0**: Basic model serving and CLI interface\n\n## 16. Appendices\n\n### A. API Usage Examples\n\n#### Basic Chat Completion\n```bash\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.1:8b\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"stream\": false\n}'\n```\n\n#### Streaming Response\n```bash\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.1:8b\",\n  \"prompt\": \"Write a poem about coding\",\n  \"stream\": true\n}'\n```\n\n#### Chat API\n```bash\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.1:8b\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello, how are you?\"\n    }\n  ]\n}'\n```\n\n### B. Integration Examples\n\n#### Python Integration\n```python\nimport requests\nimport json\n\ndef chat_with_ollama(prompt, model=\"llama3.1:8b\"):\n    url = \"http://localhost:11434/api/generate\"\n    data = {\n        \"model\": model,\n        \"prompt\": prompt,\n        \"stream\": False\n    }\n    \n    response = requests.post(url, json=data)\n    if response.status_code == 200:\n        return response.json()[\"response\"]\n    else:\n        return \"Error: \" + str(response.status_code)\n\n# Usage\nresponse = chat_with_ollama(\"Explain quantum computing\")\nprint(response)\n```\n\n#### Node.js Integration\n```javascript\nconst axios = require('axios');\n\nasync function chatWithOllama(prompt, model = 'llama3.1:8b') {\n    try {\n        const response = await axios.post('http://localhost:11434/api/generate', {\n            model: model,\n            prompt: prompt,\n            stream: false\n        });\n        \n        return response.data.response;\n    } catch (error) {\n        console.error('Error:', error.message);\n        return null;\n    }\n}\n\n// Usage\nchatWithOllama('What is machine learning?').then(response =\u003e {\n    console.log(response);\n});\n```\n\n### C. Model Customization\n\n#### Creating Custom Models\n```bash\n# Create Modelfile\ncat \u003e Modelfile \u003c\u003c 'EOF'\nFROM llama3.1:8b\n\n# Set parameters\nPARAMETER temperature 0.7\nPARAMETER top_p 0.9\n\n# Set system message\nSYSTEM \"\"\"\nYou are a helpful AI assistant specialized in programming.\nAlways provide code examples when relevant.\n\"\"\"\nEOF\n\n# Build custom model\nollama create my-coding-assistant -f Modelfile\n\n# Test custom model\nollama run my-coding-assistant \"How do I sort a list in Python?\"\n```\n\n#### Fine-tuning (Advanced)\n```bash\n# Prepare training data (JSONL format)\ncat \u003e training_data.jsonl \u003c\u003c 'EOF'\n{\"prompt\": \"Question: What is Python?\", \"completion\": \"Python is a programming language...\"}\n{\"prompt\": \"Question: How to install packages?\", \"completion\": \"Use pip install package_name...\"}\nEOF\n\n# Note: Fine-tuning requires additional tools and setup\n# Refer to Ollama documentation for detailed fine-tuning guide\n```\n\n### D. Performance Monitoring\n\n```bash\n#!/bin/bash\n# monitor-ollama.sh\n\necho \"=== Ollama Service Status ===\"\nsystemctl status ollama --no-pager\n\necho -e \"\\n=== Memory Usage ===\"\nps aux | grep ollama | grep -v grep\n\necho -e \"\\n=== GPU Usage ===\"\nif command -v nvidia-smi \u0026\u003e /dev/null; then\n    nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits\nfi\n\necho -e \"\\n=== API Health Check ===\"\ncurl -s http://localhost:11434/api/version || echo \"API not responding\"\n\necho -e \"\\n=== Loaded Models ===\"\nollama ps\n\necho -e \"\\n=== Disk Usage ===\"\ndu -sh /var/lib/ollama/models/*\n```\n\n---\n\nFor more information and updates, visit https://github.com/howtomgr/ollama","readmeHtml":"\u003cp class=\"mobile-paragraph\"\u003eOllama is a free and open-source tool for running large language models (LLMs) locally on your machine. It serves as a FOSS alternative to cloud-based AI services like OpenAI API, Anthropic Claude API, Google's Gemini API, or Azure OpenAI Service. Ollama enables privacy-focused AI deployment, offline inference, and cost-effective local AI processing with support for popular models like Llama 3, Code Llama, Mistral, and many others.\u003c/p\u003e\n\u003ch2 id=\"1-prerequisites\" class=\"mobile-header\"\u003e1. Prerequisites\u003c/h2\u003e\n\u003ch3 id=\"hardware-requirements\" class=\"mobile-header\"\u003eHardware Requirements\u003c/h3\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eCPU\u003c/strong\u003e: Modern 64-bit processor (x86_64 or ARM64)\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eRAM\u003c/strong\u003e: 8GB minimum (16GB+ recommended for larger models)\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eStorage\u003c/strong\u003e: 10GB+ free space for models\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eGPU\u003c/strong\u003e: Optional but recommended (NVIDIA with CUDA, AMD ROCm, or Apple Metal)\u003c/li\u003e\n\u003ch3 id=\"software-requirements\" class=\"mobile-header\"\u003eSoftware Requirements\u003c/h3\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eOperating System\u003c/strong\u003e: Linux, macOS, or Windows\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eInternet\u003c/strong\u003e: Required for initial model downloads\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eDocker\u003c/strong\u003e: Optional for containerized deployment\u003c/li\u003e\n\u003ch3 id=\"network-requirements\" class=\"mobile-header\"\u003eNetwork Requirements\u003c/h3\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003ePorts\u003c/strong\u003e: \u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e11434: Default API server port\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eBandwidth\u003c/strong\u003e: High-speed internet for model downloads (models range from 1GB to 70GB+)\u003c/li\u003e\n\u003ch2 id=\"2-supported-operating-systems\" class=\"mobile-header\"\u003e2. Supported Operating Systems\u003c/h2\u003e\n\u003cp class=\"mobile-paragraph\"\u003eOllama officially supports:\u003c/p\u003e\n\u003cli class=\"mobile-list-item\"\u003eRHEL 8/9 and derivatives (CentOS Stream, Rocky Linux, AlmaLinux)\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003eDebian 11/12\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003eUbuntu 20.04 LTS / 22.04 LTS / 24.04 LTS\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003eArch Linux\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003eAlpine Linux 3.18+\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003eopenSUSE Leap 15.5+ / Tumbleweed\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003emacOS 11+ (Big Sur and later)\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003eWindows 10/11\u003c/li\u003e\n\u003ch2 id=\"3-installation\" class=\"mobile-header\"\u003e3. Installation\u003c/h2\u003e\n\u003ch3 id=\"rhelcentosrocky-linuxalmalinux\" class=\"mobile-header\"\u003eRHEL/CentOS/Rocky Linux/AlmaLinux\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Method 1: Official installer script\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Method 2: Manual installation\n# Download latest release\ncurl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Create ollama user\nsudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama\nsudo usermod -a -G render,video ollama\n\n# Create systemd service\nsudo tee /etc/systemd/system/ollama.service \u0026gt; /dev/null \u0026lt;\u0026lt; \u0026#039;EOF\u0026#039;\n[Unit]\nDescription=Ollama Service\nAfter=network-online.target\n\n[Service]\nExecStart=/usr/local/bin/ollama serve\nUser=ollama\nGroup=ollama\nRestart=always\nRestartSec=3\nEnvironment=\u0026quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u0026quot;\nEnvironment=\u0026quot;OLLAMA_HOST=0.0.0.0\u0026quot;\n\n[Install]\nWantedBy=default.target\nEOF\n\n# Enable and start service\nsudo systemctl daemon-reload\nsudo systemctl enable --now ollama\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"debianubuntu\" class=\"mobile-header\"\u003eDebian/Ubuntu\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Method 1: Official installer script\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Method 2: Package installation (if available)\n# Add official repository\ncurl -fsSL https://ollama.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/ollama-keyring.gpg\necho \u0026quot;deb [signed-by=/usr/share/keyrings/ollama-keyring.gpg] https://ollama.com/debian stable main\u0026quot; | sudo tee /etc/apt/sources.list.d/ollama.list\n\n# Install package\nsudo apt update\nsudo apt install -y ollama\n\n# Start service\nsudo systemctl enable --now ollama\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"arch-linux\" class=\"mobile-header\"\u003eArch Linux\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Install from AUR\nyay -S ollama-bin\n# or\nparu -S ollama-bin\n\n# Alternative: Build from source\nyay -S ollama\n\n# Enable and start service\nsudo systemctl enable --now ollama\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"alpine-linux\" class=\"mobile-header\"\u003eAlpine Linux\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Install dependencies\napk add --no-cache curl\n\n# Install Ollama binary\ncurl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Create ollama user\nadduser -D -s /bin/false -h /usr/share/ollama ollama\naddgroup ollama video\naddgroup ollama render\n\n# Create OpenRC service\ntee /etc/init.d/ollama \u0026gt; /dev/null \u0026lt;\u0026lt; \u0026#039;EOF\u0026#039;\n#!/sbin/openrc-run\n\ndescription=\u0026quot;Ollama Service\u0026quot;\ncommand=\u0026quot;/usr/local/bin/ollama\u0026quot;\ncommand_args=\u0026quot;serve\u0026quot;\ncommand_user=\u0026quot;ollama\u0026quot;\ncommand_group=\u0026quot;ollama\u0026quot;\npidfile=\u0026quot;/run/ollama.pid\u0026quot;\ncommand_background=\u0026quot;yes\u0026quot;\n\ndepend() {\n    need net\n    after firewall\n}\n\nstart_pre() {\n    export OLLAMA_HOST=\u0026quot;0.0.0.0\u0026quot;\n    checkpath --directory --owner ollama:ollama --mode 0755 /run/ollama\n}\nEOF\n\nchmod +x /etc/init.d/ollama\nrc-update add ollama default\nrc-service ollama start\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"opensuse\" class=\"mobile-header\"\u003eopenSUSE\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Install via zypper (if available) or manual installation\nsudo zypper refresh\n\n# Manual installation\ncurl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Create ollama user\nsudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama\nsudo usermod -a -G video,render ollama\n\n# Create systemd service (same as RHEL)\nsudo systemctl enable --now ollama\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"macos\" class=\"mobile-header\"\u003emacOS\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Method 1: Official app installer\n# Download from https://ollama.com/download/mac\n\n# Method 2: Homebrew\nbrew install ollama\n\n# Method 3: Manual installation\ncurl -L https://ollama.com/download/darwin-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Start Ollama\nollama serve \u0026amp;\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"windows\" class=\"mobile-header\"\u003eWindows\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"powershell\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003epowershell\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-powershell\"\u003e# Method 1: Official installer\n# Download and run installer from https://ollama.com/download/windows\n\n# Method 2: Winget\nwinget install Ollama.Ollama\n\n# Method 3: Chocolatey\nchoco install ollama\n\n# Method 4: Scoop\nscoop bucket add extras\nscoop install ollama\n\n# Start Ollama service (automatic with installer)\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch2 id=\"4-configuration\" class=\"mobile-header\"\u003e4. Configuration\u003c/h2\u003e\n\u003ch3 id=\"environment-variables\" class=\"mobile-header\"\u003eEnvironment Variables\u003c/h3\u003e\n\u003cp class=\"mobile-paragraph\"\u003eCreate \u003ccode class=\"mobile-inline-code\"\u003e/etc/systemd/system/ollama.service.d/override.conf\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"ini\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003eini\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-ini\"\u003e[Service]\nEnvironment=\u0026quot;OLLAMA_HOST=0.0.0.0:11434\u0026quot;\nEnvironment=\u0026quot;OLLAMA_MODELS=/var/lib/ollama/models\u0026quot;\nEnvironment=\u0026quot;OLLAMA_NUM_PARALLEL=2\u0026quot;\nEnvironment=\u0026quot;OLLAMA_MAX_LOADED_MODELS=3\u0026quot;\nEnvironment=\u0026quot;OLLAMA_FLASH_ATTENTION=1\u0026quot;\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"configuration-options\" class=\"mobile-header\"\u003eConfiguration Options\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Set custom models directory\nexport OLLAMA_MODELS=/custom/path/to/models\n\n# Configure host and port\nexport OLLAMA_HOST=127.0.0.1:11434\n\n# GPU configuration\nexport CUDA_VISIBLE_DEVICES=0,1  # Use specific GPUs\nexport OLLAMA_GPU_OVERHEAD=0     # Reduce GPU memory overhead\n\n# Performance tuning\nexport OLLAMA_NUM_PARALLEL=4     # Parallel requests\nexport OLLAMA_MAX_LOADED_MODELS=2 # Max models in memory\nexport OLLAMA_FLASH_ATTENTION=1  # Enable flash attention\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"model-management\" class=\"mobile-header\"\u003eModel Management\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Download and run models\nollama pull llama3.1:8b\nollama pull codellama:13b\nollama pull mistral:7b\n\n# List installed models\nollama list\n\n# Run a model interactively\nollama run llama3.1:8b\n\n# Remove a model\nollama rm llama3.1:8b\n\n# Show model information\nollama show llama3.1:8b\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch2 id=\"5-service-management\" class=\"mobile-header\"\u003e5. Service Management\u003c/h2\u003e\n\u003ch3 id=\"systemd-linux\" class=\"mobile-header\"\u003esystemd (Linux)\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Start/stop/restart service\nsudo systemctl start ollama\nsudo systemctl stop ollama\nsudo systemctl restart ollama\n\n# Check service status\nsudo systemctl status ollama\n\n# View logs\nsudo journalctl -u ollama -f\n\n# Enable/disable auto-start\nsudo systemctl enable ollama\nsudo systemctl disable ollama\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"manual-service-management\" class=\"mobile-header\"\u003eManual Service Management\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Start Ollama server\nollama serve\n\n# Start with custom configuration\nOLLAMA_HOST=0.0.0.0:11434 ollama serve\n\n# Background process\nnohup ollama serve \u0026gt; /var/log/ollama.log 2\u0026gt;\u0026amp;1 \u0026amp;\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"windows-service-management\" class=\"mobile-header\"\u003eWindows Service Management\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"powershell\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003epowershell\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-powershell\"\u003e# Check service status\nGet-Service Ollama\n\n# Start/stop service\nStart-Service Ollama\nStop-Service Ollama\n\n# Restart service\nRestart-Service Ollama\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch2 id=\"6-troubleshooting\" class=\"mobile-header\"\u003e6. Troubleshooting\u003c/h2\u003e\n\u003ch3 id=\"common-issues\" class=\"mobile-header\"\u003eCommon Issues\u003c/h3\u003e\n\u003cp class=\"mobile-paragraph\"\u003e1. \u003cstrong\u003eService won't start\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Check logs\nsudo journalctl -u ollama -n 50\n\n# Check if port is in use\nsudo netstat -tlnp | grep 11434\n\n# Verify user permissions\nsudo -u ollama /usr/local/bin/ollama serve\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003cp class=\"mobile-paragraph\"\u003e2. \u003cstrong\u003eGPU not detected\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Check NVIDIA GPU\nnvidia-smi\n\n# Check CUDA installation\nnvcc --version\n\n# Check Ollama GPU support\nollama info\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003cp class=\"mobile-paragraph\"\u003e3. \u003cstrong\u003eModel download fails\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Check internet connectivity\ncurl -I https://ollama.com\n\n# Check disk space\ndf -h /var/lib/ollama\n\n# Manual model download\ncurl -L https://huggingface.co/model-url -o model-file\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003cp class=\"mobile-paragraph\"\u003e4. \u003cstrong\u003eHigh memory usage\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Check model memory usage\nollama ps\n\n# Reduce loaded models\nexport OLLAMA_MAX_LOADED_MODELS=1\n\n# Monitor system resources\nhtop\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"debug-mode\" class=\"mobile-header\"\u003eDebug Mode\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Enable debug logging\nexport OLLAMA_DEBUG=1\nollama serve\n\n# Verbose API logging\nexport OLLAMA_VERBOSE=1\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch2 id=\"7-security-considerations\" class=\"mobile-header\"\u003e7. Security Considerations\u003c/h2\u003e\n\u003ch3 id=\"network-security\" class=\"mobile-header\"\u003eNetwork Security\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Bind to localhost only (default)\nexport OLLAMA_HOST=127.0.0.1:11434\n\n# Configure firewall (if exposing externally)\nsudo firewall-cmd --permanent --add-port=11434/tcp\nsudo firewall-cmd --reload\n\n# Use reverse proxy for external access\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"reverse-proxy-configuration-nginx\" class=\"mobile-header\"\u003eReverse Proxy Configuration (nginx)\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"nginx\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003enginx\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-nginx\"\u003eserver {\n    listen 80;\n    server_name ollama.example.com;\n    \n    location / {\n        proxy_pass http://127.0.0.1:11434;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"authentication-setup\" class=\"mobile-header\"\u003eAuthentication Setup\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Ollama doesn\u0026#039;t have built-in auth, use reverse proxy\n# Example with basic auth in nginx:\nsudo apt install apache2-utils\nsudo htpasswd -c /etc/nginx/.htpasswd ollama_user\n\n# Add to nginx config:\n# auth_basic \u0026quot;Ollama Access\u0026quot;;\n# auth_basic_user_file /etc/nginx/.htpasswd;\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"file-permissions\" class=\"mobile-header\"\u003eFile Permissions\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Secure model directory\nsudo chown -R ollama:ollama /var/lib/ollama\nsudo chmod -R 750 /var/lib/ollama\n\n# Secure configuration files\nsudo chmod 640 /etc/systemd/system/ollama.service\nsudo chown root:root /etc/systemd/system/ollama.service\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch2 id=\"8-performance-tuning\" class=\"mobile-header\"\u003e8. Performance Tuning\u003c/h2\u003e\n\u003ch3 id=\"gpu-optimization\" class=\"mobile-header\"\u003eGPU Optimization\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# NVIDIA GPU settings\nexport CUDA_VISIBLE_DEVICES=0,1\nexport OLLAMA_GPU_OVERHEAD=0\n\n# Check GPU utilization\nnvidia-smi -l 1\n\n# AMD GPU (ROCm)\nexport HSA_OVERRIDE_GFX_VERSION=10.3.0\nexport ROCM_PATH=/opt/rocm\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"cpu-optimization\" class=\"mobile-header\"\u003eCPU Optimization\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Set CPU affinity\ntaskset -c 0-7 ollama serve\n\n# Adjust parallel processing\nexport OLLAMA_NUM_PARALLEL=4\nexport OLLAMA_MAX_LOADED_MODELS=2\n\n# Enable optimizations\nexport OLLAMA_FLASH_ATTENTION=1\nexport OLLAMA_NUMA_PREFER=0\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"memory-management\" class=\"mobile-header\"\u003eMemory Management\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Monitor memory usage\nwatch -n 1 \u0026#039;free -h \u0026amp;\u0026amp; echo \u0026quot;=== Ollama Process ===\u0026quot; \u0026amp;\u0026amp; ps aux | grep ollama\u0026#039;\n\n# Limit model cache\nexport OLLAMA_MAX_LOADED_MODELS=1\n\n# Use swap if needed (not recommended for production)\nsudo swapon --show\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"storage-optimization\" class=\"mobile-header\"\u003eStorage Optimization\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Use SSD for models\nsudo mkdir -p /mnt/ssd/ollama/models\nsudo chown ollama:ollama /mnt/ssd/ollama/models\nexport OLLAMA_MODELS=/mnt/ssd/ollama/models\n\n# Clean up unused models\nollama list | grep -v \u0026quot;NAME\u0026quot; | awk \u0026#039;{print $1}\u0026#039; | xargs ollama rm\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch2 id=\"9-backup-and-restore\" class=\"mobile-header\"\u003e9. Backup and Restore\u003c/h2\u003e\n\u003ch3 id=\"model-backup\" class=\"mobile-header\"\u003eModel Backup\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e#!/bin/bash\n# backup-ollama-models.sh\n\nBACKUP_DIR=\u0026quot;/var/backups/ollama\u0026quot;\nMODELS_DIR=\u0026quot;/var/lib/ollama/models\u0026quot;\nDATE=$(date +%Y%m%d_%H%M%S)\n\n# Create backup directory\nmkdir -p $BACKUP_DIR\n\n# Backup models directory\ntar -czf $BACKUP_DIR/ollama_models_$DATE.tar.gz -C /var/lib/ollama models\n\n# Backup model list\nollama list \u0026gt; $BACKUP_DIR/ollama_models_list_$DATE.txt\n\necho \u0026quot;Backup completed: $BACKUP_DIR/ollama_models_$DATE.tar.gz\u0026quot;\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"configuration-backup\" class=\"mobile-header\"\u003eConfiguration Backup\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e#!/bin/bash\n# backup-ollama-config.sh\n\nBACKUP_DIR=\u0026quot;/var/backups/ollama\u0026quot;\nDATE=$(date +%Y%m%d_%H%M%S)\n\n# Backup configuration\ntar -czf $BACKUP_DIR/ollama_config_$DATE.tar.gz \\\n    /etc/systemd/system/ollama.service \\\n    /etc/systemd/system/ollama.service.d/ 2\u0026gt;/dev/null || true\n\necho \u0026quot;Configuration backup: $BACKUP_DIR/ollama_config_$DATE.tar.gz\u0026quot;\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"restore-procedures\" class=\"mobile-header\"\u003eRestore Procedures\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Restore models\nsudo systemctl stop ollama\nsudo tar -xzf ollama_models_backup.tar.gz -C /var/lib/ollama\nsudo chown -R ollama:ollama /var/lib/ollama/models\nsudo systemctl start ollama\n\n# Verify restored models\nollama list\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"automated-backup\" class=\"mobile-header\"\u003eAutomated Backup\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Add to crontab\nsudo crontab -e\n\n# Daily model backup at 2 AM\n0 2 * * * /opt/ollama/scripts/backup-ollama-models.sh\n\n# Weekly configuration backup\n0 3 * * 0 /opt/ollama/scripts/backup-ollama-config.sh\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch2 id=\"10-system-requirements\" class=\"mobile-header\"\u003e10. System Requirements\u003c/h2\u003e\n\u003ch3 id=\"minimum-requirements\" class=\"mobile-header\"\u003eMinimum Requirements\u003c/h3\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eCPU\u003c/strong\u003e: 2 cores, 2.0 GHz\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eRAM\u003c/strong\u003e: 8GB\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eStorage\u003c/strong\u003e: 20GB (small models)\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eNetwork\u003c/strong\u003e: Broadband for model downloads\u003c/li\u003e\n\u003ch3 id=\"recommended-requirements\" class=\"mobile-header\"\u003eRecommended Requirements\u003c/h3\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eCPU\u003c/strong\u003e: 8+ cores, 3.0+ GHz\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eRAM\u003c/strong\u003e: 32GB+\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eGPU\u003c/strong\u003e: NVIDIA RTX 3060+ or AMD RX 6600 XT+\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eStorage\u003c/strong\u003e: 100GB+ SSD/NVMe\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eNetwork\u003c/strong\u003e: Gigabit for large model downloads\u003c/li\u003e\n\u003ch3 id=\"model-specific-requirements\" class=\"mobile-header\"\u003eModel-Specific Requirements\u003c/h3\u003e\n\u003cp class=\"mobile-paragraph\"\u003e| Model Size | RAM Required | VRAM Required | Storage |\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003e|------------|--------------|---------------|---------|\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003e| 7B         | 8GB          | 4GB           | 4GB     |\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003e| 13B        | 16GB         | 8GB           | 7GB     |\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003e| 30B        | 32GB         | 20GB          | 19GB    |\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003e| 70B        | 64GB         | 48GB          | 39GB    |\u003c/p\u003e\n\u003ch2 id=\"11-support\" class=\"mobile-header\"\u003e11. Support\u003c/h2\u003e\n\u003ch3 id=\"official-resources\" class=\"mobile-header\"\u003eOfficial Resources\u003c/h3\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eWebsite\u003c/strong\u003e: https://ollama.com\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eGitHub\u003c/strong\u003e: https://github.com/ollama/ollama\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eDocumentation\u003c/strong\u003e: https://github.com/ollama/ollama/tree/main/docs\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eModel Library\u003c/strong\u003e: https://ollama.com/library\u003c/li\u003e\n\u003ch3 id=\"community-support\" class=\"mobile-header\"\u003eCommunity Support\u003c/h3\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eDiscord\u003c/strong\u003e: https://discord.gg/ollama\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eReddit\u003c/strong\u003e: r/ollama\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eGitHub Issues\u003c/strong\u003e: https://github.com/ollama/ollama/issues\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eDiscussions\u003c/strong\u003e: https://github.com/ollama/ollama/discussions\u003c/li\u003e\n\u003ch2 id=\"12-contributing\" class=\"mobile-header\"\u003e12. Contributing\u003c/h2\u003e\n\u003ch3 id=\"how-to-contribute\" class=\"mobile-header\"\u003eHow to Contribute\u003c/h3\u003e\n\u003cp class=\"mobile-paragraph\"\u003e1. Fork the repository on GitHub\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003e2. Create a feature branch\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003e3. Submit pull request\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003e4. Follow Go coding standards\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003e5. Include tests and documentation\u003c/p\u003e\n\u003ch3 id=\"development-setup\" class=\"mobile-header\"\u003eDevelopment Setup\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Clone repository\ngit clone https://github.com/ollama/ollama.git\ncd ollama\n\n# Install Go dependencies\ngo mod tidy\n\n# Build from source\ngo build .\n\n# Run tests\ngo test ./...\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch2 id=\"13-license\" class=\"mobile-header\"\u003e13. License\u003c/h2\u003e\n\u003cp class=\"mobile-paragraph\"\u003eOllama is licensed under the MIT License.\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003eKey points:\u003c/p\u003e\n\u003cli class=\"mobile-list-item\"\u003eFree to use, modify, and distribute\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003eCommercial use allowed\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003eNo warranty provided\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003eAttribution required\u003c/li\u003e\n\u003ch2 id=\"14-acknowledgments\" class=\"mobile-header\"\u003e14. Acknowledgments\u003c/h2\u003e\n\u003ch3 id=\"credits\" class=\"mobile-header\"\u003eCredits\u003c/h3\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eOllama Team\u003c/strong\u003e: Core development team\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eMeta AI\u003c/strong\u003e: Llama model family\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eMistral AI\u003c/strong\u003e: Mistral models\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eCommunity\u003c/strong\u003e: Model creators and contributors\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eHardware Vendors\u003c/strong\u003e: NVIDIA, AMD, Apple for acceleration support\u003c/li\u003e\n\u003ch2 id=\"15-version-history\" class=\"mobile-header\"\u003e15. Version History\u003c/h2\u003e\n\u003ch3 id=\"recent-releases\" class=\"mobile-header\"\u003eRecent Releases\u003c/h3\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003ev0.3.x\u003c/strong\u003e: Latest stable with improved performance\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003ev0.2.x\u003c/strong\u003e: Added model management improvements\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003ev0.1.x\u003c/strong\u003e: Initial public release\u003c/li\u003e\n\u003ch3 id=\"major-features-by-version\" class=\"mobile-header\"\u003eMajor Features by Version\u003c/h3\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003ev0.3.0\u003c/strong\u003e: Enhanced GPU support, model compression\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003ev0.2.0\u003c/strong\u003e: REST API improvements, concurrent requests\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003ev0.1.0\u003c/strong\u003e: Basic model serving and CLI interface\u003c/li\u003e\n\u003ch2 id=\"16-appendices\" class=\"mobile-header\"\u003e16. Appendices\u003c/h2\u003e\n\u003ch3 id=\"a-api-usage-examples\" class=\"mobile-header\"\u003eA. API Usage Examples\u003c/h3\u003e\n\u003cp class=\"mobile-paragraph\"\u003e#### Basic Chat Completion\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003ecurl http://localhost:11434/api/generate -d \u0026#039;{\n  \u0026quot;model\u0026quot;: \u0026quot;llama3.1:8b\u0026quot;,\n  \u0026quot;prompt\u0026quot;: \u0026quot;Why is the sky blue?\u0026quot;,\n  \u0026quot;stream\u0026quot;: false\n}\u0026#039;\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003cp class=\"mobile-paragraph\"\u003e#### Streaming Response\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003ecurl http://localhost:11434/api/generate -d \u0026#039;{\n  \u0026quot;model\u0026quot;: \u0026quot;llama3.1:8b\u0026quot;,\n  \u0026quot;prompt\u0026quot;: \u0026quot;Write a poem about coding\u0026quot;,\n  \u0026quot;stream\u0026quot;: true\n}\u0026#039;\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003cp class=\"mobile-paragraph\"\u003e#### Chat API\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003ecurl http://localhost:11434/api/chat -d \u0026#039;{\n  \u0026quot;model\u0026quot;: \u0026quot;llama3.1:8b\u0026quot;,\n  \u0026quot;messages\u0026quot;: [\n    {\n      \u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;,\n      \u0026quot;content\u0026quot;: \u0026quot;Hello, how are you?\u0026quot;\n    }\n  ]\n}\u0026#039;\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"b-integration-examples\" class=\"mobile-header\"\u003eB. Integration Examples\u003c/h3\u003e\n\u003cp class=\"mobile-paragraph\"\u003e#### Python Integration\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"python\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003epython\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport requests\nimport json\n\ndef chat_with_ollama(prompt, model=\u0026quot;llama3.1:8b\u0026quot;):\n    url = \u0026quot;http://localhost:11434/api/generate\u0026quot;\n    data = {\n        \u0026quot;model\u0026quot;: model,\n        \u0026quot;prompt\u0026quot;: prompt,\n        \u0026quot;stream\u0026quot;: False\n    }\n    \n    response = requests.post(url, json=data)\n    if response.status_code == 200:\n        return response.json()[\u0026quot;response\u0026quot;]\n    else:\n        return \u0026quot;Error: \u0026quot; + str(response.status_code)\n\n# Usage\nresponse = chat_with_ollama(\u0026quot;Explain quantum computing\u0026quot;)\nprint(response)\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003cp class=\"mobile-paragraph\"\u003e#### Node.js Integration\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"javascript\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ejavascript\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-javascript\"\u003econst axios = require(\u0026#039;axios\u0026#039;);\n\nasync function chatWithOllama(prompt, model = \u0026#039;llama3.1:8b\u0026#039;) {\n    try {\n        const response = await axios.post(\u0026#039;http://localhost:11434/api/generate\u0026#039;, {\n            model: model,\n            prompt: prompt,\n            stream: false\n        });\n        \n        return response.data.response;\n    } catch (error) {\n        console.error(\u0026#039;Error:\u0026#039;, error.message);\n        return null;\n    }\n}\n\n// Usage\nchatWithOllama(\u0026#039;What is machine learning?\u0026#039;).then(response =\u0026gt; {\n    console.log(response);\n});\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"c-model-customization\" class=\"mobile-header\"\u003eC. Model Customization\u003c/h3\u003e\n\u003cp class=\"mobile-paragraph\"\u003e#### Creating Custom Models\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Create Modelfile\ncat \u0026gt; Modelfile \u0026lt;\u0026lt; \u0026#039;EOF\u0026#039;\nFROM llama3.1:8b\n\n# Set parameters\nPARAMETER temperature 0.7\nPARAMETER top_p 0.9\n\n# Set system message\nSYSTEM \u0026quot;\u0026quot;\u0026quot;\nYou are a helpful AI assistant specialized in programming.\nAlways provide code examples when relevant.\n\u0026quot;\u0026quot;\u0026quot;\nEOF\n\n# Build custom model\nollama create my-coding-assistant -f Modelfile\n\n# Test custom model\nollama run my-coding-assistant \u0026quot;How do I sort a list in Python?\u0026quot;\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003cp class=\"mobile-paragraph\"\u003e#### Fine-tuning (Advanced)\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Prepare training data (JSONL format)\ncat \u0026gt; training_data.jsonl \u0026lt;\u0026lt; \u0026#039;EOF\u0026#039;\n{\u0026quot;prompt\u0026quot;: \u0026quot;Question: What is Python?\u0026quot;, \u0026quot;completion\u0026quot;: \u0026quot;Python is a programming language...\u0026quot;}\n{\u0026quot;prompt\u0026quot;: \u0026quot;Question: How to install packages?\u0026quot;, \u0026quot;completion\u0026quot;: \u0026quot;Use pip install package_name...\u0026quot;}\nEOF\n\n# Note: Fine-tuning requires additional tools and setup\n# Refer to Ollama documentation for detailed fine-tuning guide\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"d-performance-monitoring\" class=\"mobile-header\"\u003eD. Performance Monitoring\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003eüìã\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e#!/bin/bash\n# monitor-ollama.sh\n\necho \u0026quot;=== Ollama Service Status ===\u0026quot;\nsystemctl status ollama --no-pager\n\necho -e \u0026quot;\\n=== Memory Usage ===\u0026quot;\nps aux | grep ollama | grep -v grep\n\necho -e \u0026quot;\\n=== GPU Usage ===\u0026quot;\nif command -v nvidia-smi \u0026amp;\u0026gt; /dev/null; then\n    nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits\nfi\n\necho -e \u0026quot;\\n=== API Health Check ===\u0026quot;\ncurl -s http://localhost:11434/api/version || echo \u0026quot;API not responding\u0026quot;\n\necho -e \u0026quot;\\n=== Loaded Models ===\u0026quot;\nollama ps\n\necho -e \u0026quot;\\n=== Disk Usage ===\u0026quot;\ndu -sh /var/lib/ollama/models/*\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003cp class=\"mobile-paragraph\"\u003e---\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003eFor more information and updates, visit https://github.com/howtomgr/ollama\u003c/p\u003e","readTime":"12 min","wordCount":2213,"tableOfContents":[{"level":2,"text":"1. Prerequisites","id":"1-prerequisites"},{"level":3,"text":"Hardware Requirements","id":"hardware-requirements"},{"level":3,"text":"Software Requirements","id":"software-requirements"},{"level":3,"text":"Network Requirements","id":"network-requirements"},{"level":2,"text":"2. Supported Operating Systems","id":"2-supported-operating-systems"},{"level":2,"text":"3. Installation","id":"3-installation"},{"level":3,"text":"RHEL/CentOS/Rocky Linux/AlmaLinux","id":"rhelcentosrocky-linuxalmalinux"},{"level":3,"text":"Debian/Ubuntu","id":"debianubuntu"},{"level":3,"text":"Arch Linux","id":"arch-linux"},{"level":3,"text":"Alpine Linux","id":"alpine-linux"},{"level":3,"text":"openSUSE","id":"opensuse"},{"level":3,"text":"macOS","id":"macos"},{"level":3,"text":"Windows","id":"windows"},{"level":2,"text":"4. Configuration","id":"4-configuration"},{"level":3,"text":"Environment Variables","id":"environment-variables"},{"level":3,"text":"Configuration Options","id":"configuration-options"},{"level":3,"text":"Model Management","id":"model-management"},{"level":2,"text":"5. Service Management","id":"5-service-management"},{"level":3,"text":"systemd (Linux)","id":"systemd-linux"},{"level":3,"text":"Manual Service Management","id":"manual-service-management"},{"level":3,"text":"Windows Service Management","id":"windows-service-management"},{"level":2,"text":"6. Troubleshooting","id":"6-troubleshooting"},{"level":3,"text":"Common Issues","id":"common-issues"},{"level":3,"text":"Debug Mode","id":"debug-mode"},{"level":2,"text":"7. Security Considerations","id":"7-security-considerations"},{"level":3,"text":"Network Security","id":"network-security"},{"level":3,"text":"Reverse Proxy Configuration (nginx)","id":"reverse-proxy-configuration-nginx"},{"level":3,"text":"Authentication Setup","id":"authentication-setup"},{"level":3,"text":"File Permissions","id":"file-permissions"},{"level":2,"text":"8. Performance Tuning","id":"8-performance-tuning"},{"level":3,"text":"GPU Optimization","id":"gpu-optimization"},{"level":3,"text":"CPU Optimization","id":"cpu-optimization"},{"level":3,"text":"Memory Management","id":"memory-management"},{"level":3,"text":"Storage Optimization","id":"storage-optimization"},{"level":2,"text":"9. Backup and Restore","id":"9-backup-and-restore"},{"level":3,"text":"Model Backup","id":"model-backup"},{"level":3,"text":"Configuration Backup","id":"configuration-backup"},{"level":3,"text":"Restore Procedures","id":"restore-procedures"},{"level":3,"text":"Automated Backup","id":"automated-backup"},{"level":2,"text":"10. System Requirements","id":"10-system-requirements"},{"level":3,"text":"Minimum Requirements","id":"minimum-requirements"},{"level":3,"text":"Recommended Requirements","id":"recommended-requirements"},{"level":3,"text":"Model-Specific Requirements","id":"model-specific-requirements"},{"level":2,"text":"11. Support","id":"11-support"},{"level":3,"text":"Official Resources","id":"official-resources"},{"level":3,"text":"Community Support","id":"community-support"},{"level":2,"text":"12. Contributing","id":"12-contributing"},{"level":3,"text":"How to Contribute","id":"how-to-contribute"},{"level":3,"text":"Development Setup","id":"development-setup"},{"level":2,"text":"13. License","id":"13-license"},{"level":2,"text":"14. Acknowledgments","id":"14-acknowledgments"},{"level":3,"text":"Credits","id":"credits"},{"level":2,"text":"15. Version History","id":"15-version-history"},{"level":3,"text":"Recent Releases","id":"recent-releases"},{"level":3,"text":"Major Features by Version","id":"major-features-by-version"},{"level":2,"text":"16. Appendices","id":"16-appendices"},{"level":3,"text":"A. API Usage Examples","id":"a-api-usage-examples"},{"level":3,"text":"B. Integration Examples","id":"b-integration-examples"},{"level":3,"text":"C. Model Customization","id":"c-model-customization"},{"level":3,"text":"D. Performance Monitoring","id":"d-performance-monitoring"}],"lastBuilt":"2025-10-18T06:15:18.356Z","metadataVersion":"2.0"},"category":{"key":"artificial-intelligence","name":"Artificial intelligence","description":"Artificial intelligence tools and applications","icon":"üèóÔ∏è","color":"#44475a"},"relatedGuides":[]},"__N_SSG":true},"page":"/[category]/[guide]","query":{"category":"artificial-intelligence","guide":"ollama"},"buildId":"rRdSpB4o7Zzz13PZ1FjN4","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>