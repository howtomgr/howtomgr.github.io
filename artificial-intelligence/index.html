<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, user-scalable=yes" data-next-head=""/><meta name="theme-color" content="#bd93f9" data-next-head=""/><link rel="icon" href="/favicon.ico" data-next-head=""/><link rel="preconnect" href="https://fonts.googleapis.com" data-next-head=""/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="true" data-next-head=""/><title class="jsx-6ca0ba86c5b4bb40" data-next-head="">Artificial intelligence Installation Guides - HowToMgr</title><meta name="description" content="1 artificial intelligence installation guides. Artificial intelligence tools and applications" class="jsx-6ca0ba86c5b4bb40" data-next-head=""/><meta name="keywords" content="installation guides, tutorials, linux, docker, kubernetes, mobile-first, responsive" class="jsx-6ca0ba86c5b4bb40" data-next-head=""/><meta property="og:title" content="Artificial intelligence Installation Guides - HowToMgr" class="jsx-6ca0ba86c5b4bb40" data-next-head=""/><meta property="og:description" content="1 artificial intelligence installation guides. Artificial intelligence tools and applications" class="jsx-6ca0ba86c5b4bb40" data-next-head=""/><meta property="og:type" content="website" class="jsx-6ca0ba86c5b4bb40" data-next-head=""/><meta property="og:url" content="https://howtomgr.github.io" class="jsx-6ca0ba86c5b4bb40" data-next-head=""/><meta property="og:image" content="https://howtomgr.github.io/social-preview.png" class="jsx-6ca0ba86c5b4bb40" data-next-head=""/><meta property="twitter:card" content="summary_large_image" class="jsx-6ca0ba86c5b4bb40" data-next-head=""/><meta property="twitter:title" content="Artificial intelligence Installation Guides - HowToMgr" class="jsx-6ca0ba86c5b4bb40" data-next-head=""/><meta property="twitter:description" content="1 artificial intelligence installation guides. Artificial intelligence tools and applications" class="jsx-6ca0ba86c5b4bb40" data-next-head=""/><link rel="dns-prefetch" href="//api.github.com"/><link rel="dns-prefetch" href="//github.com"/><link rel="preload" href="/favicon.ico" as="image"/><meta name="application-name" content="HowToMgr"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="default"/><meta name="apple-mobile-web-app-title" content="HowToMgr"/><meta name="format-detection" content="telephone=no"/><meta name="mobile-web-app-capable" content="yes"/><noscript><style>
            .requires-js { display: none !important; }
            .no-js-message { display: block !important; }
          </style></noscript><link rel="preload" href="/_next/static/css/8f63910774442189.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8f63910774442189.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-8cac0b4b405cede1.js" defer=""></script><script src="/_next/static/chunks/framework-a6e0b7e30f98059a.js" defer=""></script><script src="/_next/static/chunks/main-453f16463ca269d8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-2c7289aecf732a54.js" defer=""></script><script src="/_next/static/chunks/473-0e819be9d34ec59b.js" defer=""></script><script src="/_next/static/chunks/pages/%5Bcategory%5D-74bd91863765ea88.js" defer=""></script><script src="/_next/static/OdIEZsIQLqybmcgo39fkq/_buildManifest.js" defer=""></script><script src="/_next/static/OdIEZsIQLqybmcgo39fkq/_ssgManifest.js" defer=""></script><style id="__jsx-fba8d10132152d7">.search-box.jsx-fba8d10132152d7{position:relative;width:100%;max-width:500px}.search-input-container.jsx-fba8d10132152d7{position:relative;display:flex;align-items:center}.search-input.jsx-fba8d10132152d7{width:100%;padding:var(--space-3)var(--space-4);padding-right:80px;border:2px solid var(--bg-surface);border-radius:var(--border-radius-lg);background:var(--bg-primary);color:var(--text-primary);font-size:var(--font-base);transition:var(--transition);min-height:48px}.search-input.jsx-fba8d10132152d7:focus{border-color:var(--accent-primary);outline:none;box-shadow:0 0 0 3px rgba(189,147,249,.15)}.search-indicator.jsx-fba8d10132152d7{position:absolute;right:50px;color:var(--text-muted);pointer-events:none}.search-clear.jsx-fba8d10132152d7{position:absolute;right:var(--space-3);background:var(--bg-surface);border:none;border-radius:50%;width:24px;height:24px;display:flex;align-items:center;justify-content:center;cursor:pointer;color:var(--text-muted);font-size:var(--font-sm)}.search-clear.jsx-fba8d10132152d7:hover{background:var(--accent-error);color:var(--bg-primary)}.search-results.jsx-fba8d10132152d7{position:absolute;top:100%;left:0;right:0;background:var(--bg-secondary);border:1px solid var(--bg-surface);border-radius:var(--border-radius-lg);box-shadow:var(--box-shadow-lg);max-height:400px;overflow-y:auto;z-index:var(--z-dropdown);margin-top:var(--space-2)}.search-result-item.jsx-fba8d10132152d7{display:block;padding:var(--space-4);border-bottom:1px solid var(--bg-surface);cursor:pointer;transition:var(--transition);text-decoration:none;color:inherit}.search-result-item.jsx-fba8d10132152d7:last-child{border-bottom:none}.search-result-item.jsx-fba8d10132152d7:hover,.search-result-item.selected.jsx-fba8d10132152d7{background:var(--bg-surface);text-decoration:none;color:inherit}.result-header.jsx-fba8d10132152d7{display:flex;justify-content:space-between;align-items:flex-start;gap:var(--space-3);margin-bottom:var(--space-2)}.result-title.jsx-fba8d10132152d7{color:var(--accent-primary);font-size:var(--font-base);font-weight:600;margin:0}.result-badges.jsx-fba8d10132152d7{display:flex;gap:var(--space-1)}.result-category.jsx-fba8d10132152d7,.result-language.jsx-fba8d10132152d7{padding:var(--space-1)var(--space-2);border-radius:var(--border-radius-sm);font-size:var(--font-xs);font-weight:600;text-transform:uppercase}.result-category.jsx-fba8d10132152d7{background:var(--accent-primary);color:var(--bg-primary)}.result-language.jsx-fba8d10132152d7{background:var(--accent-secondary);color:var(--bg-primary)}.result-description.jsx-fba8d10132152d7{color:var(--text-secondary);font-size:var(--font-sm);margin:0 0 var(--space-2)0;line-height:1.4}.result-meta.jsx-fba8d10132152d7{display:flex;gap:var(--space-3);font-size:var(--font-xs);color:var(--text-muted)}.search-empty.jsx-fba8d10132152d7{padding:var(--space-6)var(--space-4);text-align:center;color:var(--text-secondary)}.empty-icon.jsx-fba8d10132152d7{font-size:var(--font-2xl);margin-bottom:var(--space-3);opacity:.5}.empty-text.jsx-fba8d10132152d7 strong.jsx-fba8d10132152d7{display:block;color:var(--text-primary);margin-bottom:var(--space-2)}.empty-text.jsx-fba8d10132152d7 p.jsx-fba8d10132152d7{margin:0;font-size:var(--font-sm)}@media(max-width:767px){.search-input.jsx-fba8d10132152d7{min-height:52px;font-size:var(--font-base)}.result-header.jsx-fba8d10132152d7{flex-direction:column;align-items:flex-start;gap:var(--space-2)}.result-badges.jsx-fba8d10132152d7{order:-1}}</style><style id="__jsx-6e7ce48e453c7e27">.breadcrumb.jsx-6e7ce48e453c7e27{display:flex;align-items:center;gap:var(--space-2);margin-bottom:var(--space-6);font-size:var(--font-sm);color:var(--text-secondary)}.breadcrumb.jsx-6e7ce48e453c7e27 a.jsx-6e7ce48e453c7e27{color:var(--accent-primary);text-decoration:none}.breadcrumb.jsx-6e7ce48e453c7e27 a.jsx-6e7ce48e453c7e27:hover{text-decoration:underline}.separator.jsx-6e7ce48e453c7e27{color:var(--text-muted)}.current.jsx-6e7ce48e453c7e27{color:var(--text-primary);font-weight:500}.category-header.jsx-6e7ce48e453c7e27{text-align:center;margin-bottom:var(--space-8);padding:var(--space-6)0;background:var(--bg-secondary);border-radius:var(--border-radius-lg)}.category-icon.jsx-6e7ce48e453c7e27{font-size:3rem;margin-bottom:var(--space-4)}.category-title.jsx-6e7ce48e453c7e27{color:var(--accent-primary);font-size:var(--font-3xl);margin-bottom:var(--space-4)}.category-description.jsx-6e7ce48e453c7e27{color:var(--text-secondary);font-size:var(--font-lg);margin-bottom:var(--space-4);max-width:600px;margin-left:auto;margin-right:auto}.category-stats.jsx-6e7ce48e453c7e27{margin-top:var(--space-4)}.category-search.jsx-6e7ce48e453c7e27{margin-bottom:var(--space-6);display:flex;justify-content:center}.controls-section.jsx-6e7ce48e453c7e27{margin-bottom:var(--space-6);display:flex;justify-content:center}.sort-select.jsx-6e7ce48e453c7e27{padding:var(--space-2)var(--space-4);border:1px solid var(--bg-surface);border-radius:var(--border-radius);background:var(--bg-primary);color:var(--text-primary);font-size:var(--font-sm)}.guides-section.jsx-6e7ce48e453c7e27{margin-bottom:var(--space-8)}.guide-card.jsx-6e7ce48e453c7e27:hover .mobile-card-title.jsx-6e7ce48e453c7e27{color:var(--accent-secondary)}.empty-state.jsx-6e7ce48e453c7e27{text-align:center;padding:var(--space-8);color:var(--text-secondary)}.empty-state.jsx-6e7ce48e453c7e27 h3.jsx-6e7ce48e453c7e27{color:var(--text-primary);margin-bottom:var(--space-4)}.loading-container.jsx-6e7ce48e453c7e27{text-align:center;padding:var(--space-8);color:var(--text-secondary)}.error-container.jsx-6e7ce48e453c7e27{text-align:center;padding:var(--space-8)}@media(max-width:767px){.category-header.jsx-6e7ce48e453c7e27{padding:var(--space-4);margin-bottom:var(--space-6)}.category-title.jsx-6e7ce48e453c7e27{font-size:var(--font-2xl)}.category-description.jsx-6e7ce48e453c7e27{font-size:var(--font-base)}.category-icon.jsx-6e7ce48e453c7e27{font-size:2.5rem}}</style><style id="__jsx-6ca0ba86c5b4bb40">.app-container.jsx-6ca0ba86c5b4bb40{min-height:100vh;display:flex;flex-direction:column;padding-top:56px}.main-content.jsx-6ca0ba86c5b4bb40{flex:1;padding:var(--space-6)0}.mobile-footer.jsx-6ca0ba86c5b4bb40{background:var(--bg-secondary);border-top:1px solid var(--bg-surface);padding:var(--space-6)0;margin-top:auto}.footer-content.jsx-6ca0ba86c5b4bb40{text-align:center}.footer-links.jsx-6ca0ba86c5b4bb40{display:flex;justify-content:center;gap:var(--space-6);margin-bottom:var(--space-4);flex-wrap:wrap}.footer-links.jsx-6ca0ba86c5b4bb40 a.jsx-6ca0ba86c5b4bb40{color:var(--accent-primary);text-decoration:none;font-size:var(--font-sm)}.footer-links.jsx-6ca0ba86c5b4bb40 a.jsx-6ca0ba86c5b4bb40:hover{color:var(--accent-secondary)}.footer-separator.jsx-6ca0ba86c5b4bb40{height:1px;background:var(--bg-surface);margin:var(--space-4)auto;width:200px}.footer-text.jsx-6ca0ba86c5b4bb40,.footer-tech.jsx-6ca0ba86c5b4bb40{color:var(--text-secondary);font-size:var(--font-sm);margin-bottom:var(--space-2)}.footer-disclaimer.jsx-6ca0ba86c5b4bb40{color:var(--text-muted);font-size:var(--font-xs)}.footer-updated.jsx-6ca0ba86c5b4bb40{color:var(--text-muted);font-size:var(--font-xs);margin-top:var(--space-2)}.nav-links.jsx-6ca0ba86c5b4bb40{display:flex;gap:var(--space-4)}.nav-link.jsx-6ca0ba86c5b4bb40{color:var(--text-primary);text-decoration:none;padding:var(--space-2)var(--space-3);border-radius:var(--border-radius);font-size:var(--font-sm);font-weight:500;transition:var(--transition);min-height:36px;display:flex;align-items:center}.nav-link.jsx-6ca0ba86c5b4bb40:hover{background:var(--bg-surface);color:var(--accent-primary);text-decoration:none}[data-theme="light"].jsx-6ca0ba86c5b4bb40 .theme-icon-dark.jsx-6ca0ba86c5b4bb40{display:none}[data-theme="light"].jsx-6ca0ba86c5b4bb40 .theme-icon-light.jsx-6ca0ba86c5b4bb40{display:inline!important}@media(max-width:767px){.main-content.jsx-6ca0ba86c5b4bb40{padding:var(--space-4)0}.footer-links.jsx-6ca0ba86c5b4bb40{gap:var(--space-4)}}</style></head><body><noscript><div class="no-js-message" style="padding:1rem;background:#ff5555;color:#f8f8f2;text-align:center;font-size:0.9rem">This site requires JavaScript to function properly. Please enable JavaScript in your browser.</div></noscript><div id="__next"><div class="jsx-6ca0ba86c5b4bb40 app-container"><a href="#main-content" class="jsx-6ca0ba86c5b4bb40 skip-link">Skip to main content</a><nav class="jsx-6ca0ba86c5b4bb40 mobile-nav"><div class="jsx-6ca0ba86c5b4bb40 mobile-nav-content"><a class="mobile-brand" href="/"><span role="img" aria-label="Books" class="jsx-6ca0ba86c5b4bb40">📚</span>HowToMgr</a><div class="jsx-6ca0ba86c5b4bb40 nav-links"><a class="nav-link" href="/all/">All</a><a class="nav-link" href="/search/">Search</a></div><button aria-label="Toggle theme" title="Switch theme" class="jsx-6ca0ba86c5b4bb40 mobile-theme-toggle touch-target"><span class="jsx-6ca0ba86c5b4bb40 theme-icon-dark">🌙</span><span style="display:none" class="jsx-6ca0ba86c5b4bb40 theme-icon-light">☀️</span></button></div></nav><main id="main-content" tabindex="-1" class="jsx-6ca0ba86c5b4bb40 main-content"><div class="jsx-6ca0ba86c5b4bb40 container"><nav aria-label="Breadcrumb" class="jsx-6e7ce48e453c7e27 breadcrumb"><a href="/">Home</a><span class="jsx-6e7ce48e453c7e27 separator">→</span><span class="jsx-6e7ce48e453c7e27 current">Artificial intelligence</span></nav><section class="jsx-6e7ce48e453c7e27 category-header"><div style="color:#44475a" class="jsx-6e7ce48e453c7e27 category-icon">🏗️</div><h1 class="jsx-6e7ce48e453c7e27 category-title">Artificial intelligence</h1><p class="jsx-6e7ce48e453c7e27 category-description">Artificial intelligence tools and applications</p><div class="jsx-6e7ce48e453c7e27 category-stats"><span class="jsx-6e7ce48e453c7e27 mobile-badge mobile-badge-primary">1<!-- --> guides available</span></div></section><section class="jsx-6e7ce48e453c7e27 category-search"><div class="jsx-fba8d10132152d7 search-box"><div class="jsx-fba8d10132152d7 search-input-container"><input type="search" placeholder="Search artificial intelligence..." autoComplete="off" aria-label="Search installation guides" class="jsx-fba8d10132152d7 search-input" value=""/><div class="jsx-fba8d10132152d7 search-indicator"><span class="jsx-fba8d10132152d7 search-icon">🔍</span></div></div></div></section><section class="jsx-6e7ce48e453c7e27 controls-section"><div class="jsx-6e7ce48e453c7e27 sort-controls"><label for="sort-select" class="jsx-6e7ce48e453c7e27 sr-only">Sort guides</label><select id="sort-select" class="jsx-6e7ce48e453c7e27 sort-select"><option value="name" class="jsx-6e7ce48e453c7e27" selected="">Name (A-Z)</option><option value="updated" class="jsx-6e7ce48e453c7e27">Recently Updated</option><option value="stars" class="jsx-6e7ce48e453c7e27">Most Popular</option></select></div></section><section class="jsx-6e7ce48e453c7e27 guides-section"><div class="jsx-6e7ce48e453c7e27 mobile-grid"><a class="mobile-card guide-card" href="/artificial-intelligence/ollama/"><div class="jsx-6e7ce48e453c7e27 mobile-card-header"><h3 class="jsx-6e7ce48e453c7e27 mobile-card-title">Ollama Installation Guide</h3><p class="jsx-6e7ce48e453c7e27 mobile-card-description">Ollama is a free and open-source tool for running large language models (LLMs) locally on your machine. It serves as a FOSS alternative to cloud-based AI services like OpenAI API, Anthropic Claude API, or Google&#x27;s Gemini API, enabling privacy-focused AI deployment, offline inference, and cost-effective local AI processing.</p></div><div class="jsx-6e7ce48e453c7e27 mobile-card-meta"><span class="jsx-6e7ce48e453c7e27 mobile-badge mobile-badge-info">12 min</span></div></a></div></section></div></main><footer class="jsx-6ca0ba86c5b4bb40 mobile-footer"><div class="jsx-6ca0ba86c5b4bb40 container"><div class="jsx-6ca0ba86c5b4bb40 footer-content"><div class="jsx-6ca0ba86c5b4bb40 footer-links"><a href="/">Home</a><a href="https://github.com/howtomgr" target="_blank" rel="noopener" class="jsx-6ca0ba86c5b4bb40">GitHub</a><a href="https://github.com/howtomgr/howtomgr.github.io" target="_blank" rel="noopener" class="jsx-6ca0ba86c5b4bb40">Source</a></div><div class="jsx-6ca0ba86c5b4bb40 footer-separator"></div><p class="jsx-6ca0ba86c5b4bb40 footer-text">© <!-- -->2025<!-- --> HowToMgr Organization. Open source installation guides.</p><p class="jsx-6ca0ba86c5b4bb40 footer-tech">Built with ❤️ using Next.js + GitHub Pages</p><p class="jsx-6ca0ba86c5b4bb40 footer-disclaimer"><small class="jsx-6ca0ba86c5b4bb40">All guides are provided as-is. Please review and test before production use.</small></p></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"category":{"key":"artificial-intelligence","name":"Artificial intelligence","description":"Artificial intelligence tools and applications","icon":"🏗️","color":"#44475a"},"guides":[{"name":"ollama","displayName":"Ollama Installation Guide","slug":"ollama","description":"Ollama is a free and open-source tool for running large language models (LLMs) locally on your machine. It serves as a FOSS alternative to cloud-based AI services like OpenAI API, Anthropic Claude API, or Google's Gemini API, enabling privacy-focused AI deployment, offline inference, and cost-effective local AI processing.","category":"artificial-intelligence","subcategory":"llm-runners","difficultyLevel":"intermediate","estimatedSetupTime":"15-30 minutes","supportedOS":["rhel","centos","rocky","almalinux","debian","ubuntu","arch","alpine","opensuse","sles","macos","windows"],"defaultPorts":[11434],"installationMethods":["official-installer","package-manager","manual-binary"],"features":["multi-os-support","local-llm-inference","gpu-acceleration","model-management","rest-api","privacy-focused","offline-capable","comprehensive-documentation","security-hardening","performance-optimization","backup-restore-procedures","troubleshooting-guides"],"tags":["ai","llm","machine-learning","local-inference","privacy","openai-alternative","gpu-acceleration","model-serving","rest-api"],"maintenanceStatus":"active","specVersion":"2.0","version":"1.0.0","license":"MIT","websiteUrl":"https://howtomgr.github.io/artificial-intelligence/ollama","documentationUrl":"https://howtomgr.github.io/artificial-intelligence/ollama","language":null,"stars":0,"forks":0,"topics":[],"githubUrl":"https://github.com/howtomgr/ollama","updatedAt":"2025-09-16T17:37:13Z","createdAt":"2025-09-16T10:48:43Z","readmeRaw":"# Ollama Installation Guide\n\nOllama is a free and open-source tool for running large language models (LLMs) locally on your machine. It serves as a FOSS alternative to cloud-based AI services like OpenAI API, Anthropic Claude API, Google's Gemini API, or Azure OpenAI Service. Ollama enables privacy-focused AI deployment, offline inference, and cost-effective local AI processing with support for popular models like Llama 3, Code Llama, Mistral, and many others.\n\n## Table of Contents\n1. [Prerequisites](#prerequisites)\n2. [Supported Operating Systems](#supported-operating-systems)\n3. [Installation](#installation)\n4. [Configuration](#configuration)\n5. [Service Management](#service-management)\n6. [Troubleshooting](#troubleshooting)\n7. [Security Considerations](#security-considerations)\n8. [Performance Tuning](#performance-tuning)\n9. [Backup and Restore](#backup-and-restore)\n10. [System Requirements](#system-requirements)\n11. [Support](#support)\n12. [Contributing](#contributing)\n13. [License](#license)\n14. [Acknowledgments](#acknowledgments)\n15. [Version History](#version-history)\n16. [Appendices](#appendices)\n\n## 1. Prerequisites\n\n### Hardware Requirements\n- **CPU**: Modern 64-bit processor (x86_64 or ARM64)\n- **RAM**: 8GB minimum (16GB+ recommended for larger models)\n- **Storage**: 10GB+ free space for models\n- **GPU**: Optional but recommended (NVIDIA with CUDA, AMD ROCm, or Apple Metal)\n\n### Software Requirements\n- **Operating System**: Linux, macOS, or Windows\n- **Internet**: Required for initial model downloads\n- **Docker**: Optional for containerized deployment\n\n### Network Requirements\n- **Ports**: \n  - 11434: Default API server port\n- **Bandwidth**: High-speed internet for model downloads (models range from 1GB to 70GB+)\n\n## 2. Supported Operating Systems\n\nOllama officially supports:\n- RHEL 8/9 and derivatives (CentOS Stream, Rocky Linux, AlmaLinux)\n- Debian 11/12\n- Ubuntu 20.04 LTS / 22.04 LTS / 24.04 LTS\n- Arch Linux\n- Alpine Linux 3.18+\n- openSUSE Leap 15.5+ / Tumbleweed\n- macOS 11+ (Big Sur and later)\n- Windows 10/11\n\n## 3. Installation\n\n### RHEL/CentOS/Rocky Linux/AlmaLinux\n\n```bash\n# Method 1: Official installer script\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Method 2: Manual installation\n# Download latest release\ncurl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Create ollama user\nsudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama\nsudo usermod -a -G render,video ollama\n\n# Create systemd service\nsudo tee /etc/systemd/system/ollama.service \u003e /dev/null \u003c\u003c 'EOF'\n[Unit]\nDescription=Ollama Service\nAfter=network-online.target\n\n[Service]\nExecStart=/usr/local/bin/ollama serve\nUser=ollama\nGroup=ollama\nRestart=always\nRestartSec=3\nEnvironment=\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\nEnvironment=\"OLLAMA_HOST=0.0.0.0\"\n\n[Install]\nWantedBy=default.target\nEOF\n\n# Enable and start service\nsudo systemctl daemon-reload\nsudo systemctl enable --now ollama\n```\n\n### Debian/Ubuntu\n\n```bash\n# Method 1: Official installer script\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Method 2: Package installation (if available)\n# Add official repository\ncurl -fsSL https://ollama.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/ollama-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/ollama-keyring.gpg] https://ollama.com/debian stable main\" | sudo tee /etc/apt/sources.list.d/ollama.list\n\n# Install package\nsudo apt update\nsudo apt install -y ollama\n\n# Start service\nsudo systemctl enable --now ollama\n```\n\n### Arch Linux\n\n```bash\n# Install from AUR\nyay -S ollama-bin\n# or\nparu -S ollama-bin\n\n# Alternative: Build from source\nyay -S ollama\n\n# Enable and start service\nsudo systemctl enable --now ollama\n```\n\n### Alpine Linux\n\n```bash\n# Install dependencies\napk add --no-cache curl\n\n# Install Ollama binary\ncurl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Create ollama user\nadduser -D -s /bin/false -h /usr/share/ollama ollama\naddgroup ollama video\naddgroup ollama render\n\n# Create OpenRC service\ntee /etc/init.d/ollama \u003e /dev/null \u003c\u003c 'EOF'\n#!/sbin/openrc-run\n\ndescription=\"Ollama Service\"\ncommand=\"/usr/local/bin/ollama\"\ncommand_args=\"serve\"\ncommand_user=\"ollama\"\ncommand_group=\"ollama\"\npidfile=\"/run/ollama.pid\"\ncommand_background=\"yes\"\n\ndepend() {\n    need net\n    after firewall\n}\n\nstart_pre() {\n    export OLLAMA_HOST=\"0.0.0.0\"\n    checkpath --directory --owner ollama:ollama --mode 0755 /run/ollama\n}\nEOF\n\nchmod +x /etc/init.d/ollama\nrc-update add ollama default\nrc-service ollama start\n```\n\n### openSUSE\n\n```bash\n# Install via zypper (if available) or manual installation\nsudo zypper refresh\n\n# Manual installation\ncurl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Create ollama user\nsudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama\nsudo usermod -a -G video,render ollama\n\n# Create systemd service (same as RHEL)\nsudo systemctl enable --now ollama\n```\n\n### macOS\n\n```bash\n# Method 1: Official app installer\n# Download from https://ollama.com/download/mac\n\n# Method 2: Homebrew\nbrew install ollama\n\n# Method 3: Manual installation\ncurl -L https://ollama.com/download/darwin-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Start Ollama\nollama serve \u0026\n```\n\n### Windows\n\n```powershell\n# Method 1: Official installer\n# Download and run installer from https://ollama.com/download/windows\n\n# Method 2: Winget\nwinget install Ollama.Ollama\n\n# Method 3: Chocolatey\nchoco install ollama\n\n# Method 4: Scoop\nscoop bucket add extras\nscoop install ollama\n\n# Start Ollama service (automatic with installer)\n```\n\n## 4. Configuration\n\n### Environment Variables\n\nCreate `/etc/systemd/system/ollama.service.d/override.conf`:\n```ini\n[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0:11434\"\nEnvironment=\"OLLAMA_MODELS=/var/lib/ollama/models\"\nEnvironment=\"OLLAMA_NUM_PARALLEL=2\"\nEnvironment=\"OLLAMA_MAX_LOADED_MODELS=3\"\nEnvironment=\"OLLAMA_FLASH_ATTENTION=1\"\n```\n\n### Configuration Options\n\n```bash\n# Set custom models directory\nexport OLLAMA_MODELS=/custom/path/to/models\n\n# Configure host and port\nexport OLLAMA_HOST=127.0.0.1:11434\n\n# GPU configuration\nexport CUDA_VISIBLE_DEVICES=0,1  # Use specific GPUs\nexport OLLAMA_GPU_OVERHEAD=0     # Reduce GPU memory overhead\n\n# Performance tuning\nexport OLLAMA_NUM_PARALLEL=4     # Parallel requests\nexport OLLAMA_MAX_LOADED_MODELS=2 # Max models in memory\nexport OLLAMA_FLASH_ATTENTION=1  # Enable flash attention\n```\n\n### Model Management\n\n```bash\n# Download and run models\nollama pull llama3.1:8b\nollama pull codellama:13b\nollama pull mistral:7b\n\n# List installed models\nollama list\n\n# Run a model interactively\nollama run llama3.1:8b\n\n# Remove a model\nollama rm llama3.1:8b\n\n# Show model information\nollama show llama3.1:8b\n```\n\n## 5. Service Management\n\n### systemd (Linux)\n\n```bash\n# Start/stop/restart service\nsudo systemctl start ollama\nsudo systemctl stop ollama\nsudo systemctl restart ollama\n\n# Check service status\nsudo systemctl status ollama\n\n# View logs\nsudo journalctl -u ollama -f\n\n# Enable/disable auto-start\nsudo systemctl enable ollama\nsudo systemctl disable ollama\n```\n\n### Manual Service Management\n\n```bash\n# Start Ollama server\nollama serve\n\n# Start with custom configuration\nOLLAMA_HOST=0.0.0.0:11434 ollama serve\n\n# Background process\nnohup ollama serve \u003e /var/log/ollama.log 2\u003e\u00261 \u0026\n```\n\n### Windows Service Management\n\n```powershell\n# Check service status\nGet-Service Ollama\n\n# Start/stop service\nStart-Service Ollama\nStop-Service Ollama\n\n# Restart service\nRestart-Service Ollama\n```\n\n## 6. Troubleshooting\n\n### Common Issues\n\n1. **Service won't start**:\n```bash\n# Check logs\nsudo journalctl -u ollama -n 50\n\n# Check if port is in use\nsudo netstat -tlnp | grep 11434\n\n# Verify user permissions\nsudo -u ollama /usr/local/bin/ollama serve\n```\n\n2. **GPU not detected**:\n```bash\n# Check NVIDIA GPU\nnvidia-smi\n\n# Check CUDA installation\nnvcc --version\n\n# Check Ollama GPU support\nollama info\n```\n\n3. **Model download fails**:\n```bash\n# Check internet connectivity\ncurl -I https://ollama.com\n\n# Check disk space\ndf -h /var/lib/ollama\n\n# Manual model download\ncurl -L https://huggingface.co/model-url -o model-file\n```\n\n4. **High memory usage**:\n```bash\n# Check model memory usage\nollama ps\n\n# Reduce loaded models\nexport OLLAMA_MAX_LOADED_MODELS=1\n\n# Monitor system resources\nhtop\n```\n\n### Debug Mode\n\n```bash\n# Enable debug logging\nexport OLLAMA_DEBUG=1\nollama serve\n\n# Verbose API logging\nexport OLLAMA_VERBOSE=1\n```\n\n## 7. Security Considerations\n\n### Network Security\n\n```bash\n# Bind to localhost only (default)\nexport OLLAMA_HOST=127.0.0.1:11434\n\n# Configure firewall (if exposing externally)\nsudo firewall-cmd --permanent --add-port=11434/tcp\nsudo firewall-cmd --reload\n\n# Use reverse proxy for external access\n```\n\n### Reverse Proxy Configuration (nginx)\n\n```nginx\nserver {\n    listen 80;\n    server_name ollama.example.com;\n    \n    location / {\n        proxy_pass http://127.0.0.1:11434;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n```\n\n### Authentication Setup\n\n```bash\n# Ollama doesn't have built-in auth, use reverse proxy\n# Example with basic auth in nginx:\nsudo apt install apache2-utils\nsudo htpasswd -c /etc/nginx/.htpasswd ollama_user\n\n# Add to nginx config:\n# auth_basic \"Ollama Access\";\n# auth_basic_user_file /etc/nginx/.htpasswd;\n```\n\n### File Permissions\n\n```bash\n# Secure model directory\nsudo chown -R ollama:ollama /var/lib/ollama\nsudo chmod -R 750 /var/lib/ollama\n\n# Secure configuration files\nsudo chmod 640 /etc/systemd/system/ollama.service\nsudo chown root:root /etc/systemd/system/ollama.service\n```\n\n## 8. Performance Tuning\n\n### GPU Optimization\n\n```bash\n# NVIDIA GPU settings\nexport CUDA_VISIBLE_DEVICES=0,1\nexport OLLAMA_GPU_OVERHEAD=0\n\n# Check GPU utilization\nnvidia-smi -l 1\n\n# AMD GPU (ROCm)\nexport HSA_OVERRIDE_GFX_VERSION=10.3.0\nexport ROCM_PATH=/opt/rocm\n```\n\n### CPU Optimization\n\n```bash\n# Set CPU affinity\ntaskset -c 0-7 ollama serve\n\n# Adjust parallel processing\nexport OLLAMA_NUM_PARALLEL=4\nexport OLLAMA_MAX_LOADED_MODELS=2\n\n# Enable optimizations\nexport OLLAMA_FLASH_ATTENTION=1\nexport OLLAMA_NUMA_PREFER=0\n```\n\n### Memory Management\n\n```bash\n# Monitor memory usage\nwatch -n 1 'free -h \u0026\u0026 echo \"=== Ollama Process ===\" \u0026\u0026 ps aux | grep ollama'\n\n# Limit model cache\nexport OLLAMA_MAX_LOADED_MODELS=1\n\n# Use swap if needed (not recommended for production)\nsudo swapon --show\n```\n\n### Storage Optimization\n\n```bash\n# Use SSD for models\nsudo mkdir -p /mnt/ssd/ollama/models\nsudo chown ollama:ollama /mnt/ssd/ollama/models\nexport OLLAMA_MODELS=/mnt/ssd/ollama/models\n\n# Clean up unused models\nollama list | grep -v \"NAME\" | awk '{print $1}' | xargs ollama rm\n```\n\n## 9. Backup and Restore\n\n### Model Backup\n\n```bash\n#!/bin/bash\n# backup-ollama-models.sh\n\nBACKUP_DIR=\"/var/backups/ollama\"\nMODELS_DIR=\"/var/lib/ollama/models\"\nDATE=$(date +%Y%m%d_%H%M%S)\n\n# Create backup directory\nmkdir -p $BACKUP_DIR\n\n# Backup models directory\ntar -czf $BACKUP_DIR/ollama_models_$DATE.tar.gz -C /var/lib/ollama models\n\n# Backup model list\nollama list \u003e $BACKUP_DIR/ollama_models_list_$DATE.txt\n\necho \"Backup completed: $BACKUP_DIR/ollama_models_$DATE.tar.gz\"\n```\n\n### Configuration Backup\n\n```bash\n#!/bin/bash\n# backup-ollama-config.sh\n\nBACKUP_DIR=\"/var/backups/ollama\"\nDATE=$(date +%Y%m%d_%H%M%S)\n\n# Backup configuration\ntar -czf $BACKUP_DIR/ollama_config_$DATE.tar.gz \\\n    /etc/systemd/system/ollama.service \\\n    /etc/systemd/system/ollama.service.d/ 2\u003e/dev/null || true\n\necho \"Configuration backup: $BACKUP_DIR/ollama_config_$DATE.tar.gz\"\n```\n\n### Restore Procedures\n\n```bash\n# Restore models\nsudo systemctl stop ollama\nsudo tar -xzf ollama_models_backup.tar.gz -C /var/lib/ollama\nsudo chown -R ollama:ollama /var/lib/ollama/models\nsudo systemctl start ollama\n\n# Verify restored models\nollama list\n```\n\n### Automated Backup\n\n```bash\n# Add to crontab\nsudo crontab -e\n\n# Daily model backup at 2 AM\n0 2 * * * /opt/ollama/scripts/backup-ollama-models.sh\n\n# Weekly configuration backup\n0 3 * * 0 /opt/ollama/scripts/backup-ollama-config.sh\n```\n\n## 10. System Requirements\n\n### Minimum Requirements\n- **CPU**: 2 cores, 2.0 GHz\n- **RAM**: 8GB\n- **Storage**: 20GB (small models)\n- **Network**: Broadband for model downloads\n\n### Recommended Requirements\n- **CPU**: 8+ cores, 3.0+ GHz\n- **RAM**: 32GB+\n- **GPU**: NVIDIA RTX 3060+ or AMD RX 6600 XT+\n- **Storage**: 100GB+ SSD/NVMe\n- **Network**: Gigabit for large model downloads\n\n### Model-Specific Requirements\n\n| Model Size | RAM Required | VRAM Required | Storage |\n|------------|--------------|---------------|---------|\n| 7B         | 8GB          | 4GB           | 4GB     |\n| 13B        | 16GB         | 8GB           | 7GB     |\n| 30B        | 32GB         | 20GB          | 19GB    |\n| 70B        | 64GB         | 48GB          | 39GB    |\n\n## 11. Support\n\n### Official Resources\n- **Website**: https://ollama.com\n- **GitHub**: https://github.com/ollama/ollama\n- **Documentation**: https://github.com/ollama/ollama/tree/main/docs\n- **Model Library**: https://ollama.com/library\n\n### Community Support\n- **Discord**: https://discord.gg/ollama\n- **Reddit**: r/ollama\n- **GitHub Issues**: https://github.com/ollama/ollama/issues\n- **Discussions**: https://github.com/ollama/ollama/discussions\n\n## 12. Contributing\n\n### How to Contribute\n1. Fork the repository on GitHub\n2. Create a feature branch\n3. Submit pull request\n4. Follow Go coding standards\n5. Include tests and documentation\n\n### Development Setup\n```bash\n# Clone repository\ngit clone https://github.com/ollama/ollama.git\ncd ollama\n\n# Install Go dependencies\ngo mod tidy\n\n# Build from source\ngo build .\n\n# Run tests\ngo test ./...\n```\n\n## 13. License\n\nOllama is licensed under the MIT License.\n\nKey points:\n- Free to use, modify, and distribute\n- Commercial use allowed\n- No warranty provided\n- Attribution required\n\n## 14. Acknowledgments\n\n### Credits\n- **Ollama Team**: Core development team\n- **Meta AI**: Llama model family\n- **Mistral AI**: Mistral models\n- **Community**: Model creators and contributors\n- **Hardware Vendors**: NVIDIA, AMD, Apple for acceleration support\n\n## 15. Version History\n\n### Recent Releases\n- **v0.3.x**: Latest stable with improved performance\n- **v0.2.x**: Added model management improvements\n- **v0.1.x**: Initial public release\n\n### Major Features by Version\n- **v0.3.0**: Enhanced GPU support, model compression\n- **v0.2.0**: REST API improvements, concurrent requests\n- **v0.1.0**: Basic model serving and CLI interface\n\n## 16. Appendices\n\n### A. API Usage Examples\n\n#### Basic Chat Completion\n```bash\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.1:8b\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"stream\": false\n}'\n```\n\n#### Streaming Response\n```bash\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.1:8b\",\n  \"prompt\": \"Write a poem about coding\",\n  \"stream\": true\n}'\n```\n\n#### Chat API\n```bash\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.1:8b\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello, how are you?\"\n    }\n  ]\n}'\n```\n\n### B. Integration Examples\n\n#### Python Integration\n```python\nimport requests\nimport json\n\ndef chat_with_ollama(prompt, model=\"llama3.1:8b\"):\n    url = \"http://localhost:11434/api/generate\"\n    data = {\n        \"model\": model,\n        \"prompt\": prompt,\n        \"stream\": False\n    }\n    \n    response = requests.post(url, json=data)\n    if response.status_code == 200:\n        return response.json()[\"response\"]\n    else:\n        return \"Error: \" + str(response.status_code)\n\n# Usage\nresponse = chat_with_ollama(\"Explain quantum computing\")\nprint(response)\n```\n\n#### Node.js Integration\n```javascript\nconst axios = require('axios');\n\nasync function chatWithOllama(prompt, model = 'llama3.1:8b') {\n    try {\n        const response = await axios.post('http://localhost:11434/api/generate', {\n            model: model,\n            prompt: prompt,\n            stream: false\n        });\n        \n        return response.data.response;\n    } catch (error) {\n        console.error('Error:', error.message);\n        return null;\n    }\n}\n\n// Usage\nchatWithOllama('What is machine learning?').then(response =\u003e {\n    console.log(response);\n});\n```\n\n### C. Model Customization\n\n#### Creating Custom Models\n```bash\n# Create Modelfile\ncat \u003e Modelfile \u003c\u003c 'EOF'\nFROM llama3.1:8b\n\n# Set parameters\nPARAMETER temperature 0.7\nPARAMETER top_p 0.9\n\n# Set system message\nSYSTEM \"\"\"\nYou are a helpful AI assistant specialized in programming.\nAlways provide code examples when relevant.\n\"\"\"\nEOF\n\n# Build custom model\nollama create my-coding-assistant -f Modelfile\n\n# Test custom model\nollama run my-coding-assistant \"How do I sort a list in Python?\"\n```\n\n#### Fine-tuning (Advanced)\n```bash\n# Prepare training data (JSONL format)\ncat \u003e training_data.jsonl \u003c\u003c 'EOF'\n{\"prompt\": \"Question: What is Python?\", \"completion\": \"Python is a programming language...\"}\n{\"prompt\": \"Question: How to install packages?\", \"completion\": \"Use pip install package_name...\"}\nEOF\n\n# Note: Fine-tuning requires additional tools and setup\n# Refer to Ollama documentation for detailed fine-tuning guide\n```\n\n### D. Performance Monitoring\n\n```bash\n#!/bin/bash\n# monitor-ollama.sh\n\necho \"=== Ollama Service Status ===\"\nsystemctl status ollama --no-pager\n\necho -e \"\\n=== Memory Usage ===\"\nps aux | grep ollama | grep -v grep\n\necho -e \"\\n=== GPU Usage ===\"\nif command -v nvidia-smi \u0026\u003e /dev/null; then\n    nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits\nfi\n\necho -e \"\\n=== API Health Check ===\"\ncurl -s http://localhost:11434/api/version || echo \"API not responding\"\n\necho -e \"\\n=== Loaded Models ===\"\nollama ps\n\necho -e \"\\n=== Disk Usage ===\"\ndu -sh /var/lib/ollama/models/*\n```\n\n---\n\nFor more information and updates, visit https://github.com/howtomgr/ollama","readmeHtml":"\u003cp class=\"mobile-paragraph\"\u003eOllama is a free and open-source tool for running large language models (LLMs) locally on your machine. It serves as a FOSS alternative to cloud-based AI services like OpenAI API, Anthropic Claude API, Google's Gemini API, or Azure OpenAI Service. Ollama enables privacy-focused AI deployment, offline inference, and cost-effective local AI processing with support for popular models like Llama 3, Code Llama, Mistral, and many others.\u003c/p\u003e\n\u003ch2 id=\"1-prerequisites\" class=\"mobile-header\"\u003e1. Prerequisites\u003c/h2\u003e\n\u003ch3 id=\"hardware-requirements\" class=\"mobile-header\"\u003eHardware Requirements\u003c/h3\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eCPU\u003c/strong\u003e: Modern 64-bit processor (x86_64 or ARM64)\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eRAM\u003c/strong\u003e: 8GB minimum (16GB+ recommended for larger models)\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eStorage\u003c/strong\u003e: 10GB+ free space for models\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eGPU\u003c/strong\u003e: Optional but recommended (NVIDIA with CUDA, AMD ROCm, or Apple Metal)\u003c/li\u003e\n\u003ch3 id=\"software-requirements\" class=\"mobile-header\"\u003eSoftware Requirements\u003c/h3\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eOperating System\u003c/strong\u003e: Linux, macOS, or Windows\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eInternet\u003c/strong\u003e: Required for initial model downloads\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eDocker\u003c/strong\u003e: Optional for containerized deployment\u003c/li\u003e\n\u003ch3 id=\"network-requirements\" class=\"mobile-header\"\u003eNetwork Requirements\u003c/h3\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003ePorts\u003c/strong\u003e: \u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e11434: Default API server port\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eBandwidth\u003c/strong\u003e: High-speed internet for model downloads (models range from 1GB to 70GB+)\u003c/li\u003e\n\u003ch2 id=\"2-supported-operating-systems\" class=\"mobile-header\"\u003e2. Supported Operating Systems\u003c/h2\u003e\n\u003cp class=\"mobile-paragraph\"\u003eOllama officially supports:\u003c/p\u003e\n\u003cli class=\"mobile-list-item\"\u003eRHEL 8/9 and derivatives (CentOS Stream, Rocky Linux, AlmaLinux)\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003eDebian 11/12\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003eUbuntu 20.04 LTS / 22.04 LTS / 24.04 LTS\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003eArch Linux\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003eAlpine Linux 3.18+\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003eopenSUSE Leap 15.5+ / Tumbleweed\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003emacOS 11+ (Big Sur and later)\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003eWindows 10/11\u003c/li\u003e\n\u003ch2 id=\"3-installation\" class=\"mobile-header\"\u003e3. Installation\u003c/h2\u003e\n\u003ch3 id=\"rhelcentosrocky-linuxalmalinux\" class=\"mobile-header\"\u003eRHEL/CentOS/Rocky Linux/AlmaLinux\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Method 1: Official installer script\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Method 2: Manual installation\n# Download latest release\ncurl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Create ollama user\nsudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama\nsudo usermod -a -G render,video ollama\n\n# Create systemd service\nsudo tee /etc/systemd/system/ollama.service \u0026gt; /dev/null \u0026lt;\u0026lt; \u0026#039;EOF\u0026#039;\n[Unit]\nDescription=Ollama Service\nAfter=network-online.target\n\n[Service]\nExecStart=/usr/local/bin/ollama serve\nUser=ollama\nGroup=ollama\nRestart=always\nRestartSec=3\nEnvironment=\u0026quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u0026quot;\nEnvironment=\u0026quot;OLLAMA_HOST=0.0.0.0\u0026quot;\n\n[Install]\nWantedBy=default.target\nEOF\n\n# Enable and start service\nsudo systemctl daemon-reload\nsudo systemctl enable --now ollama\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"debianubuntu\" class=\"mobile-header\"\u003eDebian/Ubuntu\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Method 1: Official installer script\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Method 2: Package installation (if available)\n# Add official repository\ncurl -fsSL https://ollama.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/ollama-keyring.gpg\necho \u0026quot;deb [signed-by=/usr/share/keyrings/ollama-keyring.gpg] https://ollama.com/debian stable main\u0026quot; | sudo tee /etc/apt/sources.list.d/ollama.list\n\n# Install package\nsudo apt update\nsudo apt install -y ollama\n\n# Start service\nsudo systemctl enable --now ollama\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"arch-linux\" class=\"mobile-header\"\u003eArch Linux\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Install from AUR\nyay -S ollama-bin\n# or\nparu -S ollama-bin\n\n# Alternative: Build from source\nyay -S ollama\n\n# Enable and start service\nsudo systemctl enable --now ollama\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"alpine-linux\" class=\"mobile-header\"\u003eAlpine Linux\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Install dependencies\napk add --no-cache curl\n\n# Install Ollama binary\ncurl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Create ollama user\nadduser -D -s /bin/false -h /usr/share/ollama ollama\naddgroup ollama video\naddgroup ollama render\n\n# Create OpenRC service\ntee /etc/init.d/ollama \u0026gt; /dev/null \u0026lt;\u0026lt; \u0026#039;EOF\u0026#039;\n#!/sbin/openrc-run\n\ndescription=\u0026quot;Ollama Service\u0026quot;\ncommand=\u0026quot;/usr/local/bin/ollama\u0026quot;\ncommand_args=\u0026quot;serve\u0026quot;\ncommand_user=\u0026quot;ollama\u0026quot;\ncommand_group=\u0026quot;ollama\u0026quot;\npidfile=\u0026quot;/run/ollama.pid\u0026quot;\ncommand_background=\u0026quot;yes\u0026quot;\n\ndepend() {\n    need net\n    after firewall\n}\n\nstart_pre() {\n    export OLLAMA_HOST=\u0026quot;0.0.0.0\u0026quot;\n    checkpath --directory --owner ollama:ollama --mode 0755 /run/ollama\n}\nEOF\n\nchmod +x /etc/init.d/ollama\nrc-update add ollama default\nrc-service ollama start\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"opensuse\" class=\"mobile-header\"\u003eopenSUSE\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Install via zypper (if available) or manual installation\nsudo zypper refresh\n\n# Manual installation\ncurl -L https://ollama.com/download/linux-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Create ollama user\nsudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama\nsudo usermod -a -G video,render ollama\n\n# Create systemd service (same as RHEL)\nsudo systemctl enable --now ollama\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"macos\" class=\"mobile-header\"\u003emacOS\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Method 1: Official app installer\n# Download from https://ollama.com/download/mac\n\n# Method 2: Homebrew\nbrew install ollama\n\n# Method 3: Manual installation\ncurl -L https://ollama.com/download/darwin-amd64 -o /usr/local/bin/ollama\nchmod +x /usr/local/bin/ollama\n\n# Start Ollama\nollama serve \u0026amp;\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"windows\" class=\"mobile-header\"\u003eWindows\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"powershell\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003epowershell\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-powershell\"\u003e# Method 1: Official installer\n# Download and run installer from https://ollama.com/download/windows\n\n# Method 2: Winget\nwinget install Ollama.Ollama\n\n# Method 3: Chocolatey\nchoco install ollama\n\n# Method 4: Scoop\nscoop bucket add extras\nscoop install ollama\n\n# Start Ollama service (automatic with installer)\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch2 id=\"4-configuration\" class=\"mobile-header\"\u003e4. Configuration\u003c/h2\u003e\n\u003ch3 id=\"environment-variables\" class=\"mobile-header\"\u003eEnvironment Variables\u003c/h3\u003e\n\u003cp class=\"mobile-paragraph\"\u003eCreate \u003ccode class=\"mobile-inline-code\"\u003e/etc/systemd/system/ollama.service.d/override.conf\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"ini\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003eini\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-ini\"\u003e[Service]\nEnvironment=\u0026quot;OLLAMA_HOST=0.0.0.0:11434\u0026quot;\nEnvironment=\u0026quot;OLLAMA_MODELS=/var/lib/ollama/models\u0026quot;\nEnvironment=\u0026quot;OLLAMA_NUM_PARALLEL=2\u0026quot;\nEnvironment=\u0026quot;OLLAMA_MAX_LOADED_MODELS=3\u0026quot;\nEnvironment=\u0026quot;OLLAMA_FLASH_ATTENTION=1\u0026quot;\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"configuration-options\" class=\"mobile-header\"\u003eConfiguration Options\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Set custom models directory\nexport OLLAMA_MODELS=/custom/path/to/models\n\n# Configure host and port\nexport OLLAMA_HOST=127.0.0.1:11434\n\n# GPU configuration\nexport CUDA_VISIBLE_DEVICES=0,1  # Use specific GPUs\nexport OLLAMA_GPU_OVERHEAD=0     # Reduce GPU memory overhead\n\n# Performance tuning\nexport OLLAMA_NUM_PARALLEL=4     # Parallel requests\nexport OLLAMA_MAX_LOADED_MODELS=2 # Max models in memory\nexport OLLAMA_FLASH_ATTENTION=1  # Enable flash attention\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"model-management\" class=\"mobile-header\"\u003eModel Management\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Download and run models\nollama pull llama3.1:8b\nollama pull codellama:13b\nollama pull mistral:7b\n\n# List installed models\nollama list\n\n# Run a model interactively\nollama run llama3.1:8b\n\n# Remove a model\nollama rm llama3.1:8b\n\n# Show model information\nollama show llama3.1:8b\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch2 id=\"5-service-management\" class=\"mobile-header\"\u003e5. Service Management\u003c/h2\u003e\n\u003ch3 id=\"systemd-linux\" class=\"mobile-header\"\u003esystemd (Linux)\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Start/stop/restart service\nsudo systemctl start ollama\nsudo systemctl stop ollama\nsudo systemctl restart ollama\n\n# Check service status\nsudo systemctl status ollama\n\n# View logs\nsudo journalctl -u ollama -f\n\n# Enable/disable auto-start\nsudo systemctl enable ollama\nsudo systemctl disable ollama\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"manual-service-management\" class=\"mobile-header\"\u003eManual Service Management\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Start Ollama server\nollama serve\n\n# Start with custom configuration\nOLLAMA_HOST=0.0.0.0:11434 ollama serve\n\n# Background process\nnohup ollama serve \u0026gt; /var/log/ollama.log 2\u0026gt;\u0026amp;1 \u0026amp;\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"windows-service-management\" class=\"mobile-header\"\u003eWindows Service Management\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"powershell\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003epowershell\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-powershell\"\u003e# Check service status\nGet-Service Ollama\n\n# Start/stop service\nStart-Service Ollama\nStop-Service Ollama\n\n# Restart service\nRestart-Service Ollama\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch2 id=\"6-troubleshooting\" class=\"mobile-header\"\u003e6. Troubleshooting\u003c/h2\u003e\n\u003ch3 id=\"common-issues\" class=\"mobile-header\"\u003eCommon Issues\u003c/h3\u003e\n\u003cp class=\"mobile-paragraph\"\u003e1. \u003cstrong\u003eService won't start\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Check logs\nsudo journalctl -u ollama -n 50\n\n# Check if port is in use\nsudo netstat -tlnp | grep 11434\n\n# Verify user permissions\nsudo -u ollama /usr/local/bin/ollama serve\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003cp class=\"mobile-paragraph\"\u003e2. \u003cstrong\u003eGPU not detected\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Check NVIDIA GPU\nnvidia-smi\n\n# Check CUDA installation\nnvcc --version\n\n# Check Ollama GPU support\nollama info\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003cp class=\"mobile-paragraph\"\u003e3. \u003cstrong\u003eModel download fails\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Check internet connectivity\ncurl -I https://ollama.com\n\n# Check disk space\ndf -h /var/lib/ollama\n\n# Manual model download\ncurl -L https://huggingface.co/model-url -o model-file\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003cp class=\"mobile-paragraph\"\u003e4. \u003cstrong\u003eHigh memory usage\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Check model memory usage\nollama ps\n\n# Reduce loaded models\nexport OLLAMA_MAX_LOADED_MODELS=1\n\n# Monitor system resources\nhtop\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"debug-mode\" class=\"mobile-header\"\u003eDebug Mode\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Enable debug logging\nexport OLLAMA_DEBUG=1\nollama serve\n\n# Verbose API logging\nexport OLLAMA_VERBOSE=1\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch2 id=\"7-security-considerations\" class=\"mobile-header\"\u003e7. Security Considerations\u003c/h2\u003e\n\u003ch3 id=\"network-security\" class=\"mobile-header\"\u003eNetwork Security\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Bind to localhost only (default)\nexport OLLAMA_HOST=127.0.0.1:11434\n\n# Configure firewall (if exposing externally)\nsudo firewall-cmd --permanent --add-port=11434/tcp\nsudo firewall-cmd --reload\n\n# Use reverse proxy for external access\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"reverse-proxy-configuration-nginx\" class=\"mobile-header\"\u003eReverse Proxy Configuration (nginx)\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"nginx\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003enginx\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-nginx\"\u003eserver {\n    listen 80;\n    server_name ollama.example.com;\n    \n    location / {\n        proxy_pass http://127.0.0.1:11434;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"authentication-setup\" class=\"mobile-header\"\u003eAuthentication Setup\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Ollama doesn\u0026#039;t have built-in auth, use reverse proxy\n# Example with basic auth in nginx:\nsudo apt install apache2-utils\nsudo htpasswd -c /etc/nginx/.htpasswd ollama_user\n\n# Add to nginx config:\n# auth_basic \u0026quot;Ollama Access\u0026quot;;\n# auth_basic_user_file /etc/nginx/.htpasswd;\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"file-permissions\" class=\"mobile-header\"\u003eFile Permissions\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Secure model directory\nsudo chown -R ollama:ollama /var/lib/ollama\nsudo chmod -R 750 /var/lib/ollama\n\n# Secure configuration files\nsudo chmod 640 /etc/systemd/system/ollama.service\nsudo chown root:root /etc/systemd/system/ollama.service\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch2 id=\"8-performance-tuning\" class=\"mobile-header\"\u003e8. Performance Tuning\u003c/h2\u003e\n\u003ch3 id=\"gpu-optimization\" class=\"mobile-header\"\u003eGPU Optimization\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# NVIDIA GPU settings\nexport CUDA_VISIBLE_DEVICES=0,1\nexport OLLAMA_GPU_OVERHEAD=0\n\n# Check GPU utilization\nnvidia-smi -l 1\n\n# AMD GPU (ROCm)\nexport HSA_OVERRIDE_GFX_VERSION=10.3.0\nexport ROCM_PATH=/opt/rocm\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"cpu-optimization\" class=\"mobile-header\"\u003eCPU Optimization\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Set CPU affinity\ntaskset -c 0-7 ollama serve\n\n# Adjust parallel processing\nexport OLLAMA_NUM_PARALLEL=4\nexport OLLAMA_MAX_LOADED_MODELS=2\n\n# Enable optimizations\nexport OLLAMA_FLASH_ATTENTION=1\nexport OLLAMA_NUMA_PREFER=0\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"memory-management\" class=\"mobile-header\"\u003eMemory Management\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Monitor memory usage\nwatch -n 1 \u0026#039;free -h \u0026amp;\u0026amp; echo \u0026quot;=== Ollama Process ===\u0026quot; \u0026amp;\u0026amp; ps aux | grep ollama\u0026#039;\n\n# Limit model cache\nexport OLLAMA_MAX_LOADED_MODELS=1\n\n# Use swap if needed (not recommended for production)\nsudo swapon --show\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"storage-optimization\" class=\"mobile-header\"\u003eStorage Optimization\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Use SSD for models\nsudo mkdir -p /mnt/ssd/ollama/models\nsudo chown ollama:ollama /mnt/ssd/ollama/models\nexport OLLAMA_MODELS=/mnt/ssd/ollama/models\n\n# Clean up unused models\nollama list | grep -v \u0026quot;NAME\u0026quot; | awk \u0026#039;{print $1}\u0026#039; | xargs ollama rm\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch2 id=\"9-backup-and-restore\" class=\"mobile-header\"\u003e9. Backup and Restore\u003c/h2\u003e\n\u003ch3 id=\"model-backup\" class=\"mobile-header\"\u003eModel Backup\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e#!/bin/bash\n# backup-ollama-models.sh\n\nBACKUP_DIR=\u0026quot;/var/backups/ollama\u0026quot;\nMODELS_DIR=\u0026quot;/var/lib/ollama/models\u0026quot;\nDATE=$(date +%Y%m%d_%H%M%S)\n\n# Create backup directory\nmkdir -p $BACKUP_DIR\n\n# Backup models directory\ntar -czf $BACKUP_DIR/ollama_models_$DATE.tar.gz -C /var/lib/ollama models\n\n# Backup model list\nollama list \u0026gt; $BACKUP_DIR/ollama_models_list_$DATE.txt\n\necho \u0026quot;Backup completed: $BACKUP_DIR/ollama_models_$DATE.tar.gz\u0026quot;\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"configuration-backup\" class=\"mobile-header\"\u003eConfiguration Backup\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e#!/bin/bash\n# backup-ollama-config.sh\n\nBACKUP_DIR=\u0026quot;/var/backups/ollama\u0026quot;\nDATE=$(date +%Y%m%d_%H%M%S)\n\n# Backup configuration\ntar -czf $BACKUP_DIR/ollama_config_$DATE.tar.gz \\\n    /etc/systemd/system/ollama.service \\\n    /etc/systemd/system/ollama.service.d/ 2\u0026gt;/dev/null || true\n\necho \u0026quot;Configuration backup: $BACKUP_DIR/ollama_config_$DATE.tar.gz\u0026quot;\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"restore-procedures\" class=\"mobile-header\"\u003eRestore Procedures\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Restore models\nsudo systemctl stop ollama\nsudo tar -xzf ollama_models_backup.tar.gz -C /var/lib/ollama\nsudo chown -R ollama:ollama /var/lib/ollama/models\nsudo systemctl start ollama\n\n# Verify restored models\nollama list\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"automated-backup\" class=\"mobile-header\"\u003eAutomated Backup\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Add to crontab\nsudo crontab -e\n\n# Daily model backup at 2 AM\n0 2 * * * /opt/ollama/scripts/backup-ollama-models.sh\n\n# Weekly configuration backup\n0 3 * * 0 /opt/ollama/scripts/backup-ollama-config.sh\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch2 id=\"10-system-requirements\" class=\"mobile-header\"\u003e10. System Requirements\u003c/h2\u003e\n\u003ch3 id=\"minimum-requirements\" class=\"mobile-header\"\u003eMinimum Requirements\u003c/h3\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eCPU\u003c/strong\u003e: 2 cores, 2.0 GHz\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eRAM\u003c/strong\u003e: 8GB\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eStorage\u003c/strong\u003e: 20GB (small models)\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eNetwork\u003c/strong\u003e: Broadband for model downloads\u003c/li\u003e\n\u003ch3 id=\"recommended-requirements\" class=\"mobile-header\"\u003eRecommended Requirements\u003c/h3\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eCPU\u003c/strong\u003e: 8+ cores, 3.0+ GHz\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eRAM\u003c/strong\u003e: 32GB+\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eGPU\u003c/strong\u003e: NVIDIA RTX 3060+ or AMD RX 6600 XT+\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eStorage\u003c/strong\u003e: 100GB+ SSD/NVMe\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eNetwork\u003c/strong\u003e: Gigabit for large model downloads\u003c/li\u003e\n\u003ch3 id=\"model-specific-requirements\" class=\"mobile-header\"\u003eModel-Specific Requirements\u003c/h3\u003e\n\u003cp class=\"mobile-paragraph\"\u003e| Model Size | RAM Required | VRAM Required | Storage |\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003e|------------|--------------|---------------|---------|\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003e| 7B         | 8GB          | 4GB           | 4GB     |\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003e| 13B        | 16GB         | 8GB           | 7GB     |\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003e| 30B        | 32GB         | 20GB          | 19GB    |\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003e| 70B        | 64GB         | 48GB          | 39GB    |\u003c/p\u003e\n\u003ch2 id=\"11-support\" class=\"mobile-header\"\u003e11. Support\u003c/h2\u003e\n\u003ch3 id=\"official-resources\" class=\"mobile-header\"\u003eOfficial Resources\u003c/h3\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eWebsite\u003c/strong\u003e: https://ollama.com\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eGitHub\u003c/strong\u003e: https://github.com/ollama/ollama\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eDocumentation\u003c/strong\u003e: https://github.com/ollama/ollama/tree/main/docs\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eModel Library\u003c/strong\u003e: https://ollama.com/library\u003c/li\u003e\n\u003ch3 id=\"community-support\" class=\"mobile-header\"\u003eCommunity Support\u003c/h3\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eDiscord\u003c/strong\u003e: https://discord.gg/ollama\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eReddit\u003c/strong\u003e: r/ollama\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eGitHub Issues\u003c/strong\u003e: https://github.com/ollama/ollama/issues\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eDiscussions\u003c/strong\u003e: https://github.com/ollama/ollama/discussions\u003c/li\u003e\n\u003ch2 id=\"12-contributing\" class=\"mobile-header\"\u003e12. Contributing\u003c/h2\u003e\n\u003ch3 id=\"how-to-contribute\" class=\"mobile-header\"\u003eHow to Contribute\u003c/h3\u003e\n\u003cp class=\"mobile-paragraph\"\u003e1. Fork the repository on GitHub\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003e2. Create a feature branch\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003e3. Submit pull request\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003e4. Follow Go coding standards\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003e5. Include tests and documentation\u003c/p\u003e\n\u003ch3 id=\"development-setup\" class=\"mobile-header\"\u003eDevelopment Setup\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Clone repository\ngit clone https://github.com/ollama/ollama.git\ncd ollama\n\n# Install Go dependencies\ngo mod tidy\n\n# Build from source\ngo build .\n\n# Run tests\ngo test ./...\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch2 id=\"13-license\" class=\"mobile-header\"\u003e13. License\u003c/h2\u003e\n\u003cp class=\"mobile-paragraph\"\u003eOllama is licensed under the MIT License.\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003eKey points:\u003c/p\u003e\n\u003cli class=\"mobile-list-item\"\u003eFree to use, modify, and distribute\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003eCommercial use allowed\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003eNo warranty provided\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003eAttribution required\u003c/li\u003e\n\u003ch2 id=\"14-acknowledgments\" class=\"mobile-header\"\u003e14. Acknowledgments\u003c/h2\u003e\n\u003ch3 id=\"credits\" class=\"mobile-header\"\u003eCredits\u003c/h3\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eOllama Team\u003c/strong\u003e: Core development team\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eMeta AI\u003c/strong\u003e: Llama model family\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eMistral AI\u003c/strong\u003e: Mistral models\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eCommunity\u003c/strong\u003e: Model creators and contributors\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003eHardware Vendors\u003c/strong\u003e: NVIDIA, AMD, Apple for acceleration support\u003c/li\u003e\n\u003ch2 id=\"15-version-history\" class=\"mobile-header\"\u003e15. Version History\u003c/h2\u003e\n\u003ch3 id=\"recent-releases\" class=\"mobile-header\"\u003eRecent Releases\u003c/h3\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003ev0.3.x\u003c/strong\u003e: Latest stable with improved performance\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003ev0.2.x\u003c/strong\u003e: Added model management improvements\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003ev0.1.x\u003c/strong\u003e: Initial public release\u003c/li\u003e\n\u003ch3 id=\"major-features-by-version\" class=\"mobile-header\"\u003eMajor Features by Version\u003c/h3\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003ev0.3.0\u003c/strong\u003e: Enhanced GPU support, model compression\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003ev0.2.0\u003c/strong\u003e: REST API improvements, concurrent requests\u003c/li\u003e\n\u003cli class=\"mobile-list-item\"\u003e\u003cstrong\u003ev0.1.0\u003c/strong\u003e: Basic model serving and CLI interface\u003c/li\u003e\n\u003ch2 id=\"16-appendices\" class=\"mobile-header\"\u003e16. Appendices\u003c/h2\u003e\n\u003ch3 id=\"a-api-usage-examples\" class=\"mobile-header\"\u003eA. API Usage Examples\u003c/h3\u003e\n\u003cp class=\"mobile-paragraph\"\u003e#### Basic Chat Completion\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003ecurl http://localhost:11434/api/generate -d \u0026#039;{\n  \u0026quot;model\u0026quot;: \u0026quot;llama3.1:8b\u0026quot;,\n  \u0026quot;prompt\u0026quot;: \u0026quot;Why is the sky blue?\u0026quot;,\n  \u0026quot;stream\u0026quot;: false\n}\u0026#039;\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003cp class=\"mobile-paragraph\"\u003e#### Streaming Response\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003ecurl http://localhost:11434/api/generate -d \u0026#039;{\n  \u0026quot;model\u0026quot;: \u0026quot;llama3.1:8b\u0026quot;,\n  \u0026quot;prompt\u0026quot;: \u0026quot;Write a poem about coding\u0026quot;,\n  \u0026quot;stream\u0026quot;: true\n}\u0026#039;\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003cp class=\"mobile-paragraph\"\u003e#### Chat API\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003ecurl http://localhost:11434/api/chat -d \u0026#039;{\n  \u0026quot;model\u0026quot;: \u0026quot;llama3.1:8b\u0026quot;,\n  \u0026quot;messages\u0026quot;: [\n    {\n      \u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;,\n      \u0026quot;content\u0026quot;: \u0026quot;Hello, how are you?\u0026quot;\n    }\n  ]\n}\u0026#039;\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"b-integration-examples\" class=\"mobile-header\"\u003eB. Integration Examples\u003c/h3\u003e\n\u003cp class=\"mobile-paragraph\"\u003e#### Python Integration\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"python\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003epython\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport requests\nimport json\n\ndef chat_with_ollama(prompt, model=\u0026quot;llama3.1:8b\u0026quot;):\n    url = \u0026quot;http://localhost:11434/api/generate\u0026quot;\n    data = {\n        \u0026quot;model\u0026quot;: model,\n        \u0026quot;prompt\u0026quot;: prompt,\n        \u0026quot;stream\u0026quot;: False\n    }\n    \n    response = requests.post(url, json=data)\n    if response.status_code == 200:\n        return response.json()[\u0026quot;response\u0026quot;]\n    else:\n        return \u0026quot;Error: \u0026quot; + str(response.status_code)\n\n# Usage\nresponse = chat_with_ollama(\u0026quot;Explain quantum computing\u0026quot;)\nprint(response)\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003cp class=\"mobile-paragraph\"\u003e#### Node.js Integration\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"javascript\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ejavascript\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-javascript\"\u003econst axios = require(\u0026#039;axios\u0026#039;);\n\nasync function chatWithOllama(prompt, model = \u0026#039;llama3.1:8b\u0026#039;) {\n    try {\n        const response = await axios.post(\u0026#039;http://localhost:11434/api/generate\u0026#039;, {\n            model: model,\n            prompt: prompt,\n            stream: false\n        });\n        \n        return response.data.response;\n    } catch (error) {\n        console.error(\u0026#039;Error:\u0026#039;, error.message);\n        return null;\n    }\n}\n\n// Usage\nchatWithOllama(\u0026#039;What is machine learning?\u0026#039;).then(response =\u0026gt; {\n    console.log(response);\n});\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"c-model-customization\" class=\"mobile-header\"\u003eC. Model Customization\u003c/h3\u003e\n\u003cp class=\"mobile-paragraph\"\u003e#### Creating Custom Models\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Create Modelfile\ncat \u0026gt; Modelfile \u0026lt;\u0026lt; \u0026#039;EOF\u0026#039;\nFROM llama3.1:8b\n\n# Set parameters\nPARAMETER temperature 0.7\nPARAMETER top_p 0.9\n\n# Set system message\nSYSTEM \u0026quot;\u0026quot;\u0026quot;\nYou are a helpful AI assistant specialized in programming.\nAlways provide code examples when relevant.\n\u0026quot;\u0026quot;\u0026quot;\nEOF\n\n# Build custom model\nollama create my-coding-assistant -f Modelfile\n\n# Test custom model\nollama run my-coding-assistant \u0026quot;How do I sort a list in Python?\u0026quot;\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003cp class=\"mobile-paragraph\"\u003e#### Fine-tuning (Advanced)\u003c/p\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# Prepare training data (JSONL format)\ncat \u0026gt; training_data.jsonl \u0026lt;\u0026lt; \u0026#039;EOF\u0026#039;\n{\u0026quot;prompt\u0026quot;: \u0026quot;Question: What is Python?\u0026quot;, \u0026quot;completion\u0026quot;: \u0026quot;Python is a programming language...\u0026quot;}\n{\u0026quot;prompt\u0026quot;: \u0026quot;Question: How to install packages?\u0026quot;, \u0026quot;completion\u0026quot;: \u0026quot;Use pip install package_name...\u0026quot;}\nEOF\n\n# Note: Fine-tuning requires additional tools and setup\n# Refer to Ollama documentation for detailed fine-tuning guide\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003ch3 id=\"d-performance-monitoring\" class=\"mobile-header\"\u003eD. Performance Monitoring\u003c/h3\u003e\n\u003cdiv class=\"mobile-code-block\" data-language=\"bash\"\u003e\n      \u003cdiv class=\"mobile-code-header\"\u003e\n        \u003cspan class=\"mobile-code-language\"\u003ebash\u003c/span\u003e\n        \u003cbutton class=\"mobile-copy-button\" onclick=\"copyCode(this)\" title=\"Copy code\"\u003e\n          \u003cspan class=\"copy-icon\"\u003e📋\u003c/span\u003e\n          \u003cspan class=\"copy-text\"\u003eCopy\u003c/span\u003e\n        \u003c/button\u003e\n      \u003c/div\u003e\n      \u003cdiv class=\"mobile-code-content\"\u003e\n        \u003cpre\u003e\u003ccode class=\"language-bash\"\u003e#!/bin/bash\n# monitor-ollama.sh\n\necho \u0026quot;=== Ollama Service Status ===\u0026quot;\nsystemctl status ollama --no-pager\n\necho -e \u0026quot;\\n=== Memory Usage ===\u0026quot;\nps aux | grep ollama | grep -v grep\n\necho -e \u0026quot;\\n=== GPU Usage ===\u0026quot;\nif command -v nvidia-smi \u0026amp;\u0026gt; /dev/null; then\n    nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits\nfi\n\necho -e \u0026quot;\\n=== API Health Check ===\u0026quot;\ncurl -s http://localhost:11434/api/version || echo \u0026quot;API not responding\u0026quot;\n\necho -e \u0026quot;\\n=== Loaded Models ===\u0026quot;\nollama ps\n\necho -e \u0026quot;\\n=== Disk Usage ===\u0026quot;\ndu -sh /var/lib/ollama/models/*\u003c/code\u003e\u003c/pre\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n\u003cp class=\"mobile-paragraph\"\u003e---\u003c/p\u003e\n\u003cp class=\"mobile-paragraph\"\u003eFor more information and updates, visit https://github.com/howtomgr/ollama\u003c/p\u003e","readTime":"12 min","wordCount":2213,"tableOfContents":[{"level":2,"text":"1. Prerequisites","id":"1-prerequisites"},{"level":3,"text":"Hardware Requirements","id":"hardware-requirements"},{"level":3,"text":"Software Requirements","id":"software-requirements"},{"level":3,"text":"Network Requirements","id":"network-requirements"},{"level":2,"text":"2. Supported Operating Systems","id":"2-supported-operating-systems"},{"level":2,"text":"3. Installation","id":"3-installation"},{"level":3,"text":"RHEL/CentOS/Rocky Linux/AlmaLinux","id":"rhelcentosrocky-linuxalmalinux"},{"level":3,"text":"Debian/Ubuntu","id":"debianubuntu"},{"level":3,"text":"Arch Linux","id":"arch-linux"},{"level":3,"text":"Alpine Linux","id":"alpine-linux"},{"level":3,"text":"openSUSE","id":"opensuse"},{"level":3,"text":"macOS","id":"macos"},{"level":3,"text":"Windows","id":"windows"},{"level":2,"text":"4. Configuration","id":"4-configuration"},{"level":3,"text":"Environment Variables","id":"environment-variables"},{"level":3,"text":"Configuration Options","id":"configuration-options"},{"level":3,"text":"Model Management","id":"model-management"},{"level":2,"text":"5. Service Management","id":"5-service-management"},{"level":3,"text":"systemd (Linux)","id":"systemd-linux"},{"level":3,"text":"Manual Service Management","id":"manual-service-management"},{"level":3,"text":"Windows Service Management","id":"windows-service-management"},{"level":2,"text":"6. Troubleshooting","id":"6-troubleshooting"},{"level":3,"text":"Common Issues","id":"common-issues"},{"level":3,"text":"Debug Mode","id":"debug-mode"},{"level":2,"text":"7. Security Considerations","id":"7-security-considerations"},{"level":3,"text":"Network Security","id":"network-security"},{"level":3,"text":"Reverse Proxy Configuration (nginx)","id":"reverse-proxy-configuration-nginx"},{"level":3,"text":"Authentication Setup","id":"authentication-setup"},{"level":3,"text":"File Permissions","id":"file-permissions"},{"level":2,"text":"8. Performance Tuning","id":"8-performance-tuning"},{"level":3,"text":"GPU Optimization","id":"gpu-optimization"},{"level":3,"text":"CPU Optimization","id":"cpu-optimization"},{"level":3,"text":"Memory Management","id":"memory-management"},{"level":3,"text":"Storage Optimization","id":"storage-optimization"},{"level":2,"text":"9. Backup and Restore","id":"9-backup-and-restore"},{"level":3,"text":"Model Backup","id":"model-backup"},{"level":3,"text":"Configuration Backup","id":"configuration-backup"},{"level":3,"text":"Restore Procedures","id":"restore-procedures"},{"level":3,"text":"Automated Backup","id":"automated-backup"},{"level":2,"text":"10. System Requirements","id":"10-system-requirements"},{"level":3,"text":"Minimum Requirements","id":"minimum-requirements"},{"level":3,"text":"Recommended Requirements","id":"recommended-requirements"},{"level":3,"text":"Model-Specific Requirements","id":"model-specific-requirements"},{"level":2,"text":"11. Support","id":"11-support"},{"level":3,"text":"Official Resources","id":"official-resources"},{"level":3,"text":"Community Support","id":"community-support"},{"level":2,"text":"12. Contributing","id":"12-contributing"},{"level":3,"text":"How to Contribute","id":"how-to-contribute"},{"level":3,"text":"Development Setup","id":"development-setup"},{"level":2,"text":"13. License","id":"13-license"},{"level":2,"text":"14. Acknowledgments","id":"14-acknowledgments"},{"level":3,"text":"Credits","id":"credits"},{"level":2,"text":"15. Version History","id":"15-version-history"},{"level":3,"text":"Recent Releases","id":"recent-releases"},{"level":3,"text":"Major Features by Version","id":"major-features-by-version"},{"level":2,"text":"16. Appendices","id":"16-appendices"},{"level":3,"text":"A. API Usage Examples","id":"a-api-usage-examples"},{"level":3,"text":"B. Integration Examples","id":"b-integration-examples"},{"level":3,"text":"C. Model Customization","id":"c-model-customization"},{"level":3,"text":"D. Performance Monitoring","id":"d-performance-monitoring"}],"lastBuilt":"2025-10-20T18:17:01.398Z","metadataVersion":"2.0"}]},"__N_SSG":true},"page":"/[category]","query":{"category":"artificial-intelligence"},"buildId":"OdIEZsIQLqybmcgo39fkq","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>